<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>详解让无数人顿悟的熵增定律</title>
      <link href="/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/"/>
      <url>/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="详解让无数人顿悟的熵增定律"><a href="#详解让无数人顿悟的熵增定律" class="headerlink" title="详解让无数人顿悟的熵增定律"></a>详解让无数人顿悟的熵增定律</h1><img src="/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/195327106160.gif" class=""><p>人活着就是在对抗熵增定律，生命以负熵为生。本文来自微信公众号：<a href="https://mp.weixin.qq.com/s/XbxnFWMjFS4zai_m8Z11EA">思维有了模型（ID：siweimoxing666）</a>，作者：模型君，原文标题：《熵增定律：为什么熵增理论让好多人一下子顿悟了》，头图来自：《成为简·奥斯汀》剧照</p><p><strong>如果物理学只能留一条定律，我会留熵增定律。</strong>说这句话的人叫吴国盛，清华大学的科学史系主任。</p><p>虽然你可能会反驳这个观点，难道不是牛顿的力学和爱因斯坦的相对论吗？</p><p>笔者也很迷惑，但是吴教授能说出这番话绝对不是无的放矢，不管对与不对，都可见熵增定律的分量。</p><p>无独有偶，吴军也说过类似的话。如果地球毁灭了，我们怎么能够在一张名片上写下地球文明的全部精髓，让其他文明知道我们曾有过这个文明呢？</p><p>吴军老师给出的答案是三个公式：</p><blockquote><p>1+1&#x3D;2（代表了数学文明）</p><p>E&#x3D;mc²（爱因斯坦的质能方程）</p><p>S&#x3D;-∑ P ln P（熵的定义）</p></blockquote><p>薛定谔在《生命是什么》中也说过类似的话<strong>“人活着就是在对抗熵增定律，生命以负熵为生。”</strong></p><p>爱丁顿爵士也曾说：“我认为，熵增原则是自然界所有定律中至高无上的。如果有人指出你的宇宙理论与麦克斯韦方程不符，那么麦克斯韦方程可能有不对；如果你的宇宙理论与观测相矛盾，嗯，观测的人有时也会把事情搞错；但是如果你的理论违背了热力学第二定律，我就敢说你没有指望了，你的理论只有丢尽脸、垮台。”</p><p>（注：爱丁顿说自己是除爱因斯坦之外，世界上唯一一个真正懂相对论的科学家，虽然看起来有点能吹，但应该也挺牛。）</p><p>前段时间大热的电影《信条》，诺兰大神的新作，也引用了“熵”作为整部电影的核心思想。他把熵增作为时间之失，用逆熵来穿越时空。</p><p>好多人走出影院大呼没有看懂，希望这篇文章能帮你解开“熵”的秘密。</p><p>一、为什么熵增定律让好多人一下子顿悟了</p><p><strong>因为它揭示了宇宙演化的终极规律。</strong></p><p>这个规律包括我们所有生命和非生命的演化规律，生命里又包含着个人和群体的演化规律。</p><p>非生命：比如物质总是向着熵增演化，屋子不收拾会变乱，手机会越来越卡，耳机线会凌乱，热水会慢慢变凉，太阳会不断燃烧衰变……直到宇宙的尽头——热寂。</p><p>场库《宇宙的未来》</p><p>生命与个人：比如自律总是比懒散痛苦，放弃总是比坚持轻松，变坏总是比变好容易。</p><p><strong>只有少部分意志坚定的人能做到自我管理，大多数人都是作息不规律，饮食不规律，学习不规律。</strong></p><p>生命与群体：比如大公司的组织架构会变得臃肿，员工会变得官僚化，整体效率和创新能力也会下降；封闭的国家会被世界淘汰。</p><p>这些所有的现象都可以用一个定律来解释——熵增定律。</p><p><strong>因为事物总是向着熵增的方向发展，所以一切符合熵增的，都非常的容易和舒适，比如懒散。</strong></p><p>《少有人走的路》在最后一章也如此解释自律。</p><blockquote><p>因为所有事物都在向着无规律，向着无序和混乱发展，如果你要变得自律，你就得逆着熵增做功，这个过程会非常痛苦。</p></blockquote><p>记得也曾有人问过模型君“人为什么要自律”的问题，我哑口无言，不知如何作答。因为每个人都有选择自己生活方式的权利，可以散漫也可以自律。</p><p>现在我想我找到答案了，<strong>生命本身就是自律的过程，即熵减的过程。</strong></p><p>二、什么是熵增定律</p><p>定义：<strong>在一个孤立系统里，如果没有外力做功，其总混乱度</strong>（熵）<strong>会不断增大。</strong></p><p>这里面有三个词非常重要：<strong>孤立系统、无外力做功、总混乱度</strong>（熵）。</p><p>首先我们来解释什么是熵。</p><p>熵（Entropy），最早在1865年由德国物理学家克劳修斯提出，<strong>用以度量一个系统“内在的混乱程度”。</strong></p><p>你可以理解为，<strong>系统中的无效能量。</strong></p><p>比如你花了100 J的能量把物体从A地拿到B地，这个过程中有很多能量并没有被100%的转化，而是有部分散失在了宇宙中。</p><p><strong>这部分能量不可逆，无法被再利用，且永远在增加。</strong></p><p>从这里你就可以推出，恒星终将熄灭，生命终将消失，宇宙将变成一片死寂，沦为熵。</p><p>这个状态，也被称为热寂。</p><p><strong>那么什么是熵增定律呢？</strong></p><p>就是这种熵在不断增加的过程。</p><p>但这是针对整个宇宙而言的，如果要针对地球，针对一个国家，针对一个企业，针对某一个人，则要加上两个限制条件——<strong>封闭系统＋无外力做功。</strong></p><p>任何一个系统，只要满足封闭系统，而且无外力维持，它就会趋于混乱和无序。</p><p>生命也如此。</p><img src="/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/2.jpeg" class=""><p>三、如何对抗熵增，实现超越</p><p>熵增定律被称为最让人沮丧的定律。它不仅预示了宇宙终将归于热寂，生命终将消失。</p><p>而且，从小的方面来说：</p><p><strong>它左右着国家和企业的发展规律，让组织变得臃肿，缺乏效率和创新；它左右着个人的方方面面，让我们安于懒散、难以坚持、难以自律……</strong></p><p>那么这还有办法可解吗？</p><p>从定义来说，熵增的条件有两个：<strong>封闭系统＋无外力做功。</strong></p><p>只要打破这两个条件，我们就有可能实现熵减。</p><p>听起来好抽象，怎么理解？</p><p>也许我们可以从生命里得到启示，<strong>整个生命的发展就是一部负熵的历史。</strong></p><p>当我们人从无机生命到有机生命那一刻起，就注定了这会是一部艰辛与精彩共存的史诗。</p><p>我们的始祖是一种“蛋白质＋RNA”的聚合体，科学家将她命名为LUCA。</p><p>LUCA通过吸收能量来大量复制，但是问题来了，<strong>宇宙的熵总的来说是增加的，所以LUCA的减熵会导致环境的急剧熵增。</strong></p><p>环境恶化，LUCA无奈只能进化，变得更高级以适应环境的变化，于是DNA聚合体诞生了。</p><p>DNA比RNA更稳定，也更加智能。但是这样一来，消耗的能量更大，吸收的物质更多，导致环境的熵增比以往更大。</p><p>所以DNA聚合体被逼着向单细胞演化，同样，环境的熵增再次增加，于是单细胞又向更高级的多细胞进化，于是寒武纪生命大爆发诞生了。</p><p><strong>又因为孤立系统无法获取足够的能量，所以多细胞开始移动，</strong>并且产生了感知能力，比如视觉、嗅觉、听觉等等。</p><p>从此，生命走上了智能的进化之路。</p><p>这一过程，也被王东岳老爷子称为<strong>递弱代偿。</strong></p><p>即生命的熵减过程，会加剧环境的熵增，于是环境会变得越来越恶劣，生命为了生存，为了获得足够的能量和物质，必须变得更加智能……</p><p>好了，现在我们来总结一下，生命在减熵过程中，其实一直在做三件事。</p><p><strong>第一，努力保证能量的供给。</strong>比如，从化学作用到光合作用和呼吸作用；到光合作用＋呼吸作用的结合体；到多细胞生物。</p><p><strong>第二，努力开放系统。</strong>细胞从无法移动，到进化出游动能力、爬行能力、行走能力、飞行能力。</p><p><strong>第三，努力变得更加智能。</strong>生命为了花更少的能量来获取更多物质和能量，进化出了感知能力，比如当时的霸主奇虾，就有很大一对眼睛。知道的信息越多，就能减少更多熵的耗散。</p><p>这三点正好是企业和个人的进化要件。</p><p>一、企业</p><p> <strong>1. 主动做功</strong> </p><p>许多公司在创业初期非常努力，每天花大量的精力进行各种战略和组织的进化。但是随着企业的做大和成熟，员工就会慢慢懈怠下来，组织会变得臃肿，制度会腐旧脱节。</p><p>所以，作为leader你要努力保证企业的活力。比如采取扁平化的结构，让团队各自为战，回归创业初期时的热情。</p><p>记住，舒适圈是熵增定律的第一张王牌，任何时候你都不能松懈。<strong>一旦你减少了能量的投入，企业的熵增就会立马回来。</strong></p><p><strong>2. 开放系统</strong> </p><p>关于开放系统，有一个伟大的发现，<strong>叫做耗散结构，</strong>它给我们带去了一丝希望。</p><p>什么是耗散结构？它有三个特征：</p><p><strong>①开放性</strong></p><p>怎么理解？你可以理解为，系统把无用的熵排出去，然后吸收新的可用物质、能量和信息。(注：熵有三种，物质熵、能量熵、信息熵，在相对论里物质和能量是一回事，但是为了理解，这里我们把它分开。)</p><p>比如你每天的新陈代谢，比如你通过锻炼减去一身的赘肉，比如你看一本好书。</p><p>基于此，企业也可以得到启示。</p><p><strong>企业要想对抗熵增，就必须开放，把那些衰败为熵的东西全部排出系统。</strong></p><p>比如腐败的制度、无产出的员工、落后的信息等等；然后吸收新鲜血液，比如先进的理念、新的人才、前沿信息等等。</p><p>华为就是最推崇这一理念的，任正非老爷子把这个耗散结构作为华为的底层逻辑。</p><blockquote><p>任正非说：“我们一定要避免封闭系统。我们一定要建立一个开放的体系，特别是硬件体系更要开放，不开放就是死亡。”</p></blockquote><p>与此同时，华为每年淘汰干部10%，员工淘汰5%。每年18万人会淘汰5千人到9千人来激活这个团队。</p><p><strong>②远离平衡态</strong></p><p>这怎么理解？</p><p>你可以理解为，当熵逐渐增大，虽然系统会变得越来越混乱无序，<strong>但是这种结构却更稳定，这种稳定就是平衡态，你要远离这种平衡态。</strong></p><p>比如一个企业做大了，企业内部就会形成一种非常稳固的结构，这种结构很可能就是官僚结构。</p><p>企业想要推行新的理念，引进新的人才，吸收新的信息，都会非常困难。</p><p>解决办法就是，<strong>打破这种平衡态，让系统内部流动起来。</strong></p><p>这方面模型君见过最牛逼的是韩都衣舍，他们采取小团队模式，每个团队2-3人，包括设计师、页面制作专员、货品管理专员。</p><p>员工自己可以自由选择任何团队，也可以自己组建团队。通过分成、授权、竞争、淘汰等一系列机制，来进行充分的内部流动。</p><p>最后无能的员工（熵）被淘汰出局，剩下的精英继续流动、重组，变得更加强大。</p><p><strong>③非线性</strong></p><p>怎么理解非线性？</p><p>你可以理解为，<strong>一个微小的变化也有可能导致一个巨大的突变。</strong>（与此相关的实验有贝纳尔对流）</p><p>比如在一个标准大气压下，你给一壶水加热，前面99°都没有沸腾，可是你再加热1°它就沸腾了，这就是非线性。</p><p>同样企业也如此，可能你前面做了很多努力，效果甚微，但是不要气馁，打破熵增的要素是非线性的，<strong>总有一天，你一个微小的投入就会带来巨大的突变。</strong></p><p>比如亚马逊，它可能是这个世界上失败最多的企业了，但他们对失败非常包容，因为他们不断在赌“每次小的努力和尝试，都有可能引发意想不到的超额惊喜”。</p><img src="/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/3.jpeg" class=""><p>二、个人</p><p>也许你会觉得自己还达不到在企业里运用熵增定律的高度，没关系，熵增定律也同样适用于你个人的发展。</p><p>比如工作、生活、学习、心情、成长、人际关系等等都与此相关。</p><p>就拿生活来说，每天会有各种各样的琐事涌来，<strong>如果我们任由其发展，那我们的生活就会变得越来越混乱。</strong></p><p>之后我们要想恢复到有秩序的状态，就不得不花非常大的代价才行。</p><p>这样的例子身边比比皆是，生活一团乱麻，不知道自己要什么，想改变现状也不知道如何入手，只能浑浑噩噩，得过且过。</p><p>这种状态就是生活陷入了极度的熵增状态，被无数的混乱的事情牵着走，<strong>丧失了生活的掌控权。</strong></p><p>除此之外还有很多，比如情绪。很多时候，我们感到难过、烦躁、焦虑，<strong>其实是因为情绪太过混乱，很多感情交织在一起，让你无从下手。</strong>心理学叫这，情绪颗粒度。</p><p>再比如专注这件事，好像高中之后我们就很难专注了。原因是因为，大脑里面整天要想的事情太多，一会要做这个，一会要做那个，一会这种情感，一会那种情感。</p><p><strong>这些杂七杂八的东西堆积在一起，就会扰乱我们的注意力，</strong>让我们无法的专注的做一件事情。</p><p>类似的还有作息不规律、饮食不规律、懒散等等，都是因为事情总趋于熵增。</p><p>如果我们不主动投入能量做熵减，生活就会脱离我们的掌控。</p><p> 那要怎么办呢？解决办法仍然是：</p><p><strong>1. 主动做功</strong> </p><p>你不能等到生活脱离了你的掌控，才后知后觉的介入。</p><p>你要每天都保持清晰的思绪，主动投入时间和精力，去理清你的情绪，理清你每天所做之事，理清你想要的是什么。</p><p>我在未来大学里学到一招，叫做清空干扰。</p><p><strong>把当下所有情绪和事件都清空，然后把它们都记在一个备忘录里，你可以叫它追踪系统，然后脑子里永远都只装3件事。</strong></p><p>比如，模型君今天的3件事是写文章、看书、建立写作系统。</p><p>其他的还有洗衣服、取快递、清理微信收藏等各种事情，就都先全部放到追踪系统里。</p><p>如果还有一些突发的情绪，比如突然想起某件尴尬的事，都统统丢进去。或者突发的事，比如某人发来的微信消息，你感觉不是一两分钟就能解决，也丢进去。</p><p>这样做有一个好处，首先你的大脑里永远只会有3件最重要的事，不会东搞搞西搞搞。</p><p>而且你还放心，因为你已经把事情记下来了，你不用为此担心，做完之后你会有时间来处理的。</p><p>再比如学习这件事，当我写这篇文章时，大脑是一片浆糊，怎么办呢？</p><p>画思维导图，<strong>思维导图的第一性原理就是降低信息的混乱度。</strong></p><p>但是这个过程真的极其痛苦，我需要不断对信息进行分类归纳，有的地方要反复改。</p><p>痛苦着痛苦着，突然“熵增定律”闯进了我的脑海，让知识变得有序的过程不正是熵减的过程吗？</p><p>所以痛苦是必然的，舒服是留给死人的。</p><p><strong>虽然很痛苦，但还是得主动去做这件事。</strong></p><p><strong>2. 开放系统</strong> </p><p>这里也用耗散结构来分析，但是前面企业里讲了，所以这里略讲一下。</p><p><strong>①开放性</strong></p><p>你要一直保持与外界交流的状态，<strong>把过去的熵埋葬，然后拥抱新的明天。</strong></p><p>什么是过去的熵？比如打翻的牛奶，腐旧的认知，回不去的人。</p><p>什么是拥抱新的明天？比如去新的环境（旅行），获取新的认知（读书），结交新的人（社交）。</p><p><strong>②远离平衡态</strong></p><p>我们极容易陷入平衡态，即使你尝试了一件新的事情，认识了一个新的人，你也会很快熟悉，并待在这种状态之下，认知里面叫“舒适圈”。</p><p><strong>如果你发现你的生活很久没有波澜了，想必你已经掉进平衡态了。</strong></p><p>比如我写作两年了，写作水平很大一段时间都没有进步 ，这就是平衡态，这是不好的。</p><p>你要不断超越自己，给自己新的目标，新的计划。</p><p><strong>③非线性</strong></p><p>非线性，其实就是复利效应。</p><p>也许你此刻做的很多努力，看起来杯水车薪，学习、生活都没有改变多少。</p><p>但是请不要灰心，继续坚持熵减，等到有一天，你只需要一丁点努力，就会开启你开挂的人生。</p><p><strong>3. 智能化</strong> </p><p>最后，还想谈一点，也是所有熵减方法里面最强大的一个东西。</p><p>它不仅适合任何组织的进化，也契合我们个人的进化。</p><p>这正是我们前面提到，生命的演化里面的第三点——智能化。</p><p><strong>整个生命的减熵史，就是一个不断变得智能的历史。</strong></p><p>为什么生物非得需要智能化呢？难道外力做功和开放系统都不足够我们生存的吗？</p><p>模型君不敢说100%需要智能化，但是从生命的演化来看，似乎都是在朝着这条路发展。</p><p><strong>因为一旦你熵减了，那么你的环境就会加剧熵增，</strong>也就是说环境会变得越来越恶劣。</p><p>如果生物要生存，就需要更强的减熵能力。</p><p>这种更强的减熵能力从何而来呢？显然光靠光合作用和呼吸作用，以及开放系统是远远不够的。</p><p>明白了这一点，你就明白了为什么一个RNA聚合体会进化成单细胞，进化成多细胞，进化成有限生殖，进化成猿人，进化成智人，进化成今天的我们。</p><p><strong>这种智能化的过程是必然的。</strong></p><p>只是我们非常有幸，也许在某个外太空，是类似海豚这样的生物具有智慧。</p><p>这个过程，王东岳老爷子将其整合为一个哲学概念——<strong>递弱代偿。</strong></p><p>当我们的生存环境很变得越来越艰难，为了生存我们就需要更强大的生存能力。</p><p>比如从农耕时代到工业时代，到现在的互联网时代，到未来的人工智能时代。</p><blockquote><p>“这也是为什么我们今天的竞争力会越来越大的原因，也是为什么我们变得越来越焦虑的原因，因为环境熵增了。”</p></blockquote><p>好了，现在我们懂了，减熵的终极方向是智能化。那么如何智能化呢？</p><p><strong>答案是降低信息熵。</strong></p><p>什么是信息熵？它被用来度量信息的不确定度，信息熵越大，不确定性就越大。</p><p>在你变得越来越智能的过程中，就获取了更多信息，消除了一些不确定性，所以熵减少。</p><p>前面提到熵有两种，<strong>热力学熵和信息熵。</strong>其实这两种熵是可以用公式做等号的，因为获取信息需要能量。</p><p>1bit 信息熵＝kln2（J&#x2F;K）热力学熵</p><p><strong>当你信息有局限的时候，要做成一件事，你就需要更多的能量，产生更多的熵。</strong></p><p>比如做同一套试卷，学霸跟学渣做题所需的时间和能量肯定是不同的，学霸一个小时就做出来了，学渣可能做了三四个小时还做不完。</p><p>比如炼钢厂，小炼钢厂要花很多时间和能量，而且材料利用率低，而大企业因为掌握更多信息，不仅耗能更少，效率也更高。</p><p>这也是为什么历来伟大的企业家都博览群书的原因。想起查理·芒格的一句话：</p><blockquote><p>我这辈子遇到的聪明人，没有不每天阅读的，没有，一个都没有。沃伦读书之多，我读书之多，可能会让你感到吃惊。</p></blockquote><p>可以看看“超智能体”的这个视频，能帮助你更好的理解智能的作用。</p><p>▲来自超智能体</p><p><strong>智能充当的角色，就是从无序中发现有序，减少大量的瞎几把做功。</strong></p><p>不论是企业还是个人，如果你想站在更高的维度俯视世界，光做功和开放是不够的，你还必须在信息上，上升一个维度，做到四两拨千斤的效果。</p><p>这个过程其实就是思维有了模型一直在做的事——<strong>眼界和认知。</strong></p><p>如果你想在此生有所建树的话，那么努力提升自己的眼界和认知，让自己变得更智能吧。</p><img src="/2022/06/03/%E8%AF%A6%E8%A7%A3%E8%AE%A9%E6%97%A0%E6%95%B0%E4%BA%BA%E9%A1%BF%E6%82%9F%E7%9A%84%E7%86%B5%E5%A2%9E%E5%AE%9A%E5%BE%8B/4.jpeg" class=""><p>四、关于熵增你还应知道</p><p>\1. 熵增无好坏之分</p><p>看起来，整篇文章都在避免怎么熵增，似乎熵增是一个十恶不赦的坏蛋。</p><p>但是须知道，对于宇宙而言，熵增只是一个法则，没有好坏之分。</p><p><strong>好坏只是人为在道德上的定性，这个定性对于宇宙来说，毫无意义。</strong></p><p>\2. 无序只是概率事件</p><p>看起来，事物从有序到无序是必然事件。其实不是，它只是一个概念事件。</p><p>中学的时候我们就学过，微观原子的运动是无规则的，所以如果一个房间里面只有几个原子，那么这几个原子是完全可以全部聚在房间的一边的。</p><p>但如果一个房间里有10^1000个原子，那么这些原子聚在同一边的情况就几乎不可能发生，即使有那么一瞬聚在了一起，下一秒也还是会回到混乱的状态。</p><p><strong>所以我们可以说，在现实生活中，熵增是必然的。</strong></p><p>就好比我们把脸前的空气吹走，下一秒就一定会有其他空气填充进来一来。</p><p>\3. 意义是针对参考系而言的</p><p>很多读者给我留言说：“看完这篇文章后，瞬间觉得人生活着没有意义。”</p><p>我的看法是：意义是针对参考系而言的。</p><p><strong>对于宇宙而言，人类的存在确实没有意义；但对于人类而言，你的存在就有意义。</strong></p><p>**<br>**</p><p>本文来自微信公众号：<a href="https://mp.weixin.qq.com/s/XbxnFWMjFS4zai_m8Z11EA">思维有了模型（ID：siweimoxing666）</a>，作者：模型君             </p><p>本内容为作者独立观点，不代表虎嗅立场。未经允许不得转载，授权事宜请联系 <a href="mailto:&#104;&#x65;&#x7a;&#x75;&#111;&#64;&#x68;&#x75;&#x78;&#105;&#117;&#x2e;&#99;&#111;&#109;">&#104;&#x65;&#x7a;&#x75;&#111;&#64;&#x68;&#x75;&#x78;&#105;&#117;&#x2e;&#99;&#111;&#109;</a><br>如对本稿件有异议或投诉，请联系<a href="mailto:&#116;&#x6f;&#117;&#x67;&#97;&#111;&#64;&#104;&#x75;&#x78;&#105;&#x75;&#x2e;&#99;&#x6f;&#x6d;">&#116;&#x6f;&#117;&#x67;&#97;&#111;&#64;&#104;&#x75;&#x78;&#105;&#x75;&#x2e;&#99;&#x6f;&#x6d;</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 收藏,熵增,真理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git笔记二-命令</title>
      <link href="/2022/06/03/git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2022/06/03/git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="1-常用命令"><a href="#1-常用命令" class="headerlink" title="1. 常用命令"></a>1. 常用命令</h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><h2 id="列出所有分支"><a href="#列出所有分支" class="headerlink" title="列出所有分支"></a>列出所有分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -a</span><br></pre></td></tr></table></figure><h2 id="查看跟踪分支"><a href="#查看跟踪分支" class="headerlink" title="查看跟踪分支"></a>查看跟踪分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -vv</span><br></pre></td></tr></table></figure><h2 id="切换分支"><a href="#切换分支" class="headerlink" title="切换分支"></a>切换分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gti checkout &lt;branch&gt;</span><br></pre></td></tr></table></figure><h2 id="创建跟踪分支"><a href="#创建跟踪分支" class="headerlink" title="创建跟踪分支"></a>创建跟踪分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b [分支名] [远程名]/[分支名]</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout --<span class="built_in">track</span> <span class="built_in">origin</span>/branch</span><br></pre></td></tr></table></figure><h2 id="创建并切换分支"><a href="#创建并切换分支" class="headerlink" title="创建并切换分支"></a>创建并切换分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b &lt;branch&gt;</span><br></pre></td></tr></table></figure><h2 id="删除远程仓库分支"><a href="#删除远程仓库分支" class="headerlink" title="删除远程仓库分支"></a>删除远程仓库分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push orign --delete &lt;branch&gt;</span><br></pre></td></tr></table></figure><h2 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge</span><br></pre></td></tr></table></figure><h2 id="状态查看命令"><a href="#状态查看命令" class="headerlink" title="状态查看命令"></a>状态查看命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><h2 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure><h2 id="拉取代码"><a href="#拉取代码" class="headerlink" title="拉取代码"></a>拉取代码</h2><h2 id="从仓库拉取代码"><a href="#从仓库拉取代码" class="headerlink" title="从仓库拉取代码"></a>从仓库拉取代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch</span><br></pre></td></tr></table></figure><h2 id="从仓库拉取代码并合并到当前分支"><a href="#从仓库拉取代码并合并到当前分支" class="headerlink" title="从仓库拉取代码并合并到当前分支"></a>从仓库拉取代码并合并到当前分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git plull</span><br></pre></td></tr></table></figure><p>该命令等同于</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch &amp;&amp; git merge &lt;拉去的分支&gt; </span><br></pre></td></tr></table></figure><h2 id="重置代码"><a href="#重置代码" class="headerlink" title="重置代码"></a>重置代码</h2><p>使用 <code>&lt;code&gt;</code> git reset [–mixed|–soft|–hard] <code>&lt;commit&gt;</code> <code>&lt;/code&gt;</code> 命令重置本地仓库已提交的代码</p><ol><li>mixed</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset &lt;commit&gt;</span><br></pre></td></tr></table></figure><p>默认不指定参数是使用的是–mixed方式，会重置归档区、暂存区，工作区代码会变回到commit时</p><ol start="2"><li>soft</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --soft  &lt;commit&gt;</span><br></pre></td></tr></table></figure><p>使用–soft参数时，会重置归档区，暂存区、工作区会保留</p><ol start="3"><li>hard</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard  &lt;commit&gt;</span><br></pre></td></tr></table></figure><p>使用–soft参数时，归档区、暂存区、工作区都会被重置</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>centos firewalld</title>
      <link href="/2022/06/03/CentOS%20systemctl/"/>
      <url>/2022/06/03/CentOS%20systemctl/</url>
      
        <content type="html"><![CDATA[<h2 id="1、firewalld的基本使用"><a href="#1、firewalld的基本使用" class="headerlink" title="1、firewalld的基本使用"></a>1、firewalld的基本使用</h2><ul><li>启动：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start firewalld</span><br></pre></td></tr></table></figure><ul><li>关闭：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br></pre></td></tr></table></figure><ul><li>查看状态：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld </span><br></pre></td></tr></table></figure><ul><li>开机禁用 ：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><ul><li>开机启用 ：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> firewalld</span><br></pre></td></tr></table></figure><h2 id="2、systemctl控制服务"><a href="#2、systemctl控制服务" class="headerlink" title="2、systemctl控制服务"></a>2、systemctl控制服务</h2><ul><li>启动一个服务：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start firewalld.service</span><br></pre></td></tr></table></figure><ul><li>关闭一个服务：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br></pre></td></tr></table></figure><ul><li>重启一个服务：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart firewalld.service</span><br></pre></td></tr></table></figure><ul><li>显示一个服务的状态：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure><ul><li>在开机时启用一个服务：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> firewalld.service</span><br></pre></td></tr></table></figure><ul><li>在开机时禁用一个服务：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">disable</span> firewalld.service</span><br></pre></td></tr></table></figure><ul><li>查看服务是否开机启动：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl is-enabled firewalld.service</span><br></pre></td></tr></table></figure><ul><li>查看已启动的服务列表：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-unit-files|grep enabled</span><br></pre></td></tr></table></figure><ul><li>查看启动失败的服务列表：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl --failed</span><br></pre></td></tr></table></figure><h2 id="3、配置firewalld-cmd"><a href="#3、配置firewalld-cmd" class="headerlink" title="3、配置firewalld-cmd"></a>3、配置firewalld-cmd</h2><ul><li><p>查看版本：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd <span class="comment">--version</span></span><br></pre></td></tr></table></figure></li><li><p>查看帮助：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --<span class="built_in">help</span></span><br></pre></td></tr></table></figure></li><li><p>显示状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --state</span><br></pre></td></tr></table></figure></li><li><p>查看所有打开的端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --list-ports</span><br></pre></td></tr></table></figure></li><li><p>更新防火墙规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure></li><li><p>查看区域信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --get-active-zones</span><br></pre></td></tr></table></figure></li><li><p>查看指定接口所属区域：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --get-zone-of-interface=eth0</span><br></pre></td></tr></table></figure></li><li><p>拒绝所有包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --panic-on</span><br></pre></td></tr></table></figure></li><li><p>取消拒绝状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --panic-off</span><br></pre></td></tr></table></figure></li><li><p>查看是否拒绝：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --query-panic</span><br></pre></td></tr></table></figure></li></ul><h2 id="4、端口管理"><a href="#4、端口管理" class="headerlink" title="4、端口管理"></a>4、端口管理</h2><p>添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --permanent永久生效，没有此参数重启后失效</span></span><br><span class="line">firewall-cmd --zone=public --add-port=80/tcp --permanent   </span><br></pre></td></tr></table></figure><p>重新载入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><p>查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone= public --query-port=80/tcp</span><br></pre></td></tr></table></figure><p>删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone= public --remove-port=80/tcp --permanent</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> centos </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git使用教程：最详细、最傻瓜、最浅显、真正手把手教！</title>
      <link href="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/"/>
      <url>/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>出处：<a href="http://www.cnblogs.com/tugenhua0707">www.cnblogs.com/tugenhua0707</a></p><p><strong>导读：</strong>因为教程详细，所以行文有些长，新手边看边操作效果出乎你的预料。GitHub虽然有些许改版，但并无大碍。</p><h2 id="一、Git是什么？"><a href="#一、Git是什么？" class="headerlink" title="一、Git是什么？"></a><strong>一、Git是什么？</strong></h2><p>Git是目前世界上最先进的分布式版本控制系统。<br>工作原理 &#x2F; 流程：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112037562.png" class=""><ul><li>Workspace：工作区</li><li>Index &#x2F; Stage：暂存区</li><li>Repository：仓库区（或本地仓库）</li><li>Remote：远程仓库</li></ul><h2 id="二、SVN与Git的最主要的区别？"><a href="#二、SVN与Git的最主要的区别？" class="headerlink" title="二、SVN与Git的最主要的区别？"></a><strong>二、SVN与Git的最主要的区别？</strong></h2><p>SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑，所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。</p><p>Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。</p><h2 id="三、在windows上如何安装Git？"><a href="#三、在windows上如何安装Git？" class="headerlink" title="三、在windows上如何安装Git？"></a><strong>三、在windows上如何安装Git？</strong></h2><p>msysgit是 windows版的Git,如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640.jpeg" class=""><p>需要从网上下载一个，然后进行默认安装即可。安装完成后，在开始菜单里面找到 “Git –&gt; Git Bash”,如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112055167.jpeg" class=""><p>会弹出一个类似的命令窗口的东西，就说明Git安装成功。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112104904.jpeg" class=""><p>安装完成后，还需要最后一步设置，在命令行输入如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112114776.jpeg" class=""><p>因为Git是分布式版本控制系统，所以需要填写用户名和邮箱作为一个标识。</p><p>注意：git config –global 参数，有了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然你也可以对某个仓库指定的不同的用户名和邮箱。</p><h2 id="四、如何操作？"><a href="#四、如何操作？" class="headerlink" title="四、如何操作？"></a><strong>四、如何操作？</strong></h2><p><strong>1. 创建版本库。</strong></p><p>什么是版本库？版本库又名仓库，英文名repository,你可以简单的理解一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改，删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻还可以将文件”还原”。</p><p>所以创建一个版本库也非常简单，如下我是D盘 –&gt; www下 目录下新建一个testgit版本库。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112125876.png" class=""><p>pwd 命令是用于显示当前的目录。</p><p>通过命令 git init 把这个目录变成git可以管理的仓库，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112139480.png" class=""><p>这时候你当前testgit目录下会多了一个.git的目录，这个目录是Git来跟踪管理版本的，没事千万不要手动乱改这个目录里面的文件，否则，会把git仓库给破坏了。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112145907.png" class=""><p>下面先看下demo如下演示：</p><p>我在版本库testgit目录下新建一个记事本文件 readme.txt 内容如下：11111111</p><p>第一步：使用命令 git add readme.txt添加到暂存区里面去。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112203423.png" class=""><p>如果和上面一样，没有任何提示，说明已经添加成功了。</p><p>第二步：用命令 git commit告诉Git，把文件提交到仓库。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112210552.png" class=""><p>现在我们已经提交了一个readme.txt文件了，我们下面可以通过命令git status来查看是否还有文件未提交，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112219188.png" class=""><p>说明没有任何文件未提交，但是我现在继续来改下readme.txt内容，比如我在下面添加一行2222222222内容，继续使用git status来查看下结果，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112226600.png" class=""><p>上面的命令告诉我们 readme.txt文件已被修改，但是未被提交的修改。</p><p>把文件添加到版本库中。</p><p>首先要明确下，所有的版本控制系统，只能跟踪文本文件的改动，比如txt文件，网页，所有程序的代码等，Git也不列外，版本控制系统可以告诉你每次的改动，但是图片，视频这些二进制文件，虽能也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是知道图片从1kb变成2kb，但是到底改了啥，版本控制也不知道。</p><p>接下来我想看下readme.txt文件到底改了什么内容，如何查看呢？可以使用如下命令：</p><p>git diff readme.txt 如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112234836.png" class=""><p>如上可以看到，readme.txt文件内容从一行11111111改成 二行 添加了一行22222222内容。</p><p>知道了对readme.txt文件做了什么修改后，我们可以放心的提交到仓库了，提交修改和提交文件是一样的2步(第一步是git add 第二步是：git commit)。</p><p>如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112241137.png" class=""><p><strong>2. 版本回退：</strong><br>如上，我们已经学会了修改文件，现在我继续对readme.txt文件进行修改，再增加一行</p><p>内容为33333333333333.继续执行命令如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112249698.png" class=""><p>现在我已经对readme.txt文件做了三次修改了，那么我现在想查看下历史记录，如何查呢？我们现在可以使用命令 git log 演示如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112258834.png" class=""><p>git log命令显示从最近到最远的显示日志，我们可以看到最近三次提交，最近的一次是,增加内容为333333.上一次是添加内容222222，第一次默认是 111111.如果嫌上面显示的信息太多的话，我们可以使用命令 git log –pretty&#x3D;oneline 演示如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112308346.png" class=""><p>现在我想使用版本回退操作，我想把当前的版本回退到上一个版本，要使用什么命令呢？可以使用如下2种命令，第一种是：git reset –hard HEAD^ 那么如果要回退到上上个版本只需把HEAD^ 改成 HEAD^^ 以此类推。那如果要回退到前100个版本的话，使用上面的方法肯定不方便，我们可以使用下面的简便命令操作：git reset –hard HEAD~100 即可。未回退之前的readme.txt内容如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112316643.png" class=""><p>如果想回退到上一个版本的命令如下操作：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112324086.png" class=""><p>再来查看下 readme.txt内容如下：通过命令cat readme.txt查看</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112331881.png" class=""><p>可以看到，内容已经回退到上一个版本了。我们可以继续使用git log 来查看下历史记录信息，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112337807.png" class=""><p>我们看到 增加333333 内容我们没有看到了，但是现在我想回退到最新的版本，如：有333333的内容要如何恢复呢？我们可以通过版本号回退，使用命令方法如下：</p><p>git reset –hard 版本号 ，但是现在的问题假如我已经关掉过一次命令行或者333内容的版本号我并不知道呢？要如何知道增加3333内容的版本号呢？可以通过如下命令即可获取到版本号：git reflog 演示如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112344029.png" class=""><p>通过上面的显示我们可以知道，增加内容3333的版本号是 6fcfc89.我们现在可以命令</p><p>git reset –hard 6fcfc89来恢复了。演示如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112352829.png" class=""><p>可以看到 目前已经是最新的版本了。</p><p><strong>3. 理解工作区与暂存区的区别？</strong><br>工作区：就是你在电脑上看到的目录，比如目录下testgit里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴。<br>版本库(Repository)：工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD。</p><p>我们前面说过使用Git提交文件到版本库有两步：</p><p>第一步：是使用 git add 把文件添加进去，实际上就是把文件添加到暂存区。</p><p>第二步：使用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上。</p><p>我们继续使用demo来演示下：</p><p>我们在readme.txt再添加一行内容为4444444，接着在目录下新建一个文件为test.txt 内容为test，我们先用命令 git status来查看下状态，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112402339.png" class=""><p>现在我们先使用git add 命令把2个文件都添加到暂存区中，再使用git status来查看下状态，如下：</p><p>接着我们可以使用git commit一次性提交到分支上，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112409993.png" class=""><p><strong>4. Git撤销修改和删除文件操作。</strong></p><ol><li>撤销修改：<br>比如我现在在readme.txt文件里面增加一行 内容为555555555555，我们先通过命令查看如下：</li></ol><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112418549.png" class=""><p>在我未提交之前，我发现添加5555555555555内容有误，所以我得马上恢复以前的版本，现在我可以有如下几种方法可以做修改：</p><p>第一：如果我知道要删掉那些内容的话，直接手动更改去掉那些需要的文件，然后add添加到暂存区，最后commit掉。</p><p>第二：我可以按以前的方法直接恢复到上一个版本。使用 git reset –hard HEAD^</p><p>但是现在我不想使用上面的2种方法，我想直接想使用撤销命令该如何操作呢？首先在做撤销之前，我们可以先用 git status 查看下当前的状态。如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112426720.png" class=""><p>可以发现，Git会告诉你，git checkout – file 可以丢弃工作区的修改，如下命令：<br>git checkout – readme.txt,如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112435546.png" class=""><p>命令 git checkout –readme.txt 意思就是，把readme.txt文件在工作区做的修改全部撤销，这里有2种情况，如下：</p><p>1.readme.txt自动修改后，还没有放到暂存区，使用 撤销修改就回到和版本库一模一样的状态。<br>2.另外一种是readme.txt已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。<br>对于第二种情况，我想我们继续做demo来看下，假如现在我对readme.txt添加一行 内容为6666666666666，我git add 增加到暂存区后，接着添加内容7777777，我想通过撤销命令让其回到暂存区后的状态。如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112442662.png" class=""><p>注意：命令git checkout – readme.txt 中的 – 很重要，如果没有 – 的话，那么命令变成创建分支了。</p><ol start="2"><li>删除文件。<br>假如我现在版本库testgit目录添加一个文件b.txt,然后提交。如下：</li></ol><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112542855.png" class="640-20220503112505767.png&quot; } 如上：一般情况下，可以直接在文件目录中把文件删了，或者使用如上rm命令：rm b.txt ，如果我想彻底从版本库中删掉了此文件的话，可以再执行commit命令 提交掉，现在目录是这样的， {% asset_img"><p>只要没有commit之前，如果我想在版本库中恢复此文件如何操作呢？</p><p>可以使用如下命令 git checkout – b.txt，如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112549914.png" class=""><p>再来看看我们testgit目录，添加了3个文件了。如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112559213.png" class=""><h2 id="五、远程仓库"><a href="#五、远程仓库" class="headerlink" title="五、远程仓库"></a><strong>五、远程仓库</strong></h2><p>在了解之前，先注册github账号，由于你的本地Git仓库和github仓库之间的传输是通过SSH加密的，所以需要一点设置：<br>第一步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果有的话，直接跳过此如下命令，如果没有的话，打开命令行，输入如下命令：</p><p>ssh-keygen -t rsa –C “<a href="mailto:&#121;&#111;&#117;&#x72;&#x65;&#109;&#97;&#x69;&#x6c;&#64;&#x65;&#120;&#97;&#109;&#112;&#108;&#101;&#46;&#99;&#x6f;&#109;">&#121;&#111;&#117;&#x72;&#x65;&#109;&#97;&#x69;&#x6c;&#64;&#x65;&#120;&#97;&#109;&#112;&#108;&#101;&#46;&#99;&#x6f;&#109;</a>”, 由于我本地此前运行过一次，所以本地有，如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112607035.png" class=""><p>id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。</p><p>第二步：登录github,打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”,填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112621911.jpeg" class=""><p>点击 Add Key，你就应该可以看到已经添加的key。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112629119.png" class=""><ol><li>如何添加远程库？<br>现在的情景是：我们已经在本地创建了一个Git仓库后，又想在github创建一个Git仓库，并且希望这两个仓库进行远程同步，这样github的仓库可以作为备份，又可以其他人通过该仓库来协作。</li></ol><p>首先，登录github上，然后在右上角找到“create a new repo”创建一个新的仓库。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112703539.png" class=""><p>在Repository name填入testgit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112713032.jpeg" class=""><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前，在GitHub上的这个testgit仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。</span><br></pre></td></tr></table></figure><p>现在，我们根据GitHub的提示，在本地的testgit仓库下运行命令：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin https:<span class="regexp">//gi</span>thub.com<span class="regexp">/tugenhua0707/</span>testgit.git</span><br></pre></td></tr></table></figure><p>所有的如下：</p>  <img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112722423.png" class=""><p>把本地库的内容推送到远程，使用 git push命令，实际上是把当前分支master推送到远程。</p><p>由于远程库是空的，我们第一次推送master分支时，加上了 –u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。推送成功后，可以立刻在github页面中看到远程库的内容已经和本地一模一样了，上面的要输入github的用户名和密码如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112730461.jpeg" class=""><p>从现在起，只要本地作了提交，就可以通过如下命令：</p><p>git push origin master</p><p>把本地master分支的最新修改推送到github上了，现在你就拥有了真正的分布式版本库了。</p><ol start="2"><li>如何从远程库克隆？</li></ol><p>上面我们了解了先有本地库，后有远程库时候，如何关联远程库。</p><p>现在我们想，假如远程库有新的内容了，我想克隆到本地来 如何克隆呢？</p><p>首先，登录github，创建一个新的仓库，名字叫testgit2.如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112751184.jpeg" class=""><p>如下，我们看到：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503130335316.jpeg" class=""><p>现在，远程库已经准备好了，下一步是使用命令git clone克隆一个本地库了。如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112759396.png" class=""><p>接着在我本地目录下 生成testgit2目录了，如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112807806.png" class=""><h2 id="六、创建与合并分支"><a href="#六、创建与合并分支" class="headerlink" title="六、创建与合并分支"></a><strong>六、创建与合并分支</strong></h2><p>在 版本回填退里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。</p><p>首先，我们来创建dev分支，然后切换到dev分支上。如下操作：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112813427.png" class=""><p>git checkout 命令加上 –b参数表示创建并切换，相当于如下2条命令</p><p>git branch dev</p><p>git checkout dev</p><p>git branch查看分支，会列出所有的分支，当前分支前面会添加一个星号。然后我们在dev分支上继续做demo，比如我们现在在readme.txt再增加一行 7777777777777</p><p>首先我们先来查看下readme.txt内容，接着添加内容77777777，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112820231.png" class=""><p>现在dev分支工作已完成，现在我们切换到主分支master上，继续查看readme.txt内容如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112825871.png" class=""><p>现在我们可以把dev分支上的内容合并到分支master上了，可以在master分支上，使用如下命令 git merge dev 如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112834277.png" class=""><p>git merge命令用于合并指定分支到当前分支上，合并后，再查看readme.txt内容，可以看到，和dev分支最新提交的是完全一样的。</p><p>注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。</p><p>合并完成后，我们可以接着删除dev分支了，操作如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112840806.png" class=""><p>总结创建与合并分支命令如下：</p><p>查看分支：git branch</p><p>创建分支：git branch name</p><p>切换分支：git checkout name</p><p>创建+切换分支：git checkout –b name</p><p>合并某分支到当前分支：git merge name</p><p>删除分支：git branch –d name</p><p>如何解决冲突？<br>下面我们还是一步一步来，先新建一个新分支，比如名字叫fenzhi1，在readme.txt添加一行内容8888888，然后提交，如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112846609.png" class=""><p>同样，我们现在切换到master分支上来，也在最后一行添加内容，内容为99999999，如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112853591.png" class=""><p>现在我们需要在master分支上来合并fenzhi1，如下操作：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113753921.png" class=""><p>Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，其中&lt;&lt;&lt;HEAD是指主分支修改的内容，&gt;&gt;&gt;&gt;&gt;fenzhi1 是指fenzhi1上修改的内容，我们可以修改下如下后保存：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113746099.png" class=""><p>如果我想查看分支合并的情况的话，需要使用命令 git log.命令行演示如下：</p><p>3.分支管理策略。 通常合并分支时，git一般使用”Fast forward”模式，在这种模式下，删除分支后，会丢掉分支信息，现在我们来使用带参数 –no-ff来禁用”Fast forward”模式。首先我们来做demo演示下：</p><p>创建一个dev分支。<br>修改readme.txt内容。<br>添加到暂存区。<br>切换回主分支(master)。<br>合并dev分支，使用命令 git merge –no-ff -m “注释” dev<br>查看历史记录<br>截图如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113739405.jpeg" class=""><p>分支策略：首先master主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的dev分支上干活，干完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。</p><h2 id="七、bug分支"><a href="#七、bug分支" class="headerlink" title="七、bug分支"></a><strong>七、bug分支</strong></h2><p>在开发中，会经常碰到bug问题，那么有了bug就需要修复，在Git中，分支是很强大的，每个bug都可以通过一个临时分支来修复，修复完成后，合并分支，然后将临时的分支删除掉。</p><p>比如我在开发中接到一个404 bug时候，我们可以创建一个404分支来修复它，但是，当前的dev分支上的工作还没有提交。比如如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113729681.png" class=""><p>并不是我不想提交，而是工作进行到一半时候，我们还无法提交，比如我这个分支bug要2天完成，但是我issue-404 bug需要5个小时内完成。怎么办呢？还好，Git还提供了一个stash功能，可以把当前工作现场 ”隐藏起来”，等以后恢复现场后继续工作。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113722597.png" class=""><p>所以现在我可以通过创建issue-404分支来修复bug了。</p><p>首先我们要确定在那个分支上修复bug，比如我现在是在主分支master上来修复的，现在我要在master分支上创建一个临时分支，演示如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113659883.png" class=""><p>修复完成后，切换到master分支上，并完成合并，最后删除issue-404分支。演示如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113650665.png" class=""><p>现在，我们回到dev分支上干活了。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113640631.png" class=""><p>工作区是干净的，那么我们工作现场去哪里呢？我们可以使用命令 git stash list来查看下。如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113635358.png" class=""><p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，可以使用如下2个方法：</p><p>1.git stash apply恢复，恢复后，stash内容并不删除，你需要使用命令git stash drop来删除。<br>2.另一种方式是使用git stash pop,恢复的同时把stash内容也删除了。<br>演示如下</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113626179.png" class=""><h2 id="八、多人协作"><a href="#八、多人协作" class="headerlink" title="八、多人协作"></a><strong>八、多人协作</strong></h2><p>当你从远程库克隆时候，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且远程库的默认名称是origin。</p><ol><li>要查看远程库的信息 使用 git remote</li><li>要查看远程库的详细信息 使用 git remote –v<br>如下演示：</li></ol><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113618536.png" class=""><ol><li>推送分支：</li></ol><p>推送分支就是把该分支上所有本地提交到远程库中，推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： 使用命令 git push origin master</p><p>比如我现在的github上的readme.txt代码如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113603718.png" class=""><p>本地的readme.txt代码如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113557696.png" class=""><p>现在我想把本地更新的readme.txt代码推送到远程库中，使用命令如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113550548.png" class=""><p>我们可以看到如上，推送成功，我们可以继续来截图github上的readme.txt内容 如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113542854.png" class=""><p>可以看到 推送成功了，如果我们现在要推送到其他分支，比如dev分支上，我们还是那个命令 git push origin dev</p><p>那么一般情况下，那些分支要推送呢？</p><p>master分支是主分支，因此要时刻与远程同步。<br>一些修复bug分支不需要推送到远程去，可以先合并到主分支上，然后把主分支master推送到远程去。</p><ol start="2"><li>抓取分支：</li></ol><p>多人协作时，大家都会往master分支上推送各自的修改。现在我们可以模拟另外一个同事，可以在另一台电脑上（注意要把SSH key添加到github上）或者同一台电脑上另外一个目录克隆，新建一个目录名字叫testgit2</p><p>但是我首先要把dev分支也要推送到远程去，如下</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113533766.jpeg" class=""><p>接着进入testgit2目录，进行克隆远程的库到本地来，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113527770.png" class=""><p>现在目录下生成有如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113520571.png" class=""><p>现在我们的小伙伴要在dev分支上做开发，就必须把远程的origin的dev分支到本地来，于是可以使用命令创建本地dev分支：</p><p>git checkout –b dev origin&#x2F;dev</p><p>现在小伙伴们就可以在dev分支上做开发了，开发完成后把dev分支推送到远程库时。</p><p>如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113514177.jpeg" class=""><p>小伙伴们已经向origin&#x2F;dev分支上推送了提交，而我在我的目录文件下也对同样的文件同个地方作了修改，也试图推送到远程库时，如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113506680.jpeg" class=""><p>由上面可知：推送失败，因为我的小伙伴最新提交的和我试图推送的有冲突，解决的办法也很简单，上面已经提示我们，先用git pull把最新的提交从origin&#x2F;dev抓下来，然后在本地合并，解决冲突，再推送。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503113457994.png" class=""><p>git pull也失败了，原因是没有指定本地dev分支与远程origin&#x2F;dev分支的链接，根据提示，设置dev和origin&#x2F;dev的链接：如下：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-1548888.png" class=""><p>这回git pull成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的 解决冲突完全一样。解决后，提交，再push：<br>我们可以先来看看readme.txt内容了。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-20220503112009223.png" class=""><p>现在手动已经解决完了，我接在需要再提交，再push到远程库里面去。如下所示：</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640.png" class=""><p>因此：多人协作工作模式一般是这样的：</p><p>首先，可以试图用git push origin branch-name推送自己的修改.<br>如果推送失败，则因为远程分支比你的本地更新早，需要先用git pull试图合并。<br>如果合并有冲突，则需要解决冲突，并在本地提交。再用git push origin branch-name推送。</p><img src="/2022/06/03/Git-%E5%82%BB%E7%93%9C%E6%95%99%E7%A8%8B/640-1548877.jpeg" class="">]]></content>
      
      
      <categories>
          
          <category> git </category>
          
          <category> 教程 </category>
          
          <category> 收藏 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>elasticsearch常用命令</title>
      <link href="/2022/05/19/elasticsearch%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2022/05/19/elasticsearch%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="快速初始化密码"><a href="#快速初始化密码" class="headerlink" title="快速初始化密码"></a>快速初始化密码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elasticsearch-setup-passwords interactive</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python之虚拟隔离环境</title>
      <link href="/2022/05/15/python%E4%B9%8B%E8%99%9A%E6%8B%9F%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83/"/>
      <url>/2022/05/15/python%E4%B9%8B%E8%99%9A%E6%8B%9F%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<p>我们在python开发项目过程中不可避免的会用到外部依赖库,这些依赖库默认都安装到python安装目录下面。项目越做越多，依赖的库也会越来越多,相同的依赖库还会有多个版本,这就会导致依赖库管混乱。</p><p>为了避免依赖库管理我们可以使用python给我们提供的虚拟隔离环境(virtual environment),也叫<strong>venv</strong>。python的3.4之前默认没有提供venv模块，如果要使用venv需要安全第三方提供的venv管理工具。python3.4之后默认内置venv模块。</p><p>python内置的venv模块使用非常方便，在我们的项目根目录下面执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure><p>命令后,python会在项目目录下面创建一个 <strong>.venv</strong> （这个文件名字可以随便指定,我用.venv是因为vscode默认会加载.venv文件作为当前项目的虚拟隔离环境）的文件夹，目录结构如下图：</p><img src="/2022/05/15/python%E4%B9%8B%E8%99%9A%E6%8B%9F%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83/venv.png" class=""><p>如果在终端执行python命令你还需要手动执行如下命令：</p><figure class="highlight bash"><figcaption><span>激活</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> .venv/bin/activate</span><br></pre></td></tr></table></figure><p>来激活虚拟环境。这是激活后的终端样子：</p><img src="/2022/05/15/python%E4%B9%8B%E8%99%9A%E6%8B%9F%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83/activate.png" class=""><p>此时在执行任何命令，终端都会有个 <strong>(.venv)</strong> 标记,提示我们正在使用虚拟隔离环境来执行命令。</p><p>现在我们再用</p><figure class="highlight bash"><figcaption><span>安装依赖</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pkg名字</span><br></pre></td></tr></table></figure><p>你会发现新安装的依赖库都放在当前.venv文件夹里面。</p><p>这里需要注意的是</p><ul><li>创建的虚拟隔离环境是完全物理隔离的（不是通过快捷方式链接到系统python环境）</li><li>系统原来已经安装的pkg不会拷贝，需要在当前虚拟环境重新安装</li></ul><p>执行<code>deactivate</code>命令</p><figure class="highlight bash"><figcaption><span>反激活</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure><img src="/2022/05/15/python%E4%B9%8B%E8%99%9A%E6%8B%9F%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83/deactivate.png" class=""><p>就可以退出虚拟隔离环境使用了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> python,venv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode没有python的代码提示</title>
      <link href="/2022/05/14/vscode%E6%B2%A1%E6%9C%89python%E7%9A%84%E4%BB%A3%E7%A0%81%E6%8F%90%E7%A4%BA/"/>
      <url>/2022/05/14/vscode%E6%B2%A1%E6%9C%89python%E7%9A%84%E4%BB%A3%E7%A0%81%E6%8F%90%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[  <p>好久没用vscod写Python代码了⌨️，今天打开vscode写python代码时,发现没有代码提示，可能是vscode频繁升级和最近重新安装python环境有关。<p>  <code>Cmd</code>+<code>Shift</code>+<code>P</code>打开设置一顿折腾依然没有结果。</p><p>  看来只能求助百度了。<br>  网上有好多小伙伴和我遇到的情况一样，我按照小伙伴们的解决办法试了一个遍，vscode依旧是没有提示。<br>  就在快要放弃的时候，一个小伙伴提到</p><blockquote><p>python.defaultlangserver设置错误，导致coding python没有提示。</p></blockquote><p>  我打开自己的设置搜索”python.language”<br>  <img src="/2022/05/14/vscode%E6%B2%A1%E6%9C%89python%E7%9A%84%E4%BB%A3%E7%A0%81%E6%8F%90%E7%A4%BA/settings.png" class=""><br>  发现自己的 <strong><code>python.language.server</code><strong>设置竟然是：</strong>None</strong>。</p><p>  按照小伙伴的设置改成成<strong>Pylance</strong>,保存,熟悉代码提示又回来了。<br>  <img src="/2022/05/14/vscode%E6%B2%A1%E6%9C%89python%E7%9A%84%E4%BB%A3%E7%A0%81%E6%8F%90%E7%A4%BA/suggestion.png" class=""></p>]]></content>
      
      
      <categories>
          
          <category> vscode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python,vscode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jenkins远程ssh部署踩坑</title>
      <link href="/2022/05/07/Jenkins%E8%BF%9C%E7%A8%8Bssh%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91/"/>
      <url>/2022/05/07/Jenkins%E8%BF%9C%E7%A8%8Bssh%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91/</url>
      
        <content type="html"><![CDATA[<img src="/2022/05/07/Jenkins%E8%BF%9C%E7%A8%8Bssh%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91/jenkins-ssh.png" class="">]]></content>
      
      
      
        <tags>
            
            <tag> jenkins,devOps </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>哈希算法</title>
      <link href="/2022/05/04/%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/"/>
      <url>/2022/05/04/%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6943145184364413443/">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>哈希算法，又叫Hash算法，哈希是音译。</p><p>哈希算法从数学上讲是遵循一定映射规则集合的多对一（或一对一）的函数实现。相同的输入值必定得到相同的输出值，不同的输入值可能会得到相同的输出值，这就是哈希算法中所谓的碰撞问题。它是一种函数变换，我们把这种函数叫hash函数、散列函数、杂凑函数或哈希函数。</p><p>哈希算法，大部分情况下会把一个高维空间或无限维空间上的数据映射成一个更小的有限维空间的数据（也可以把这看成一种有信息损失的不可逆的数据压缩），所谓大道由简，大乐必易（越简单的东西越是好的），哈希算法正体现了这个道理。</p><p>哈希算法，是将任意长度的二进制值映射为较短的固定长度的二进制值,这个小的二进制值称为哈希值或散列值。</p><p>哈希算法，有一个输入和一个输出，输入任意长度的数据，在算法内部不管输入的数据是何种形式，都以单纯的二进制比特序列来处理。简单地说，哈希算法，它看到的输入就是一串由0和1组成的二进制数, 输出就是最后的哈希值或者散列值。</p><p>在安全和密码学领域，根据哈希函数计算出来的有固定长度的二进制数据，就是哈希数据指纹，可以称为定海神纹，每一串原始数据，可以根据哈希函数计算得到唯一的指纹。数据被篡改，指纹就会跟着变化（在冲撞概率发生忽略不计的条件下）。</p><h1 id="二、作用"><a href="#二、作用" class="headerlink" title="二、作用"></a>二、作用</h1><p>我们来看看哈希算法有啥用？</p><p>1&gt; 哈希值变得更为短小，可以提高存储空间的利用率。</p><p>2&gt; 哈希值变得更为短小，可以提高数据的查询效率。</p><p>3&gt; 哈希值的不变性，可以方便定位分布式服务器。</p><p>4&gt; 哈希值枚举的复杂性，用于区块链中共识算法工作量证明。</p><p>5&gt; 哈希指纹或消息摘要可以进行文件或数据校验，确保文件或数据没有被篡改。</p><p>6&gt; 与加密或数字签名结合， 来验证身份和保障数据传递的安全性， 即数字签名和鉴权。</p><p>7&gt; 哈希算法在密码学、图像、AI、机器学习、大数据、互联网如搜索引擎等领域都有广泛的应用。</p><h1 id="三、哈希函数分类"><a href="#三、哈希函数分类" class="headerlink" title="三、哈希函数分类"></a>三、哈希函数分类</h1><p>哈希函数长啥样呢？Hash算法没有一个固定的公式，只要符合散列思想的算法都可以被称为是Hash算法，所以哈希函数也是变幻万千。如有：</p><ul><li>普通哈希函数</li><li>数据结构哈希函数</li><li>工作量证明哈希函数</li><li>分布式哈希函数</li><li>加密哈希函数（MD5、SHA256等）</li><li>检索哈希函数</li><li>感知或比较哈希函数</li><li>特征匹配哈希函数</li><li>字符串哈希函数BKDRHash，APHash，DJBHash，JSHash，RSHash，SDBMHash，</li><li>PJWHash，ELFHash等等</li></ul><h1 id="四、哈希函数详解"><a href="#四、哈希函数详解" class="headerlink" title="四、哈希函数详解"></a>四、哈希函数详解</h1><h1 id="1、普通的哈希函数"><a href="#1、普通的哈希函数" class="headerlink" title="1、普通的哈希函数"></a>1、普通的哈希函数</h1><p>举一个最简单的普通哈希函数的例子，输入是电话号码，哈希函数是取中间三位，得到三位数字，这就是哈希值，其长度是3。如图4-1示：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/38d9229c5e45490b90186ad268340cdf?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图4-1</p><h1 id="2、计算机数据结构的散列函数与散列表"><a href="#2、计算机数据结构的散列函数与散列表" class="headerlink" title="2、计算机数据结构的散列函数与散列表"></a>2、计算机数据结构的散列函数与散列表</h1><p>我们再看看应用于计算机数据结构（哈希表，Hash table，也叫散列表）、存储和查询上的散列函数。这类数据结构散列函数能使一个数据序列的存储和访问更加迅速有效，数据元素能被更快地定位。</p><p>散列表，是根据关键码值对(key，value&#x3D;Hash(key))而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。它是基于快速存取的角度设计的，散列表可以理解为一个线性表，但是其中的元素不是紧密排列的，而是可能存在空隙。</p><p>散列表是算法在时间和空间上作出权衡的经典产物，如果没有内存限制，我们可以直接将关键码作为（可能是一个超大的）数组的索引，那么所有查找操作只需要访问内存一次即可完成，但这种理想情况不会经常出现，因为当键很多时需要的内存太大。另一方面，如果没有时间限制，我们可以使用无序数组并进行顺序查找，这样就只需要很少的内存。而散列表则使用了适度的空间和时间并在这两个极端之间找到了一种平衡。事实上，我们不必重写代码，只需要调整散列算法的参数就可以在空间和时间之间作出取舍,利用概率论的经典结论来帮助选择适当的参数，这又印证了天下无处不数学。</p><p>散列表的查询可以分为两步：一是用散列函数将被查找的键转化为线性表的一个索引；二是处理碰撞冲突。</p><p><strong>如何构造散列表？如何寻找散列函数？以下是最常见的方法：</strong></p><h1 id="直接寻址法"><a href="#直接寻址法" class="headerlink" title="直接寻址法"></a>直接寻址法</h1><p>取关键字或关键字的某个线性函数值为散列地址。即H(key)&#x3D;key或H(key) &#x3D; a·key + b，其中a和b为常数（这种散列函数叫做自身函数）。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e9ecd1c5b61c429daf4ecfcef0f77d55?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图4-2</p><p>如图4-2所示，T是一个最简单的散列表(哈希表)，可以看作是根据关键字直接访问内存存储位置的数据结构。根据哈希函数H(key)&#x3D;key可以直接找该关键字在T的位置，该位置存储了对应的数据地址。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/2cfd0d74775143298c448f6556bc1b42?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图4-3</p><p>如图4-3所示，它是T的一个升级版，关键字经过哈希函数计算后发生了碰撞（两个关键字映射到散列表同一个位置），解决碰撞的办法有链接法和开放地址法，上图采用的是链接法。</p><p>直接寻址技术优点：当数据量较小时，相对简单有效。</p><p>直接寻址技术缺点：</p><ul><li>当所有关键字的域集合U很大时，建立列表T，所消耗的内存空间非常大</li><li>如果U非常大，而实际出现的key非常少，这样就对空间造成了极大的浪费</li><li>当关键字key不是数字的时候就无法处理了</li></ul><h1 id="数字分析法"><a href="#数字分析法" class="headerlink" title="数字分析法"></a>数字分析法</h1><p>分析一组数据，比如一组员工的出生年月日，这时我们发现出生年月日的前几位数字大体相同，这样的话，出现冲突的几率就会很大，但是我们发现年月日的后几位表示月份和具体日期的数字差别很大，如果用后面的数字来构成散列地址，则冲突的几率会明显降低。因此数字分析法就是找出数字的规律，尽可能利用这些数据来构造冲突几率较低的散列地址。</p><h1 id="平方取中法"><a href="#平方取中法" class="headerlink" title="平方取中法"></a>平方取中法</h1><p>取关键字平方后的中间几位作为散列地址。</p><h1 id="折叠法"><a href="#折叠法" class="headerlink" title="折叠法"></a>折叠法</h1><p>将关键字分割成位数相同的几部分，最后一部分位数可以不同，然后取这几部分的叠加和（去除进位）作为散列地址。</p><h1 id="随机数法"><a href="#随机数法" class="headerlink" title="随机数法"></a>随机数法</h1><p>选择一随机函数，取关键字作为随机函数的种子生成随机值作为散列地址，通常用于关键字长度不同的场合。</p><h1 id="除留余数法"><a href="#除留余数法" class="headerlink" title="除留余数法"></a>除留余数法</h1><p>也叫除法散列法，取关键字被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 H(key) &#x3D; key MOD p, p&lt;&#x3D;m。不仅可以对关键字直接取模，也可在折叠、平方取中等运算之后取模。对p的选择很重要，若p选的不好，容易产生碰撞。p一般取素数，不应该是2的幂，不然的话，如果p&#x3D;2^n（2的n次幂），则 H(key)就是p的n个最低位数字（二进制），除非已经知道关键字的最低n位数的排列是等可能的，否则在设计散列函数时，应该考虑关键字的所有位。一个不太接近2的整数幂的素数p是一个比较好的选择。</p><h1 id="平方散列法"><a href="#平方散列法" class="headerlink" title="平方散列法"></a>平方散列法</h1><p>乘法的运算要比除法来得省时，可以考虑把除法换成乘法和一个位移操作。公式：Hash(k) &#x3D; (k * k) &gt;&gt; 28 ，对于某些输入值，结果有分布不均匀的现象。</p><h1 id="斐波那契（Fibonacci）散列法"><a href="#斐波那契（Fibonacci）散列法" class="headerlink" title="斐波那契（Fibonacci）散列法"></a>斐波那契（Fibonacci）散列法</h1><p>平方散列法的缺点是显而易见的，所以我们能不能找出一个理想的乘数，而不是拿输入本身当作乘数呢？答案是肯定的。</p><ul><li>对于16位整数而言，这个乘数是40503。</li><li>对于32位整数而言，这个乘数是2654435769。</li><li>对于64位整数而言，这个乘数是11400714819323198485。</li></ul><p>这几个“理想乘数”是如何得出来的呢？这跟一个法则有关，叫黄金分割法则，而描述黄金分割法则的最经典表达式无疑就是著名的斐波那契数列，即如此形式的序列：0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,233, 377, 610， 987, 1597, 2584, 4181, 6765, 10946，…。另外，斐波那契数列的值和太阳系八大行星的轨道半径的比例出奇吻合。</p><p>对我们常见的32位整数而言，公式： hash(k) &#x3D; (k * 2654435769) &gt;&gt; 28。</p><h1 id="乘法散列法"><a href="#乘法散列法" class="headerlink" title="乘法散列法"></a>乘法散列法</h1><p>Hash(k) &#x3D; floor(m * ((k*A) mod 1))</p><p>其中m为散列表表长，0&lt;A&lt;1, mod 1表示取出k<em>A的小数部分，也可这样计算小数部分：(k</em>A) mod 1&#x3D; k<em>A-floor(k</em>A), floor(x)表示不大于x的最大整数。</p><p>在乘法的情况中，对于m的选择与除法时刚好相反，倾向于选择2的幂，同时，会把A表示为s&#x2F;(2^w)的形式（其中w是计算机的字长,如32位）。Knuth建议采用A为0.618…，也就是我们所知的黄金分割比。通过这样的设定，这个哈希函数的执行得到了一定的简化。</p><p>举例：k &#x3D; 123456，m &#x3D; 16384（即2^14)，w &#x3D; 32 时， 可将A表示为特定的形式：A &#x3D; s&#x2F;(2^w) &#x3D; 2654435769&#x2F;(2^32)。</p><p>具体步骤如下：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/8198dd274f8544a49346bbd72bed8363?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><h1 id="全域散列法"><a href="#全域散列法" class="headerlink" title="全域散列法"></a>全域散列法</h1><p>其定义如下：随机选择散列函数，使之不依赖于要存储的关键字，其平均性能会很好。</p><p>全域散列法解决的是确定性散列算法无法应对特殊输入的问题。我们有m（为方便讨论，不妨设m远大于2）个槽时，单个好的散列函数的冲突概率是 1&#x2F;m（已经均匀散列了，但还会恰好两个掉到同一个槽里）。但是，我们可以为这个“好的”散列函数精心构造输入数据，把正好都掉到一个槽里的数拿出来作为输入，这样冲突概率达到100%了。我们要解决的问题是，对于精心构造的输入，冲突概率仍然达到 1&#x2F;m。</p><p>一个词可解决这个问题——random, 如果散列函数是随机选择的，那么精心构造的数据就不一定起作用了。这些随机选择的散列函数也是有讲究的：</p><ol><li>多少个备选散列函数才够呢？比如，两个是不够的。比如我们有两个散列函数 h1 和 h2 来随机选择，各 50% 概率被选到。那么构造一个 h1 的特殊输入（让 h1 100% 冲突），这个输入里任意两个元素仍然会有 50% 的情况一定冲突（就是 h1 被选中的概率），没有达到理想的 1&#x2F;m。</li><li>备选散列函数够多就可以吗？比如，这些函数都会在两个特殊的点上面冲突，即存在 x !&#x3D; y，使得任取 h 都有 h(x) &#x3D; h(y)，那么用这两个点作为输入，冲突概率就是100%。也就是说，这些函数冲突的地方还不能太重合。</li></ol><p>全域散列指出可以选择|H|个散列函数，且它们最大重合≤|H|&#x2F;m。其中重合是指，对任意 x !&#x3D; y，散列函数集合 H中h(x) &#x3D; h(y)的散列函数个数。随机选择散列函数后，对于精心构造的 x, y（我知道 x, y 会在某个或某些函数上冲突），能够被这个 x, y 命中的散列函数个数就会 ≤ |H|&#x2F;m，即命中概率 ≤ (|H|&#x2F;m) &#x2F; |H| &#x3D; 1&#x2F;m。也就是说，对于精心构造的输入，冲突率重新达到了 1&#x2F;m。</p><p>举例：</p><p>为防范恶意存储数据到相同槽（哈希表的同一个位置），哈希函数被设置为从哈希函数集中随机选取。</p><p>{key1，key2,…} &#x3D;&#x3D;&gt; {hash_f1, hash_f2, hash_f3,…} &#x3D;&#x3D;&gt; {value1, value2, …}</p><p>取 m 为素数，取随机数 A 为r+1位m进制数，表示为&lt;A0,A1,A2,..,Ar&gt; (其中0 &lt;&#x3D; Ai &lt;&#x3D; m-1), 将 key 转换为r+1位m进制数，表示为&lt;k0, k1,..,kr&gt; (其中0 &lt;&#x3D; ki &lt;&#x3D; m-1),</p><p>则哈希函数H<a href="key">A</a>为：</p><p>H<a href="key">A</a> &#x3D; sum{ Ai * ki, i&#x3D; 0,..r} mod m</p><p>如果r+1&#x3D;3, m&#x3D;31, A &#x3D; 8205 &#x3D; 8 * 31^2 + 16 * 31 + 21, A表示为&lt;21, 16, 8&gt;,</p><p>key &#x3D; 5206 &#x3D; 5 * 31^2 + 12 * 31 + 29, key表示为&lt;29, 12, 5&gt;,</p><p>则哈希函数值value &#x3D; H<a href="5206">8205</a>&#x3D;(21<em>29+16</em>12+8*5) mod 31&#x3D;4</p><p>如何设计一个通用的全域散列函数类？(可以证明其冲突率为 1&#x2F;m，证明过程另行介绍。)</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/f9770f670c9a488f8f77347222b54c03?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><h1 id="完全散列法（perfect-hashing"><a href="#完全散列法（perfect-hashing" class="headerlink" title="完全散列法（perfect hashing)"></a>完全散列法（perfect hashing)</h1><p>当键值是静态(即固定不变)的时候，我们可以设计方案使得最差情况下的查询性能也很出色，这就是完美哈希。</p><p>完美散列函数是一个将集合S的每个元素映射到一系列无冲突的整数的哈希函数。一个完美散列函数的应用与其他哈希函数的应用基本一致，但不需要任何冲突解决方案。在数学术语中，这是一个完全单射函数。</p><p>在静态集合S或集合S极少更新且查询频率非常多的情况下，使用完美散列函数是非常有效的。对集合S更新频率的限定是由于对任何集合S的修改，都将导致该完美散列函数退化为非完美散列函数。每次集合S被修改后自动更新hash函数的解决方案被称为dynamic perfect hashing，但这类方法非常复杂，难以实现。一个简单的允许动态更新集合S的完美散列函数的替代品叫cuckoo hashing。</p><p>最小完美散列函数是一个能将<em>n</em>个键（key）映射到<em>n</em>个连续的整数的完美散列函数。若对一个最小完美散列函数F，其应用变换后得到的值保持了键（key）的字典序列，我们称该最小完美散列函数F为单调的。</p><p>实际上，很多地方都会用到静态关键字集合。比如一种语言的保留字集合，一张CD-ROM里的文件名集合。 而完美哈希可以在最坏情况下以O(1)复杂度查找，性能非常出色的。完美哈希的思想就是采用两级的框架，每一级上都用全域哈希。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d24ab2f675f6459b8ddae5444ceab034?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>完美哈希的结构如上图。具体来说，第一级和带链表的哈希非常的相似，只是第一级发生冲突后后面接的不是链表，而是一个新的哈希表。</p><p>我们可以看到前端存储了一些哈希表的基本性质：m 哈希表槽数；a,b 全域哈希函数要确定的两个值(一般是随机选然后确定下来的)，后面跟着哈希表。为了保证不冲突，每个二级哈希表的数量是第一级映射到这个槽中元素个数的平方（在第 1 级，如果有 Ni 个元素映射到第 i 个槽，那么第 Ni 个槽对应的 2 级哈希表采用全域哈希。表的长度取 Mi &#x3D; Ni^2 ），这样可以保证整个哈希表非常的稀疏。</p><p>一次散列表中的每个槽都指向了一个二次散列表，而这个二次散列表的大小是它所存数据个数的平方</p><p><strong>如何解决冲突？</strong></p><p>在哈希表中，不同的关键字值对应到同一个存储位置的现象。即关键字K1≠K2，但H（K1）&#x3D; H（K2）。均匀的哈希函数可以减少冲突，但不能避免冲突。发生冲突后，必须解决，也即必须寻找下一个可用地址。</p><p>常用的Hash冲突解决方法有以下几种：</p><h1 id="开放定址法"><a href="#开放定址法" class="headerlink" title="开放定址法"></a>开放定址法</h1><p>这种方法也称再散列法，其基本思想是：当关键字key的哈希地址p&#x3D;H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p1为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。这种方法有一个通用的再散列函数形式：</p><p>Hi&#x3D;（H（key）+di）% m i&#x3D;1，2，…，n</p><p>其中H（key）为哈希函数，m 为表长，di称为增量序列。增量序列的取值方式不同，相应的再散列方式也不同。主要有以下三种：</p><ul><li>线性探测再散列</li></ul><p>di&#x3D;1，2，3，…，m-1。这种方法的特点是，冲突发生时，顺序查看表中下一单元，直到找出一个空单元或查遍全表。</p><p>例如：关键字集合为{12,67,56,16,25,37,22,29,15,47,48,34},表长为12。 我们用散列函数f(key) &#x3D; key mod 12。</p><p>当计算前5个数{12,67,56,16,25}时，都是没有冲突的散列地址，直接存入：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/1e7449d365f54ba1b4653583d3718f31?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>计算key &#x3D; 37时，发现f(37) &#x3D; 1，此时就与25所在的位置冲突。</p><p>于是我们应用上面的公式f(37) &#x3D; (f(37)+1) mod 12 &#x3D; 2。于是将37存入下标为2的位置。这其实就是房子被人买了于是买下一间的做法。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/947270a222424fd182b5317fcba12786?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>接下来22,29,15,47都没有冲突，正常的存入：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/477c79af06534f80b3d4c63c0cbf9201?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>到了 key&#x3D;48，我们计算得到f(48) &#x3D; 0，与12所在的0位置冲突了，不要紧，我们f(48) &#x3D; (f(48)+1) mod 12 &#x3D; 1，此时又与25所在的位置冲突。于是f(48) &#x3D; (f(48)+2) mod 12&#x3D;2，还是冲突……一直到 f(48) &#x3D; (f(48)+6) mod 12 &#x3D; 6时，才有空位，机不可失，赶快存入：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3e65d0a4426b4a0a9b045123862f88af?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>线性探测也有弊端，就是会造成元素聚集现象，降低查找效率。</p><ul><li>二次探测再散列</li></ul><p>di&#x3D;12，-12，22，-22，…，k2，-k2 ( k&lt;&#x3D;m&#x2F;2 )。这种方法的特点是，冲突发生时，在表的左右进行跳跃式探测，比较灵活。</p><ul><li>伪随机探测再散列</li></ul><p>di&#x3D;伪随机数序列。具体实现时，应建立一个伪随机数发生器（如i&#x3D;(i+p) % m），并给定一个随机数做起点。</p><p>例如，已知哈希表长度m&#x3D;11，哈希函数为：H（key）&#x3D; key % 11，则H（47）&#x3D;3，H（26）&#x3D;4，H（60）&#x3D;5，假设下一个关键字为69，则H（69）&#x3D;3，与47冲突。</p><p>如果用线性探测再散列处理冲突，下一个哈希地址为H1&#x3D;（3 + 1）% 11 &#x3D; 4，仍然冲突，再找下一个哈希地址为H2&#x3D;（3 + 2）% 11 &#x3D; 5，还是冲突，继续找下一个哈希地址为H3&#x3D;（3 + 3）% 11 &#x3D; 6，此时不再冲突，将69填入5号单元。</p><p>如果用二次探测再散列处理冲突，下一个哈希地址为H1&#x3D;（3 + 12）% 11 &#x3D; 4，仍然冲突，再找下一个哈希地址为H2&#x3D;（3 - 12）% 11 &#x3D; 2，此时不再冲突，将69填入2号单元。</p><p>如果用伪随机探测再散列处理冲突，且伪随机数序列为：2，5，9，……..，则下一个哈希地址为H1&#x3D;（3 + 2）% 11 &#x3D; 5，仍然冲突，再找下一个哈希地址为H2&#x3D;（3 + 5）% 11 &#x3D; 8，此时不再冲突，将69填入8号单元。</p><ul><li>平方探测法</li></ul><p>在平方探测法中，我们通常使用的是公式：di &#x3D; f(i)&#x3D;i^2。若使用平方探测，且表的大小为素数，则当表至少有一半空的时候，总能插入一个新的元素。</p><ul><li>双散列</li></ul><p>双散列中，我们通常用的算法是：di &#x3D; f(i)&#x3D;i * h2(x)。不得不说，这个散列方法，对于散列函数h2(x)的选择至关重要。对任意的x，h2(x) ≠ 0，探测序列还应该保证所有的散列存储单元都应该能够被探测到。选择以下形式有良好的效果：<br>h2(x) &#x3D; p - (x mod p)<br>其中：p &lt; TableSize，p、TableSize都是素数。</p><h1 id="再哈希法"><a href="#再哈希法" class="headerlink" title="再哈希法"></a>再哈希法</h1><p>这种方法是同时构造多个不同的哈希函数：</p><p>Hi&#x3D;RH1（key） i&#x3D;1，2，…，k</p><p>当哈希地址Hi&#x3D;RH1（key）发生冲突时，再计算Hi&#x3D;RH2（key）……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。</p><h1 id="链地址法"><a href="#链地址法" class="headerlink" title="链地址法"></a>链地址法</h1><p>这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。</p><p>例如，假设关键字是前10个完全平方数，hash(x)&#x3D;x%10，这里Table Size&#x3D;10不是素数，只是为了简便。如图4-4所示。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5679c6d5d2a94e79807d2623c29c6214?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图4-4</p><p>优点：</p><ul><li>无需预留多个槽位</li><li>可解决任意多次冲突</li><li>删除操作简单、统一</li></ul><p>缺点：</p><ul><li>指针需要额外空间</li><li>节点需要动态申请，开销比正常高2个数量级</li><li>空间未必连续分布，系统缓存几乎失效，对于稍大规模的词条集合，查找中将做大量的I／O操作，无法利用系统预先缓存，导致效率低下。</li></ul><h1 id="建立公共溢出区"><a href="#建立公共溢出区" class="headerlink" title="建立公共溢出区"></a>建立公共溢出区</h1><p>这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。适用于相对于基本表来说冲突数据很少的情况。</p><h1 id="桶定址法"><a href="#桶定址法" class="headerlink" title="桶定址法"></a>桶定址法</h1><p>桶：一片足够大的存储空间。桶定址：为表中的每个地址关联一个桶。如果桶已经满了，可以使用开放定址法来处理。例如，插入A5,A2,A3,B5,A9,B2,B9,C2，采用线性探查法解决冲突。如图4-5所示。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d85664a377c144b7891debd640202c6a?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>4-5</p><p><strong>如何选择散列表的载荷因子？</strong></p><p>散列表的载荷因子定义为: α&#x3D;填入表中的元素个数&#x2F;散列表的长度，α是散列表装满程度的标志因子。由于表长是定值，α与“填入表中的元素个数”成正比，所以，α越大，表明填入表中的元素越多，产生冲突的可能性就越大:反之，α越小，标明填入表中的元素越少，产生冲突的可能性就越小。实际上，散表的平均查找长度是载荷因子α的函数，只是不同处理冲突的方法有不同的函数。<br>对于开放定址法，荷载因子是特别重要因素，应严格限制在0.7-0.8以下。超过0.8，查表时的CPU缓存不命中(cache missing)按照指数曲线上升。因此，一些采用开放定址法的hash库，如Java的系统库限制了荷载因子为0.75，超过此值将resize散列表。</p><p><strong>如何评价散列函数的性能？</strong></p><p>应用于计算机数据结构、存储和查询上的散列函数，其性能评价由如下几点决定：</p><ul><li>高效性：给定输入和hash函数，在有限时间和有限资源内能计算出hash值。<br>均匀性：结果分布均匀。</li><li>一致性：相同的输入，必然产生相同的输出。</li><li>碰撞难：对于任意两个不同的输入，其hash值相同的可能性极小。</li></ul><h1 id="3、加密哈希函数"><a href="#3、加密哈希函数" class="headerlink" title="3、加密哈希函数"></a>3、加密哈希函数</h1><p>应用于安全和密码学领域的哈希函数，哈希值被称为哈希指纹或消息摘要。最常用的哈希函数如CRC16&#x2F;CRC32、MD4、MD5、SHA1&#x2F;SHA2&#x2F;SHA3、SHA256、SHA512等。</p><p>加密哈希函数是密码学中的瑞士军刀，它们在众多各具特色的应用中找到了一席之地。为了保证安全，不同的应用会要求不同的哈希函数特点。</p><p>在同一输入字符串下，这些哈希函数计算出来的哈希值长成什么样子呢？百闻不如一见，美女们闪亮登场。</p><p><strong>输入字符串：1234567890ABCDEFGHIJKLMN</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Hash函数</strong></th><th><strong>计算结果(大写十六进制表示)</strong></th><th><strong>字节长度</strong></th></tr></thead><tbody><tr><td><strong>1</strong></td><td>md5</td><td>41217C45889B5378C3DAD3879D7BFAC9</td><td>16</td></tr><tr><td><strong>2</strong></td><td>sha1</td><td>28FECDB4B82226F8B4068EADC4E32F7D4BA86DC7</td><td>20</td></tr><tr><td><strong>3</strong></td><td>sha256</td><td>60CCFA1CF448B78C4E6D5315D082EA71701199AA5D0DB2D196F650C44F049BF9</td><td>32</td></tr><tr><td><strong>4</strong></td><td>sha512</td><td>3A9FC5E91DA972B6F419EB4B61CC5CBEF8ADC019F61C6AF4797CD40CF5FDCE96840613926D5FC5AD9DC790EEAC758AFD3D86998D33B7D51B05EBBA4A0A3E5D2F</td><td>42</td></tr><tr><td><strong>5</strong></td><td>adler32</td><td>446005F7</td><td>4</td></tr><tr><td><strong>6</strong></td><td>crc32</td><td>2BA5A4CC</td><td>4</td></tr><tr><td><strong>7</strong></td><td>crc32b</td><td>35442A37</td><td>4</td></tr><tr><td><strong>8</strong></td><td>fnv132</td><td>987F7013</td><td>4</td></tr><tr><td><strong>9</strong></td><td>fnv164</td><td>24AC53E3313B3A33</td><td>8</td></tr><tr><td><strong>10</strong></td><td>fnv1a32</td><td>8AA89F4B</td><td>4</td></tr><tr><td><strong>11</strong></td><td>fnv1a64</td><td>4641D4000FE4D4AB</td><td>8</td></tr><tr><td><strong>12</strong></td><td>gost</td><td>0CF078DFB2F0BDDAF26E13C66114130C302B61FD4139744DEDB00C6883B45FE6</td><td>32</td></tr><tr><td><strong>13</strong></td><td>gost-crypto</td><td>6622751E1E8ED1CBC93101BBB12D07FDAD3C824598FE695981F7009E4968E40C</td><td>32</td></tr><tr><td><strong>14</strong></td><td>haval128,3</td><td>768EECD507B4D90848A1CACE3455BA45</td><td>16</td></tr><tr><td><strong>15</strong></td><td>haval128,4</td><td>66DB79B43D5EEC6071758050FE5341CB</td><td>16</td></tr><tr><td><strong>16</strong></td><td>haval128,5</td><td>26A6F582506729D1BE459ADF522D30F5</td><td>16</td></tr><tr><td><strong>17</strong></td><td>haval160,3</td><td>89C2F50E71740DDA03F18DD233A7B1E36425E455</td><td>20</td></tr><tr><td><strong>18</strong></td><td>haval160,4</td><td>EC82ECC1FF2AF778127E00BB03D182FA9E7FDB7A</td><td>20</td></tr><tr><td><strong>19</strong></td><td>haval160,5</td><td>7B7D1C4B4895134689FC7CDBE314996287566CA1</td><td>20</td></tr><tr><td><strong>20</strong></td><td>haval192,3</td><td>84479F9F0C0A461ED6AC1905DB096724BA99CA5D35BF7667</td><td>24</td></tr><tr><td><strong>21</strong></td><td>haval192,4</td><td>56B8277B166B35333E184213D2488E2188E734A06BE2F234</td><td>24</td></tr><tr><td><strong>22</strong></td><td>haval192,5</td><td>BC2EBD6E28177B8AD10EA4332522A0A2DE4FAB18EBC6FAB0</td><td>24</td></tr><tr><td><strong>23</strong></td><td>haval224,3</td><td>AF6CB4290B09F938E3C154373E2B9F9F9B33593CD96F5DBD7E47AEE5</td><td>28</td></tr><tr><td><strong>24</strong></td><td>haval224,4</td><td>A3476877257CD826EF326E9835C53A66A1F11B3CB211D92267BB1C1C</td><td>28</td></tr><tr><td><strong>25</strong></td><td>haval224,5</td><td>B80F6FB6D2D84A118E8292BA55BB3ACFB1ABC5B58D653A29BFD032CA</td><td>28</td></tr><tr><td><strong>26</strong></td><td>haval256,3</td><td>D6366A0AC4566823253914C24B9F299690913DF5CBDC553ABF63C17FF559399B</td><td>32</td></tr><tr><td><strong>27</strong></td><td>haval256,4</td><td>C7FC432E4451638AD96EC220EEC8B9C7BDB7E2B084419E2945B779D85C65E527</td><td>32</td></tr><tr><td><strong>28</strong></td><td>haval256,5</td><td>3CBD8C7508FB4AB25E789C3130072728728F3DBF70C84BCCDB15334E457ADC87</td><td>32</td></tr><tr><td><strong>29</strong></td><td>joaat</td><td>1B3644CA</td><td>4</td></tr><tr><td><strong>30</strong></td><td>md2</td><td>44768715E0EED35C49E41DAFE552540B</td><td>16</td></tr><tr><td><strong>31</strong></td><td>md4</td><td>1676F9685AAC70677805B48CEEFBE45A</td><td>16</td></tr><tr><td><strong>32</strong></td><td>ripemd128</td><td>803C624B40AE08285C0FD0EE8138E319</td><td>16</td></tr><tr><td><strong>33</strong></td><td>ripemd160</td><td>1E0907E2FF5EEA6986B1EF8A94100A79D61225DC</td><td>20</td></tr><tr><td><strong>34</strong></td><td>ripemd256</td><td>4A2B4E911B9426DB5E69B5038C4FA11035D28D3C6CE8680250D9D1ACCF55976B</td><td>32</td></tr><tr><td><strong>35</strong></td><td>ripemd320</td><td>CCA4AA0F2C3172AB369578E3E9A42A0C67FCD57123FC956C273008CFC58C0F02FE7507B2A7B7D1DB</td><td>40</td></tr><tr><td><strong>36</strong></td><td>sha224</td><td>6A22F8EDD6120CB384990F0654A501CE2E3C396338B6FFDF2DED3414</td><td>28</td></tr><tr><td><strong>37</strong></td><td>sha3-224</td><td>B068CD6C17B4EF2FB935003ED475D8F9552CC9293664448AAE85D4C9</td><td>28</td></tr><tr><td><strong>38</strong></td><td>sha3-256</td><td>87AFBB7C5BB40A1498FFF3D1A46244EF34527D9D9F07EA98F9D8EFC83F417D6D</td><td>32</td></tr><tr><td><strong>39</strong></td><td>sha3-384</td><td>08164F852DD20DA5F6181196AC7F9037CED86579D5606DD6CDD1AB78F7B4F50674DFB307E8C3690088630EC9EA0F52F3</td><td>48</td></tr><tr><td><strong>40</strong></td><td>sha3-512</td><td>27005A88BAEFE4E7553DDB562E3BABCB9DFE349399A910D59FA35A705A09CF9BE4ED2136ECAAFAAB022137254FAD70F5DF9E235E87E075EA4265A717888F234D</td><td>64</td></tr><tr><td><strong>41</strong></td><td>sha384</td><td>34B9C0B5400FDBF1D70767338976531C9258D7FCC7589C560C9205B613A534414C296E8A1A79FF90CD8D0B151C54C911</td><td>48</td></tr><tr><td><strong>42</strong></td><td>sha512&#x2F;224</td><td>6D95D2DFCD1BF14FE2FB8BD987965F9A2245651829399E8FDCA29869</td><td>28</td></tr><tr><td><strong>43</strong></td><td>sha512&#x2F;256</td><td>206137CE29D6C695CC94C10C184839AD01EB4F5EA0EF0F5A266A3E51899C68E5</td><td>32</td></tr><tr><td><strong>44</strong></td><td>snefru</td><td>8CB42DD89F46157AC297B732D6DA5D6E07E38C4A80A82FD4499432B9ABE5D79D</td><td>32</td></tr><tr><td><strong>45</strong></td><td>snefru256</td><td>8CB42DD89F46157AC297B732D6DA5D6E07E38C4A80A82FD4499432B9ABE5D79D</td><td>32</td></tr><tr><td><strong>46</strong></td><td>tiger128,3</td><td>5C3523D6948A2C30677D42D9AFA4FBED</td><td>16</td></tr><tr><td><strong>47</strong></td><td>tiger128,4</td><td>747C10A63EDB75E8ACAEB60D0C69E890</td><td>16</td></tr><tr><td><strong>48</strong></td><td>tiger160,3</td><td>5C3523D6948A2C30677D42D9AFA4FBED918128F7</td><td>20</td></tr><tr><td><strong>49</strong></td><td>tiger160,4</td><td>747C10A63EDB75E8ACAEB60D0C69E890266B2B40</td><td>20</td></tr><tr><td><strong>50</strong></td><td>tiger192,3</td><td>5C3523D6948A2C30677D42D9AFA4FBED918128F728496C75</td><td>24</td></tr><tr><td><strong>51</strong></td><td>tiger192,4</td><td>747C10A63EDB75E8ACAEB60D0C69E890266B2B406B596256</td><td>24</td></tr><tr><td><strong>52</strong></td><td>whirlpool</td><td>7E84DD80799BC8590249B3B2FA25578A0F5EE2835065BB6234A59BA279D49081CB0B5942AF2F7C5745FBA565085D4CBADE195043E773A0EA81DAB98F506B97DA</td><td>64</td></tr></tbody></table><p>显示更多</p><p>这类加密哈希函数具有如下特征：</p><ul><li>压缩性：任意长度的数据，算出的哈希值的长度都是固定的。</li><li>容易计算：从原数据计算出哈希值很容易。</li><li>抗逆运算：即抗原像性、单向性，要进行逆运算得付出天大的代价。</li><li>抗修改性：对原数据进行任何改动，哪怕只修改一个字节，所得到的哈希值都有很大区别。</li><li>弱抗碰撞：即抗第二原像性，已知原数据和其哈希值，想找到一个具有相同哈希值的数据(即伪造数据)是非常困难的。</li><li>强抗碰撞：即抗碰撞性，想找到两个不同数据，使他们具有相同的哈希值，是非常困难的。</li></ul><p>简而言之，这类加密哈希算法具有正向快速、逆向困难、输入敏感和冲突避免等特点。</p><h1 id="五、哈希算法的应用"><a href="#五、哈希算法的应用" class="headerlink" title="五、哈希算法的应用"></a>五、哈希算法的应用</h1><h1 id="应用一-区块链与比特币中的工作量证明：挖矿"><a href="#应用一-区块链与比特币中的工作量证明：挖矿" class="headerlink" title="应用一 区块链与比特币中的工作量证明：挖矿"></a>应用一 区块链与比特币中的工作量证明：挖矿</h1><p>挖矿就是做让矿机做一个计算量很大的数学题，即进行哈希计算。伪代码实现如图5-1所示。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d9eb34b6bfa94cb587c8f12784a45931?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图5-1</p><p>区块链的最新区块头部信息如下：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a5688e2712ec4847801b9b579d3c1f2f?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p><strong>挖矿的本质就是找到一个Nonce值，使最新区块头部信息的哈希值满足难度目标条件，伪代码中的target就是难度目标。target</strong>难度值是矿工们挖矿时的重要参考指标，它决定了矿工大约需要经过多少次哈希运算才能产生一个合法的区块。哈希函数计算的难度值对保证区块链系统的安全意义重大。</p><p>区块链协议规定，使用一个常量（targetMax）除以难度系数（difficulty），可以得到目标值（target）。显然，难度系数越大，目标值就越小，哈希的计算越耗时，采矿越难。根据协议，Nonce 是一个32位的二进制值，即最大可以到21.47亿，它是一个随机值，矿工的作用其实就是猜出 Nonce 的值，使得最新区块头的哈希值可以小于目标值。运气好，也许很快就找到了 Nonce，因此采矿具有随机性，没法保证正好十分钟产出一个区块，有时一分钟就算出来了，有时几个小时也没结果。难度值必须根据全网矿工算力的变化进行调整，为了将最新区块产出的速率稳定在十分钟左右，协议设计了难度系数的动态调节机制，难度系数定期（假定是每两周）调整一次。如果这段时间内，最新区块的平均生成速度是9分钟左右，难度系数就要调高10%；如果平均生成速度是11分钟左右，难度系数就要调低10%。</p><p>参与挖矿的矿主只有购买更多更快的矿机，才能在单位时间内提高运算速度，有更多的机会去尝试那个随机数Nonce，从而获得比特币的奖励,现在比特币的价格涨到几万美元&#x2F;枚，是不是很心动呢？</p><p>就整合进区块链协议的哈希算法而言，比较早的比特币选择了 SHA256 ，而以太坊采用了改进后的 SHA3（KECCAK256）作为工作量证明算法。对于采用工作量证明的区块链来说，选择哈希函数的一大重要标准是哈希运算效率。有趣的是，比特币工作量证明协议需要重复运行两遍 SHA256 算法。为什么是双重SHA256？这不是为了抵御生日攻击，毕竟在 hash(x) &#x3D; hash(y) 的情况下，hash(hash(x)) &#x3D; hash(hash(y)) ，双重 SHA256 旨在抵御长度扩展攻击。从本质上来说，所谓的长度扩展攻击，指的是如果恶意攻击者知道了某个哈希输入的长度，就可以在哈希值上添加一个秘密的字符串、欺骗哈希函数从其内部状态的一个特定部分开始计算。作为 SHA2 算法家族的一员，SHA256 也存在这一缺陷。因此，比特币采取执行两遍哈希计算的方式来解决这一缺陷。</p><h1 id="应用二-分布式服务器架构：负载均衡"><a href="#应用二-分布式服务器架构：负载均衡" class="headerlink" title="应用二 分布式服务器架构：负载均衡"></a>应用二 分布式服务器架构：负载均衡</h1><h1 id="负载均衡主要解决用户只需一个域名或IP地址就能透明地访问本地多台服务器，或者访问离自己距离最近的服务器获得最快的访问速度。负载均衡算法有很多，比如轮询法、随机法、最小连接法、加权轮询法等。那如何才能实现一个会话粘滞（session-sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。"><a href="#负载均衡主要解决用户只需一个域名或IP地址就能透明地访问本地多台服务器，或者访问离自己距离最近的服务器获得最快的访问速度。负载均衡算法有很多，比如轮询法、随机法、最小连接法、加权轮询法等。那如何才能实现一个会话粘滞（session-sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。" class="headerlink" title="负载均衡主要解决用户只需一个域名或IP地址就能透明地访问本地多台服务器，或者访问离自己距离最近的服务器获得最快的访问速度。负载均衡算法有很多，比如轮询法、随机法、最小连接法、加权轮询法等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。"></a>负载均衡主要解决用户只需一个域名或IP地址就能透明地访问本地多台服务器，或者访问离自己距离最近的服务器获得最快的访问速度。负载均衡算法有很多，比如轮询法、随机法、最小连接法、加权轮询法等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。</h1><p>最直接的方法，就是维护一张映射关系表，这张表的内容是客户端IP地址和会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：</p><ul><li>如果客户端很多，映射表可能会很大，比较浪费内存空间；</li><li>客户端上线、下线，服务器扩容、裁剪都会导致映射失效，这样维护映射表的成本会很大；</li></ul><p>如果借助哈希算法，这些问题都可以非常完美地解决。我们可以通过哈希算法，对客户端IP地址和会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个分布式服务器上。</p><h1 id="应用三-分布式缓存与一致性哈希算法"><a href="#应用三-分布式缓存与一致性哈希算法" class="headerlink" title="应用三 分布式缓存与一致性哈希算法"></a>应用三 分布式缓存与一致性哈希算法</h1><p>数字经济时代，为了应付海量的数据，提高数据的读取、写入能力，一般都采用分布式的方式来缓存数据，这就需要将不同的缓存对象按照相应的hash算法映射到相应的机器上去，例如：key&#x3D;36, 总服务器数是N&#x3D;25，key%N&#x3D;11, 即哈希值key为36对应的缓存对象将存储到ID&#x3D;11的服务器上。那么当添加一台机器或者是其中某一台机器宕机之后，总服务器数N发生了变换，需要将缓存清空，然后重新将内容hash运算映射到所有的机器上，这样的代价是巨大的。</p><p>一致性哈希算法在1997年由麻省理工学院提出，是一种特殊的哈希算法，目的就是解决分布式缓存的这个问题。</p><p>一致性哈希算法将整个哈希值空间映射成一个虚拟的圆环，整个哈希空间的取值范围为（0，2^32-1）。整个空间按顺时针方向组织，0与2^32重合。接下来使用本算法对服务请求进行映射，将服务请求使用哈希算法算出对应的hash值，然后根据hash值的位置沿圆环顺时针查找，第一台遇到的服务器就是所对应的处理请求服务器。当增加一台新的服务器，受影响的数据仅仅是新添加的服务器到其环空间中前一台的服务器（也就是顺着逆时针方向遇到的第一台服务器）之间的数据，其他都不会受到影响。综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。如图5-1所示：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/141d5b296b264cb5ac9b6c27f986749c?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>图5-1</p><p>将服务器的标识（如IP地址）作为哈希函数的Key映射到环上。如：hash(Node1) &#x3D;Key1，hash(Node2) &#x3D; Key2。缓存对象object1的哈希值key1按顺时针靠近NODE1,它就会存储到服务器（NODE1）上。为了解决数据分配不均衡的问题, 还会引入虚拟机器节点。</p><h1 id="应用四-图片标识与快速查找"><a href="#应用四-图片标识与快速查找" class="headerlink" title="应用四 图片标识与快速查找"></a>应用四 图片标识与快速查找</h1><p>如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？</p><p>比较笨的办法就是直接进行图片文件二进制流比较，如果相同，则说明图片在图库中存在。但是，每个图片小则几十KB、大则几十MB，比对起来非常耗时。有没有比较快的方法呢？</p><p>我们可以给每一个图片通过哈希算法（比如MD5），得到一个哈希指纹，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。</p><p>如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。</p><p>如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。</p><h1 id="应用五-数据分片"><a href="#应用五-数据分片" class="headerlink" title="应用五 数据分片"></a>应用五 数据分片</h1><h1 id="1-如何快速检索几百亿张图片？"><a href="#1-如何快速检索几百亿张图片？" class="headerlink" title="1 如何快速检索几百亿张图片？"></a>1 如何快速检索几百亿张图片？</h1><p>如果图库中有几百亿张海量图片，很显然，为了考虑网络访问压力和负载均衡，考虑单台机器CPU计算能力、构建哈希表所需的物理内存限制等，都需要将图库进行分布式存储，即数据分片。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表，每次从图库中读取一个图片，利用哈希函数（如MD5）计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号k，然后去编号k的机器构建的散列表中查找。散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制，可以降低网络访问压力，达到快速检索海量图片的目的。</p><h1 id="2-如何统计关键词出现的次数？"><a href="#2-如何统计关键词出现的次数？" class="headerlink" title="2. 如何统计关键词出现的次数？"></a>2. 如何统计关键词出现的次数？</h1><p>假如我们有 1P的日志文件，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？</p><p>我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。</p><p>针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，我们用 n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个关键词key，并且通过哈希函数（ Hash(Key)&#x3D;（(Key的首字母序号)*100+(Key的尾字母序号)）Mod n）计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。哈希值相同的关键词就被分配到了同一个机器上，每个机器会分别计算各关键词出现的次数，最后合并起来就是最终的结果。</p><p>实际上，这里的处理过程也是 MapReduce 的基本设计思想。</p><h1 id="应用六-文件数据校验"><a href="#应用六-文件数据校验" class="headerlink" title="应用六 文件数据校验"></a>应用六 文件数据校验</h1><p>我们从网站下载文件时，网站经常会同时提供CRC32或MD5指纹字符串。这是为了防止网络传输出错或者文件被恶意串改。我们下载到本地后，可以计算下载文件的CRC32或MD5值，然后与网站提供的指纹字符串进行比较，来验证文件的完整性。</p><h1 id="应用七-HMAC的实现"><a href="#应用七-HMAC的实现" class="headerlink" title="应用七 HMAC的实现"></a>应用七 HMAC的实现</h1><p>HMAC (Hash-based Message Authentication Code) 是一种利用密钥和哈希函数来进行消息认证的一种机制，HMAC支持的哈希函数有 md5、sha1、sha256、sha512、adler32、crc32、gost等。它所能提供的消息认证包括两方面内容：</p><p>①消息完整性认证：能够证明消息内容在传送过程没有被修改。</p><p>②信源身份认证：因为通信双方共享了认证的密钥，接收方能够认证发送该数据的信源与所宣称的一致。</p><p>比如你和对方共享了一个密钥K，现在你要发消息给对方，既要保证消息没有被篡改，又要能证明信息确实是你本人发的，那么就把原信息和使用K计算的HMAC的值一起发过去。对方接到之后，使用自己手中的K把消息计算一下HMAC，如果和你发送的HMAC一致，那么可以认为这个消息既没有被篡改也没有冒充。</p><p>HMAC是当前许多安全协议所选用的提供认证服务的方式，应用十分广泛，并且经受住了多种形式攻击的考验，HMAC 常用于互联网API接口签名验证。各大银行、BAT大厂的后台和前端软件基本上用到了HMAC。</p><h1 id="应用八-消息摘要与数字签名验证"><a href="#应用八-消息摘要与数字签名验证" class="headerlink" title="应用八 消息摘要与数字签名验证"></a>应用八 消息摘要与数字签名验证</h1><p>密码学中,哈希函数与RSA公钥、私钥一起使用实现数字签名验证。先利用哈希函数对电子合同或文件生成消息摘要，然后利用RSA对消息摘要进行加密，其中哈希函数的作用是防止文件数据被篡改，如果再有一个第三方的认证机构，还可以防止合同或文件作者的“抵赖”，这就是所谓的数字签名的应用。</p><h1 id="应用九-布隆过滤器"><a href="#应用九-布隆过滤器" class="headerlink" title="应用九 布隆过滤器"></a>应用九 布隆过滤器</h1><p>布隆过滤器（Bloom Filter）是1970年由布隆提出的，它实际上是一个很长的二进制向量(位阵列)和一系列随机映射函数。位阵列长得像这样：000100100001000001000000。</p><p>布隆过滤器可以用于检索一个元素是否在一个集合中，它的优点是空间效率和查询时间都比一般的算法要好的多。它通过m个Hash函数将一个元素映射成一个位阵列（Bit array）中的m个点。例如，将一个字符串经过m个hash函数计算得到m个hash值，m个hash值代表了位阵列中的m个位置，将位阵列中 m个位置的位设为1. 查询时只要检测到这些位都为1，则证明该字符串找到了。</p><p>布隆过滤器认为不在的，一定不会在集合中；布隆过滤器认为在的，可能在也可能不在集合中。</p><p>比较简单的hash函数组实现如下：</p><figure class="highlight nim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">int</span> hash(<span class="type">int</span> randSeed, <span class="type">int</span> bitArraySize, <span class="type">String</span> key) &#123;</span><br><span class="line">  <span class="type">int</span> <span class="built_in">result</span> = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> len = key.length();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    <span class="built_in">result</span> = randSeed * <span class="built_in">result</span> + key.charAt(i);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> (bitArraySize - <span class="number">1</span>) &amp; <span class="built_in">result</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><p>randSeed — 随机种子，不同的hash函数，这个随机种子就不同。</p><p>bitArraySize — 位阵列长度。</p><p>Key — 用于检索的元素，这里是字符串。</p><p>输出：位阵列中置1的位置。</p><p>布隆过滤器广泛应用于网页URL的去重、垃圾邮件的判别、集合重复元素的判别、查询加速（比如基于key-value的存储系统）、Redis数据库防止查询时缓存穿透和减少不存在的行或列的磁盘查找等。</p><h1 id="应用十-java集合中的hashmap和hashcode"><a href="#应用十-java集合中的hashmap和hashcode" class="headerlink" title="应用十 java集合中的hashmap和hashcode"></a>应用十 java集合中的hashmap和hashcode</h1><p>hashmap是java集合的实现之一，其利用hash函数创建数据结构哈希表，该哈希表&#x3D;位桶（可以看作是数组）+链表(或红黑树），主要作用是提高大数据的检索效率。传统的数组或链表线性结构，查找数据的方式是线性检索，即把所有数据都遍历一遍，对大数据而言效率极低，其时间复杂度为O(n)；二分搜索要求数据排列有序，每次只用查找集合中一半的元素。其时间复杂度为O(logn)；Hash表闪亮登场，这是一种时间复杂度为O(1)的检索，就是说不管你数据有多少只需要查一次就可以找到目标数据。</p><p>Hash冲突的策略是相同的hash值组成链表，链表太长就转换成红黑树。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/b3ada031ee3648f582579c1e42e2361e?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>HashMap高度依赖的 hashcode 和 hash 算法。</p><p>Java中String对象的计算hashcode方法：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="type">int</span> <span class="title">hashCode</span><span class="params">(<span class="type">String</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> val[] = value;</span><br><span class="line">    <span class="type">int</span> h=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; value.length; i++) &#123;</span><br><span class="line">        h = <span class="number">31</span> * h + val[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> h;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>选择31相乘的理由：31不大，可以防止乘法结果溢出；31&#x3D;2^5-1,且是素数，可以将乘法操作变成与的操作，加快运算速度，素数可以让结果均匀分布。</p><p>hash函数可以是hash(h)&#x3D; h%size, size是位桶的长度,这个长度可以随数据量增大动态扩容。</p><h1 id="应用十一-检索中的各类Hash及其应用"><a href="#应用十一-检索中的各类Hash及其应用" class="headerlink" title="应用十一 检索中的各类Hash及其应用"></a>应用十一 检索中的各类Hash及其应用</h1><h1 id="TF-IDF-Hashing-TF-and-IDF"><a href="#TF-IDF-Hashing-TF-and-IDF" class="headerlink" title="TF-IDF(Hashing TF and IDF)"></a>TF-IDF(Hashing TF and IDF)</h1><p>“词频－逆向文件频率”（TF-IDF）是一种在文本挖掘中广泛使用的特征向量化方法，它可以体现一个文档中词语在语料库中的重要程度。内容的搜索问题其本质上即为Top N排序问题。对于排序的准则主要包括动态相关性和静态相关性这两个方面。其中动态相关性是指与查询词的相关性，如TF-IDF。而静态相关性即表示查询对象的质量（权威性），如PageRank。</p><h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><p>作为Google战胜Yahoo的秘密之一，大名鼎鼎的PageRank算法首当其冲。Pagerank是Google排名运算法则(排名公式)的一部分,是Google用于用来标识网页的等级&#x2F;重要性的一种方法,是Google用来衡量一个网站的好坏的唯一标准。</p><p>其核心思想即认为某一链接的质量由其本身即指向它的连接共同决定。它是一种与与查询无关的静态算法，即其最终排序结果可提前计算给出，能够较好的满足实时性的需求。而该算法的缺点则是，其最终结果与主题无关，且旧网页的排名往往要比新网页高。</p><h1 id="局部敏感哈希（LSH）"><a href="#局部敏感哈希（LSH）" class="headerlink" title="局部敏感哈希（LSH）"></a>局部敏感哈希（LSH）</h1><p>局部敏感哈希是非常简单的一种哈希函数，其通过无监督学习生成。由于结构、运算简单而在许多领域得到广泛应用。尤其是当编码位数较长时其效率有明显的提高，因此在很多任务中LSH是我们必须尝试的编码之一。LSH通过将原始数据进行随机投影后经过符号函数得到Hash编码，其表达式如下：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/959e2c3911e941c1926dfd6079ca712c?from=pc" alt="流行算法：哈希算法 - 比特币就靠她了"></p><p>上式中 X为原始数据， W为变换矩阵， B为Hash编码。可以看出LSH真的是非常简洁，这也充分验证了“Simple is beauty.”这句名言。</p><h1 id="语义哈希（Semantic-Hashing）"><a href="#语义哈希（Semantic-Hashing）" class="headerlink" title="语义哈希（Semantic Hashing）"></a>语义哈希（Semantic Hashing）</h1><p>Semantic hashing由Hinton于2007年提出，其主要是利用深度受限制玻尔兹曼机RBMs，学习Hash编码。</p><h1 id="谱哈希（Spectral-Hashing-SH）"><a href="#谱哈希（Spectral-Hashing-SH）" class="headerlink" title="谱哈希（Spectral Hashing, SH）"></a>谱哈希（Spectral Hashing, SH）</h1><p>谱哈希是哈希函数族中的又一经典代表，其天才的将哈希编码过程转化为图像分割问题，进而通过放松约束条件将分割问题转化成拉普拉斯特征图的降维问题，从而求解原问题得到图像数据的哈希编码。</p><p>此外，类似还有在线哈希学习（Online Hashing）、迭代量化哈希（Iterative Quantization, ITQ）、PCA-ITQ、PCA-RR、SKLSH等检索哈希函数。</p><h1 id="应用十二-基于哈希的特征匹配"><a href="#应用十二-基于哈希的特征匹配" class="headerlink" title="应用十二 基于哈希的特征匹配"></a>应用十二 基于哈希的特征匹配</h1><p>通过Hash特征匹配，主要能解决图像搜索、三维重建、图像分类、目标识别等问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">unsigned <span class="built_in">int</span> SDBMHash(char *<span class="built_in">str</span>)</span><br><span class="line">&#123;</span><br><span class="line">    unsigned <span class="built_in">int</span> <span class="built_in">hash</span> = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (*<span class="built_in">str</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        // equivalent to: <span class="built_in">hash</span> = <span class="number">65599</span>*<span class="built_in">hash</span> + (*<span class="built_in">str</span>++);</span><br><span class="line">        <span class="built_in">hash</span> = (*<span class="built_in">str</span>++) + (<span class="built_in">hash</span> &lt;&lt; <span class="number">6</span>) + (<span class="built_in">hash</span> &lt;&lt; <span class="number">16</span>) - <span class="built_in">hash</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">hash</span> &amp; <span class="number">0x7FFFFFFF</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>tomcat缓存不足问题</title>
      <link href="/2022/05/04/tomcat%E7%BC%93%E5%AD%98%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/"/>
      <url>/2022/05/04/tomcat%E7%BC%93%E5%AD%98%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>最近部署项目启动服务器后，控制台重复报错警告:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Unable to add the resource at [/WEB-INF/lib/xpp3_min-<span class="number">1.1</span>.4c.jar] to the </span><br><span class="line">cache because there was insufficient  free space available after evicting </span><br><span class="line">expired cache entries - consider increasing the maximum size of the cache  </span><br></pre></td></tr></table></figure><p>解决办法 ：修改context.xml在任意位置增加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">Resources</span> <span class="attr">cachingAllowed</span>=<span class="string">&quot;true&quot;</span> <span class="attr">cacheMaxSize</span>=<span class="string">&quot;100000&quot;</span> /&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>cacheMaxSize根据你自己项目大小去设置</p>]]></content>
      
      
      
        <tags>
            
            <tag> tomcat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch网络设置</title>
      <link href="/2022/05/03/Elasticsearch%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/"/>
      <url>/2022/05/03/Elasticsearch%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Elasticsearch 缺省情况下是绑定 localhost。对于本地开发服务是足够的（如果你在相同机子上启动多个节点，它还可以形成一个集群），但是你需要配置<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#common-network-settings">基本的网络设置</a>，为了能够在实际的多服务器生产集群中运行。<br><em><strong>WARNING</strong></em>：<strong>注意网络配置，永远不要暴露未受保护的节点到公网上</strong></p><h2 id="常用的网络配置"><a href="#常用的网络配置" class="headerlink" title="常用的网络配置"></a>常用的网络配置</h2><p><strong>network.host</strong><br>节点将绑定到一个主机名或者 ip 地址并且会将该这个节点通知集群中的其他节点。接受 ip 地址，主机名，指定值或者包含这些值的数组<br>默认值：<em>local</em><br><strong>discovery.zen.ping.unicast.hosts</strong><br>为了加入集群，一个节点至少需要知道集群中其他节点的主机名或者 ip 地址。这个设置提供初始其他节点列表，当前节点将尝试联系。接收 ip 地址或者主机名。<br>默认值：[“127.0.0.1”, “[::1]”]<br><strong>http.port</strong><br>HTTP 请求通信端口。接收单值或者一个范围。如果指定一个范围，该节点将会绑定范围的第一个可用顶点。<br>默认值：9200-9300<br><strong>transport.tcp.port</strong><br>节点间通信端口。接收单值或者一个范围。如果指定一个范围，该节点将会绑定范围的第一个可用顶点。<br>默认值：9300-9400</p><h2 id="network-host-的特殊值"><a href="#network-host-的特殊值" class="headerlink" title="network.host 的特殊值"></a>network.host 的特殊值</h2><p>以下特殊值将可以传递给 <strong>network.host</strong>：</p><ul><li><em>[networkInterface]</em> 网络接口的地址，例如 <em>en0</em>。</li><li><em>local</em> 系统中的回路地址，例如 127.0.0.1。</li><li><em>site</em> 系统中任何的本地站点地址，例如 192.168.0.1。</li><li><em>global</em> 系统中的任何全局作用域地 8.8.8.8。</li></ul><h3 id="IPv4-vs-IPv6"><a href="#IPv4-vs-IPv6" class="headerlink" title="IPv4 vs IPv6"></a>IPv4 vs IPv6</h3><p>默认情况下这些特殊值都可以在 IPv4 和IPv6 中使用，但是你可以使用 :ipv4，:ipv6 字符限制使用。例如，<em>en0:ipv4</em> 将绑定 en0 接口的 IPv4 地址。<br><em><strong>Tip</strong></em>：<strong>在云上使用，更多特别设定可用，当你在 AWS 云或者 Google Compute Engine 云上使用时</strong></p><h2 id="高级网络配置"><a href="#高级网络配置" class="headerlink" title="高级网络配置"></a>高级网络配置</h2><p>在<a href="https://www.cnblogs.com/xiaoheike/p/5750222.html#%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">常用的网络配置</a>中解释的 network.host 是快捷方式，同时设置绑定地址和发布地址。在高级使用情况下，例如在一个代理服务器中运行，你可能需要设置如下不同的值：<br><strong>network.bind_host</strong><br>这将指定用于监听请求的网络接口。一个节点可以绑定多个接口，例如有两块网卡，一个本地站点地址，一个本地地址。<br>默认值：network.host<br><strong>network.publish_host</strong><br>发布地址，一个单一地址，用于通知集群中的其他节点，以便其他的节点能够和它通信。当前，一个 elasticsearch 节点可能被绑定到多个地址，但是仅仅有一个发布地址。如果没有指定，这个默认值将为 network.host 配置中的最好地址，以 IPv4&#x2F;Ipv6 堆栈性能，之后以稳定性排序。<br>上述两个设置可以向 network.host 那样被设置–他们都接受 IP 地址，主机名和特定值</p><h2 id="高级-TCP-设置"><a href="#高级-TCP-设置" class="headerlink" title="高级 TCP 设置"></a>高级 TCP 设置</h2><p>任何使用 TCP（像 HTTP 和 Transport 模块）共享如下设置：</p><ul><li>network.tcp.no_delay 开启或关闭 TCP 无延迟设置。默认值为 true。</li><li>network.tcp.keep_alive 开启或关闭 TCP 长连接，默认值为 true。</li><li>network.tcp.reuse_address 一个地址是否可以被重用。在非 windows 机子上默认值为 true。</li><li>network.tcp.send_buffer_size TCP 发送缓冲区大小（以<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#size-units">size unit</a>指定）。没有默认值。</li><li>network.tcp.receive_buffer_size TCP 接收缓冲区大小（以<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#size-units">size unit</a>指定）。没有默认值。</li></ul><h2 id="Transport-和-HTTP-协议"><a href="#Transport-和-HTTP-协议" class="headerlink" title="Transport 和 HTTP 协议"></a>Transport 和 HTTP 协议</h2><p>一个Elasticsearch节点暴露两个网络协议配置继承上面的设置，但可独立地进一步配置两个网络协议：<br><strong>TCP Transport</strong><br>用于集群中节点之间的通信。<br><strong>HTTP</strong><br>暴露基于 HTTP JSON 请求接口，被所有客户端使用，比局限于 Java 客户端。</p><p>翻译自：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html</a></p><p>转自：<a href="https://www.cnblogs.com/xiaoheike/p/5750222.html">Elasticsearch Network Settings - xiaoheike - 博客园 (cnblogs.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>局部搜索算法</title>
      <link href="/2022/04/27/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
      <url>/2022/04/27/%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6941705589646950924">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>局部搜索算法(Local Search Algorithm)是一种用于解决最优化问题的启发式算法，是一类近似算法（Approximate Algorithms）的通称。它从一个(或一组)初始解出发,通过邻域函数生成解的邻域,再在邻域中搜索出更优的解来替换当前解, 使目标函数逐步优化，通过不断地迭代过程实现解的最优化。</p><h1 id="二、基本思想与分类"><a href="#二、基本思想与分类" class="headerlink" title="二、基本思想与分类"></a>二、基本思想与分类</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/baa15f1d953e44fabb544ce8a5acfd0b?from=pc" alt="流行算法：局部搜索算法"></p><p>图2-1</p><p>局部搜索算法是一种启发式搜索算法，在搜索过程中，始终选择当前点的邻居中向着离目标最接近的方向搜索。目标可以是最大值，也可以是最小值。如果存在解，最优的局部搜索算法总能找到全局最大&#x2F;最小值。如图2-1，我们的目标是搜索全局最大值。</p><p>启发式搜索是与盲目搜索相比而言的，盲目搜索采用的方法，如穷举法、全局随机法等。考察一个算法的性能通常用局部搜索能力和全局收敛能力这两个指标。局部搜索能力是指能够无穷接近最优解的能力，而全局收敛能力是指找到全局最优解所在大致位置的能力。局部搜索能力和全局收敛能力，缺一不可。</p><p>不同局部搜索算法的区别就在于：邻域动作的定义和选择邻居解的策略。这也是决定搜索算法集中性和发散性（Intensification and Diversification）好坏的关键。</p><p>局部搜索算法分类：</p><p>1&gt; 爬山法（Hill-climbing）。其主要缺点是可能会陷入局部最优解，而不一定能搜索到全局最优解。</p><p>针对局部邻域搜索，为了实现全局优化，有：</p><p>2&gt; 模拟退火法（Simulated annealing）。以可控性概率接受劣解来逃逸局部极小。</p><p>3&gt; 禁忌搜索法（Tabu&#x2F;Taboo Search, TS）。确定性的局部极小跳跃策略。</p><p>4&gt; 变邻域搜索法（Variable neighborhood search，VNS）。</p><p>采用多个不同的邻域进行系统搜索。首先采用最小的邻域搜索，如果无法改进解，则切换到稍大一点的邻域。如果能继续改进解，则退回到最小的邻域，否则继续切换到更大的邻域。</p><p>5&gt; 局部剪枝搜索法。</p><p>6&gt; 进化算法。多点并行搜索。</p><p>7&gt; 遗传算法等。</p><h1 id="三、局部搜索算法优点"><a href="#三、局部搜索算法优点" class="headerlink" title="三、局部搜索算法优点"></a>三、局部搜索算法优点</h1><p>局部搜索从当前结点出发，通常只移动到它的邻近状态，不保留路径，根据目标函数寻找最优的状态。局部搜索算法的优点有：</p><ul><li>将指数时间化为确定性时间，寻求近似解逼近最优解。</li><li>只用很少的内存。通常情况下，局部搜索不保留搜索过程中经过的路径，换句话说，它不关心实现的路径，而全局搜索则必须保留路径。</li><li>可对大量NP问题求近似解。</li><li>在连续的并且状态空间很大甚至无限的问题中，局部搜索通常都可以找到足够好的解，但是这样的问题不适合用全局搜索算法解决。</li><li>算法通用易实现，且容易理解。</li></ul><h1 id="四、对NP问题求近似解"><a href="#四、对NP问题求近似解" class="headerlink" title="四、对NP问题求近似解"></a>四、对NP问题求近似解</h1><p>在计算机领域，一般可以将问题分为可解问题和不可解问题。不可解问题也可以分为两类：一类如停机问题，的确无解；另一类虽然有解，但时间复杂度很高。可解问题也分为P问题(Polynomial Problem，多项式问题)和NP问题(Nondeterministic Polynomial Problem，非确定性多项式问题)。</p><p>P问题成了区别问题是否可以被计算机求解的一个重要标志。</p><p>NP问题是指一个复杂问题不能确定是否在多项式时间内找到答案，但是可以在多项式时间内验证答案是否正确。NP类问题数量很大，如完全子图问题、图着色问题、旅行商问题等。</p><p>对NP问题找最优解很困难，时间复杂度都很高，即求解需要大量时间。通常它们的时间复杂度都是指数变量，如</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b209d77309a647f9a1b7a23b2d6a0736?from=pc" alt="流行算法：局部搜索算法"></p><p>常见的算法时间复杂度由小到大依次为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7de4150878cb44febc0dbb64e4b01c02?from=pc" alt="流行算法：局部搜索算法"></p><p>时间复杂性函数比较（10亿次&#x2F;秒）</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/435697f779294e5fbc60baec06ea9089?from=pc" alt="流行算法：局部搜索算法"></p><p>对于某些计算起来非常复杂的最优化问题，比如各种NP完全问题，要找到最优解需要的时间随问题规模呈指数增长，因此诞生了各种启发式算法来退而求其次寻找次优解，是一种近似算法，以时间换精度的思想。局部搜索算法就是其中的一种启发式算法。</p><h1 id="五、邻域的概念"><a href="#五、邻域的概念" class="headerlink" title="五、邻域的概念"></a>五、邻域的概念</h1><p>局部搜索算法，依赖于对解空间进行按邻域搜索。</p><p>描述算法时需用到邻域的概念。所谓邻域，简单地说就是一个点附近的其它点的集合。</p><p>在距离空间，邻域就是以某一点为中心的圆。</p><p>在组合优化问题中，邻域一般定义为由给定转化规则对给定的问题域上每结点进行转化所得到的问题域上结点的集合。公式描述如下：</p><p>设D是问题的定义域，若存在一个映射N，使得：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/658d40c0164a43efbe62eb0f7b64ef4e?from=pc" alt="流行算法：局部搜索算法"></p><p>则称P&#x3D; N(S)为S的邻域。 A∈2^D为S的邻居。</p><p>邻域动作是一个函数，通过这个函数，对当前解s，产生其相应的邻居解集合。</p><p><strong>例子1：bool型问题</strong></p><p>对解空间进行编码，当前解为s &#x3D; 1001，当将邻域动作定义为翻转其中一个bit时，得到当前解的邻域N(s)&#x3D;{0001,1101,1011,1000}，其中N(s) ∈ 2^D。同理，当将邻域动作定义为互换相邻bit时，得到当前解的邻域P &#x3D; N(s)&#x3D;{0101,1001,1010}。</p><p><strong>例子2：4皇后问题</strong></p><p>如何将 4 个皇后放置在 4×4 的棋盘上，并且使皇后彼此之间不能相互攻击？（在4×4格的国际象棋上摆放4个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上。）</p><p>如图5-1所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/be7f0cc8186a47758345392331921ac2?from=pc" alt="流行算法：局部搜索算法"></p><p>图5-1</p><p>S&#x3D;{Si}表示一个可能解，其中Si表示在第i行，第Si列有一个皇后。</p><p>如四皇后问题的一个解：S&#x3D;(4, 3, 1, 2)。（这里S表示第1行第4列有一个皇后；第2行第3列有一个皇后；第3行第1列有一个皇后；第4行第2列有一个皇后。）</p><p>定义映射N为棋盘上任意两个皇后的所在行或列进行交换，即S中任意两个元素交换位置。</p><p>例：当S &#x3D; (4, 3, 1, 2)时，其邻域为：</p><p>P &#x3D; N(S) &#x3D; {(3, 4, 1, 2), (1, 3, 4, 2), (2, 3, 1, 4), (4, 1, 3, 2), (4, 2, 1, 3), (4, 3, 1, 2)}</p><p><strong>例子3：旅行商问题(TSP, Traveling Salesman Problem)</strong></p><p>一个商品推销员要去9个城市推销商品，该推销员从一个城市出发，需要经过所有城市后，回到出发地。应如何选择行进路线，以使总的行程最短？</p><p>对解空间按城市编号进行编码，设初始解为s &#x3D; {2，8，3，5，9，1，4，6，7}。</p><p>当前状态的状态产生函数N(S),即邻域可以定义为任意一种方式：</p><p>1）互换操作，随机交换两个城市的顺序；</p><p>如图5-2所示。新解s &#x3D; {2，8，1，5，9，3，4，6，7}</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/59126343c42b454e8e4f2e3b76312d76?from=pc" alt="流行算法：局部搜索算法"></p><p>图5-2</p><p>2）逆序操作，两个随机位置间的城市逆序；</p><p>如图5-3所示。新解s &#x3D; {2，8，3，4，1，9，5，6，7}</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f36afc2946ed474dacbfae4d972d3bbe?from=pc" alt="流行算法：局部搜索算法"></p><p>图5-3</p><p>3）插入操作，随机选择某点插入某随机位置。</p><p>如图5-4所示。新解s &#x3D; {2，3，5，9，9，1，4，6，7}</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/98419bf4a588487e8882d3b4d52fabbd?from=pc" alt="流行算法：局部搜索算法"></p><p>图5-4</p><h1 id="六、局部搜索法具体步骤"><a href="#六、局部搜索法具体步骤" class="headerlink" title="六、局部搜索法具体步骤"></a>六、局部搜索法具体步骤</h1><h1 id="普通局部搜索算法-爬山法"><a href="#普通局部搜索算法-爬山法" class="headerlink" title="普通局部搜索算法-爬山法"></a>普通局部搜索算法-爬山法</h1><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>，随机的选择一个初始的可能解<span class="keyword">X</span><span class="number">0</span>∈D，Xb=<span class="keyword">X</span><span class="number">0</span>，<span class="keyword">P</span>=N(Xb)，f(<span class="keyword">X</span>)为目标函数。</span><br><span class="line"><span class="number">2</span>，如果不满足结束条件，则</span><br><span class="line"><span class="number">3</span>，Begin</span><br><span class="line"><span class="number">4</span>，   选择<span class="keyword">P</span>的一个子集<span class="keyword">P</span><span class="string">&#x27;，Xn为P&#x27;</span>中的最优解</span><br><span class="line"><span class="number">5</span>，   如果f(Xn) &lt; f(Xb)，则Xb ＝ Xn，<span class="keyword">P</span> = N(Xb)，</span><br><span class="line">          转<span class="number">2</span>；</span><br><span class="line"><span class="number">6</span>，   否则<span class="keyword">P</span> = <span class="keyword">P</span> – <span class="keyword">P</span><span class="string">&#x27;，转2。</span></span><br><span class="line"><span class="string">7，End</span></span><br><span class="line"><span class="string">8，输出计算结果</span></span><br><span class="line"><span class="string">9，结束</span></span><br></pre></td></tr></table></figure><p>该算法缺点：该局部最优问题，也叫局部峰值局部陷井。</p><p>现实问题中，f在D上往往有多个局部的极值点。</p><p>一般的局部搜索算法一旦陷入局部极值点，算法就在该点处结束，这时得到的可能是一个糟糕的结果。如图6-1中的A点。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a459c893ed9544c2b8f16a1d386850cb?from=pc" alt="流行算法：局部搜索算法"></p><p>图6-1</p><p>解决方法：</p><p>每次并不一定选择邻域内最优的点，而是依据一定的概率，从邻域内选择一个点。</p><p>目标函数优的点，被选中的概率大，目标函数差的点，被选中的概率小。</p><p>考虑归一化问题，使得邻域内所有点被选中的概率和为1。</p><p>如图6-2，以概率选择邻域的点，有机会跳出局部极值点。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ac26b228af9c416ca20d7d3da8881361?from=pc" alt="流行算法：局部搜索算法"></p><p>图6-2</p><h1 id="局部搜索算法1"><a href="#局部搜索算法1" class="headerlink" title="局部搜索算法1"></a>局部搜索算法1</h1><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>，随机的选择一个初始的可能解<span class="keyword">X</span><span class="number">0</span>∈D，Xb=<span class="keyword">X</span><span class="number">0</span>，<span class="keyword">P</span>=N(Xb)，f(<span class="keyword">X</span>)为目标函数。</span><br><span class="line"><span class="number">2</span>，如果不满足结束条件，则</span><br><span class="line"><span class="number">3</span>，Begin</span><br><span class="line"><span class="number">4</span>，   对于所有的<span class="keyword">X</span>∈<span class="keyword">P</span>计算目标函数f(<span class="keyword">X</span>)，</span><br><span class="line">         并选择适当的概率公式计算每一个点<span class="keyword">X</span>的概率</span><br><span class="line"><span class="number">5</span>，   依计算的概率值，从<span class="keyword">P</span>中随机选择一个点</span><br><span class="line">         Xn，Xb ＝ Xn，<span class="keyword">P</span> = N(Xb)，转<span class="number">2</span></span><br><span class="line"><span class="number">6</span>，End</span><br><span class="line"><span class="number">7</span>，输出计算结果</span><br><span class="line"><span class="number">8</span>，结束</span><br></pre></td></tr></table></figure><p>该算法缺点：步长问题</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f0de0cba69ca4beaae6f48115b4088ca?from=pc" alt="流行算法：局部搜索算法"></p><p>解决办法</p><p>改变步长</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/29d89047106c4d9c8deb0cca297f7e9f?from=pc" alt="流行算法：局部搜索算法"></p><h1 id="局部搜索算法2"><a href="#局部搜索算法2" class="headerlink" title="局部搜索算法2"></a>局部搜索算法2</h1><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>，随机的选择一个初始的可能解<span class="keyword">X</span><span class="number">0</span>∈D，Xb=<span class="keyword">X</span><span class="number">0</span>，，f(<span class="keyword">X</span>)为目标函数。</span><br><span class="line">      确定一个初始步长计算<span class="keyword">P</span>=N(Xb)</span><br><span class="line"><span class="number">2</span>，如果不满足结束条件，则</span><br><span class="line"><span class="number">3</span>，Begin</span><br><span class="line"><span class="number">4</span>，    选择<span class="keyword">P</span>的一个子集<span class="keyword">P</span><span class="string">&#x27;，Xn为P&#x27;</span>中的最优解</span><br><span class="line"><span class="number">5</span>，    如果f(Xn) &lt; f(Xb)，则Xb ＝ Xn</span><br><span class="line"><span class="number">6</span>，    按照某种策略改变步长，计算<span class="keyword">P</span> = N(Xb)，</span><br><span class="line">           转<span class="number">2</span></span><br><span class="line"><span class="number">7</span>，    否则<span class="keyword">P</span> = <span class="keyword">P</span> – <span class="keyword">P</span><span class="string">&#x27;，转2。</span></span><br><span class="line"><span class="string">8，End</span></span><br><span class="line"><span class="string">9，输出计算结果</span></span><br><span class="line"><span class="string">10，结束</span></span><br></pre></td></tr></table></figure><p>该算法缺点：起始点问题</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/763226dbab4a41818d30ecf55dcfafe2?from=pc" alt="流行算法：局部搜索算法"></p><p>解决办法：</p><p>随机的生成一些初始点，从每个初始点出发进行搜索，找到各自的最优解。再从这些最优解中选择一个最好的结果作为最终的结果。</p><h1 id="局部搜索算法3"><a href="#局部搜索算法3" class="headerlink" title="局部搜索算法3"></a>局部搜索算法3</h1><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>，k = <span class="number">0</span></span><br><span class="line"><span class="number">2</span>，随机的选择一个初始的可能解<span class="keyword">X</span><span class="number">0</span>∈D，</span><br><span class="line">     Xb=<span class="keyword">X</span><span class="number">0</span>，<span class="keyword">P</span>=N(Xb)，f(<span class="keyword">X</span>)为目标函数。</span><br><span class="line"><span class="number">3</span>，如果不满足结束条件，则</span><br><span class="line"><span class="number">4</span>，Begin</span><br><span class="line"><span class="number">5</span>，   选择<span class="keyword">P</span>的一个子集<span class="keyword">P</span><span class="string">&#x27;，Xn为P&#x27;</span>中的最优解</span><br><span class="line"><span class="number">6</span>，   如果f(Xn) &lt; f(Xb)，则Xb ＝ Xn，<span class="keyword">P</span> = N(Xb)，转<span class="number">3</span></span><br><span class="line"><span class="number">7</span>，   否则<span class="keyword">P</span> = <span class="keyword">P</span> – <span class="keyword">P</span><span class="string">&#x27;，转3。</span></span><br><span class="line"><span class="string">8，End</span></span><br><span class="line"><span class="string">9，k = k+1</span></span><br><span class="line"><span class="string">10，如果k达到了指定的次数，则从k个结果中选</span></span><br><span class="line"><span class="string">        择一个最好的结果输出，否则转2</span></span><br><span class="line"><span class="string">11，输出结果</span></span><br><span class="line"><span class="string">12,  结束</span></span><br></pre></td></tr></table></figure><p><strong>小结：</strong></p><p>以上几种解决方法可以结合在一起使用，比如第一、第二种方法的结合，就产生了模拟退火方法。</p><h1 id="七、案例说明"><a href="#七、案例说明" class="headerlink" title="七、案例说明"></a>七、案例说明</h1><h1 id="例1：5城市TSP旅行商问题。"><a href="#例1：5城市TSP旅行商问题。" class="headerlink" title="例1：5城市TSP旅行商问题。"></a>例1：5城市TSP旅行商问题。</h1><p>一个商品推销员要去5个城市推销商品，该推销员从一个城市出发，需要经过所有城市后，回到出发地。应如何选择行进路线，以使总的行程最短？</p><p>如图7-1所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/dd5c86212d9c4775a32fe9e2bdcaf3d5?from=pc" alt="流行算法：局部搜索算法"></p><p>图7-1</p><p>设初始的可能解：X0 &#x3D; (a, b, c, d, e) , f(Xb) &#x3D; f(X0) &#x3D; ab+bc+cd+de+ea&#x3D;7+7+5+6 +13 &#x3D; 38</p><p>通过交换两个城市获得邻域</p><p>P &#x3D; {(a, c, b, d, e), (a, d, c, b, e), (a, e, c, d, b), (a, b, d, c, e), (a, b, e, d, c), (a, b, c, e, d)}</p><p>设每次随机从P中选择一个邻居。</p><p>第1步，从P中选择一个元素Xn &#x3D; (a, c, b, d, e), f(Xn) &#x3D; 42, f(Xn) &gt; f(Xb),</p><p>P &#x3D; P – {Xn}</p><p>&#x3D; {(a, d, c, b, e), (a, e, c, d, b), (a, b, d, c, e), (a, b, e, d, c), (a, b, c, e, d)}</p><p>第2步，从P中选择一个元素Xn &#x3D; (a, d, c, b, e), f(Xn) &#x3D; 45, f(Xn) &gt; f(Xb),</p><p>P &#x3D; P – {Xn}</p><p>&#x3D; {(a, e, c, d, b), (a, b, d, c, e), (a, b, e, d, c), (a, b, c, e, d)}</p><p>第3步，从P中选择一个元素Xn &#x3D; (a, e, c, d, b), f(Xn) &#x3D; 44, f(Xn) &gt; f(Xb),</p><p>P &#x3D; P – {Xn}</p><p>&#x3D; {(a, b, d, c, e), (a, b, e, d, c), (a, b, c, e, d)}</p><p>第4步，从P中选择一个元素Xn &#x3D; (a, b, d, c, e), f(Xn) &#x3D; 44, f(Xn) &gt; f(Xb),</p><p>P &#x3D; P – {Xn} &#x3D; {(a, b, e, d, c), (a, b, c, e, d)}</p><p>第5步，从P中选择一个元素Xn &#x3D; (a, b, e, d, c), f(Xn) &#x3D; 34, f(Xn) &lt; f(Xb),</p><p><strong>更新当前解</strong>Xb&#x3D;Xn , Xb &#x3D; (a, b, e, d, c),</p><p><strong>按照新解，更新邻域空间</strong>P’ &#x3D; {(a, e, b, d, c), (a, d, e, b, c), (a, c, e, d, b), (a, b, d, e, c), (a, b, c, d, e), (a, b, e, c, d)}</p><p>第6步，从P’中选择一个元素Xn &#x3D; (a, e, b, d, c), f(Xn) &#x3D; 44, f(Xn) &gt; f(Xb),</p><p>P’ &#x3D; P’ – {Xn}</p><p>&#x3D; {(a, d, e, b, c), (a, c, e, d, b), (a, b, d, e, c), (a, b, c, d, e), (a, b, e, c, d)}</p><p>第7步，从P’中选择一个元素Xn &#x3D; (a, d, e, b, c), f(Xn) &#x3D; 39, f(Xn) &gt; f(Xb),</p><p>P’ &#x3D; P’ – {Xn}</p><p>&#x3D; {(a, c, e, d, b), (a, b, d, e, c), (a, b, c, d, e), (a, b, e, c, d)}</p><p>第8步，从P’中选择一个元素Xn &#x3D; (a, c, e, d, b), f(Xn) &#x3D; 38, f(Xn) &gt; f(Xb),</p><p>P’ &#x3D; P’ – {Xn}</p><p>&#x3D; {(a, b, d, e, c), (a, b, c, d, e), (a, b, e, c, d)}</p><p>第9步，从P’中选择一个元素Xn &#x3D; (a, b, d, e, c), f(Xn) &#x3D; 38, f(Xn) &gt; f(Xb),</p><p>P’ &#x3D; P’ – {Xn} &#x3D; {(a, b, c, d, e), (a, b, e, c, d)}</p><p>第10步，从P中选择一个元素Xn &#x3D; (a, b, c, d, e), f(Xn) &#x3D; 38, f(Xn) &gt; f(Xb),</p><p>P’ &#x3D; P’ – {Xn} &#x3D; {(a, b, e, c, d)}</p><p>第11步，从P中选择一个元素Xn &#x3D; (a, b, e, c, d), f(Xn) &#x3D; 41, f(Xn) &gt; f(Xb),</p><p>P &#x3D; P – {Xn} &#x3D; {}</p><p>P等于空，算法结束。</p><p><strong>得到最终结果为Xb &#x3D; (a, b, e, d, c), f(Xb) &#x3D; 34。如图7-2所示，红色标记为最短路径。</strong></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/162d3a5aa7404368a217494039e0eeaa?from=pc" alt="流行算法：局部搜索算法"></p><p>图7-2</p><h1 id="例子2-皇后搜索算法（Queen-Search）"><a href="#例子2-皇后搜索算法（Queen-Search）" class="headerlink" title="例子2 皇后搜索算法（Queen Search）"></a>例子2 皇后搜索算法（Queen Search）</h1><p>如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击？（在n×n格的国际象棋上摆放n个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上。）</p><p>解：用局部搜索法。</p><p>定义S&#x3D;{Si}表示一个可能解，其中Si表示在第i行，第Si列有一个皇后。</p><p>如4皇后问题， 设一个初始解：S&#x3D;(4, 3, 1, 2)。（这里S表示第1行第4列有一个皇后；第2行第3列有一个皇后；第3行第1列有一个皇后；第4行第2列有一个皇后。）</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b7cab17a178f42378a9d90fd2dff5186?from=pc" alt="流行算法：局部搜索算法"></p><p>定义映射N为棋盘上任意两个皇后的所在行或列进行交换，即S中任意两个元素交换位置。</p><p>随机创建初始解S &#x3D; (4, 3, 1, 2)，依据映射N，其邻域为：</p><p>P &#x3D; N(S) &#x3D; {(3, 4, 1, 2), (1, 3, 4, 2), (2, 3, 1, 4), (4, 1, 3, 2), (4, 2, 1, 3), (4, 3, 1, 2)}</p><p>任选一个子集P’&#x3D; {(3, 4, 1, 2)}, 如果没有发生冲突，则P&#x3D;P-P’ （如果P&#x3D;0, 冲突仍然不能下降，则重新创建初始解）；否则，更新冲突数，接受新解，获取新解的邻域，再次选择子集。直到冲突数为0，算法结束。</p><p>对于n皇后，具体步骤如下</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>，随机地将<span class="built_in">n</span>个皇后分布在棋盘上，使得棋盘</span><br><span class="line">     的每行、每列只有一个皇后。</span><br><span class="line"><span class="number">2</span>，计算皇后间的冲突数conflicts。</span><br><span class="line"><span class="number">3</span>，如果冲突数conflicts等于<span class="number">0</span>，则转<span class="number">6</span></span><br><span class="line"><span class="number">4</span>，对于棋盘上的任意两个皇后，交换他们的行</span><br><span class="line">      或者列，如果交换后的冲突数conflicts减少，</span><br><span class="line">      则接受这种交换，更新冲突数conflicts，转<span class="number">3</span>。</span><br><span class="line"><span class="number">5</span>，如果陷入了局部极小，既交换了所有的皇后</span><br><span class="line">      后，冲突数仍然不能下降，则转<span class="number">1</span>。</span><br><span class="line"><span class="number">6</span>，输出结果</span><br><span class="line"><span class="number">7</span>，结束。</span><br></pre></td></tr></table></figure><p>最终结果，n个皇后中任意两个皇后不在同一斜线上。</p><p><strong>其它例子，如模拟退火算法、遗传算法等局部搜索再另行介绍。</strong></p><h1 id="八、在具体设计算法时的细节考虑"><a href="#八、在具体设计算法时的细节考虑" class="headerlink" title="八、在具体设计算法时的细节考虑"></a>八、在具体设计算法时的细节考虑</h1><h1 id="搜索机制的选择"><a href="#搜索机制的选择" class="headerlink" title="搜索机制的选择"></a>搜索机制的选择</h1><p>搜索机制是构造算法框架和实现优化的关键，是决定算法搜索行为的根本点。比如爬山法是基于局部贪婪的搜索机制，而模拟退火算法时基于概率分布的搜索机制，禁忌搜索采用避免迂回的策略进行搜索。</p><h1 id="邻域函数的设计"><a href="#邻域函数的设计" class="headerlink" title="邻域函数的设计"></a>邻域函数的设计</h1><p>邻域函数决定了邻域结构和邻域解的产生方式。算法对问题解的不同描述方式，会直接影响邻域函数的设计，进而影响算法的搜索行为。同时，即使在编码机制确定的情况下，邻域结构也可以采用不同的形式，以考虑新状态产生的可行性、合法性和对搜索效率的影响。在确定邻域结构后，当前状态邻域中候选解的产生方式即可以是确定的，也可以是随机性地。</p><h1 id="状态更新方式的设计"><a href="#状态更新方式的设计" class="headerlink" title="状态更新方式的设计"></a>状态更新方式的设计</h1><p>更新方式是指以何种策略在新旧状态中确定新的当前状态，是决定算法整体优化特性的关键步骤之一。基于确定性地状态更新方式的搜索，容易陷入局部最优；而随机性地状态更新方式，尤其是概率性劣向转移，往往取得较好的全局优化解，但是计算时间也比较长。</p><h1 id="参数的控制方式"><a href="#参数的控制方式" class="headerlink" title="参数的控制方式"></a>参数的控制方式</h1><p>控制参数是决定算法搜索进程和行为的又一关键因素。合适的参数控制有助于增强算法在邻域中的优化能力和效率，同时也必须以一定的准则和方式进行修改以适应算法性能的动态变化。</p><h1 id="终止准则的设计"><a href="#终止准则的设计" class="headerlink" title="终止准则的设计"></a>终止准则的设计</h1><p>终止准则是判断算法是否收敛的标准，决定了算法的最终优化性能。算法收敛理论为终止理论提供了明确的设计方案，但是基于理论分析所得的收敛准则往往很苛刻的，甚至难以应用。实际设计时，应兼顾算法的优化质量和搜索效率等多方面性能，或根据问题需要着重强调算法的某方面性能。</p><h1 id="九、应用"><a href="#九、应用" class="headerlink" title="九、应用"></a>九、应用</h1><h1 id="主要应用于组合优化问题。"><a href="#主要应用于组合优化问题。" class="headerlink" title="主要应用于组合优化问题。"></a>主要应用于组合优化问题。</h1><p>组合优化所研究的问题涉及信息技术、经济管理、工业工程、交通运输、通讯网络等诸多领域。这是几个最经典的应用：</p><ol><li>TSP问题：从某个城市出发，经过n个指定的城市，每个城市只能且必须经过一次，最后回到出发城市，如何安排旅行商的行走路线以使总路程最短？</li><li>约束机器排序问题：n 个加工量为di(i&#x3D;1,2,… n)的产品在一台机器上加工，机器在第t个时段的工作能力为ct,完成所有产品加工的最少时段数。</li><li>指派问题： 一家公司经理准备安排N名员工去完成N项任务，每人一项。由于各员工的特点不同，不同的员工去完成同一项任务时获得的回报是不同的。如何分配工作方案可以获得最大收益？</li><li>0-1背包问题： 设有一个容积为b的背包，n个体积分别为ai(i&#x3D;1,2,… n),价值分别为ci ( i&#x3D;1,2,… n)的物品，如何以最大的价值装包？</li><li>装箱问题： 如何用个数最少的尺寸为1的箱子装进n个尺寸不超过1的物品？</li><li>SAT问题：称 判定一个公式是否存在一个模型的问题为可满足性问题(以后简称为SAT问题)。如果一个公式存在模型，则称该公式是可满足的，否则称为不可满足的。</li><li>皇后问题： 在n×n的国际象棋棋盘上，摆放n个皇后，使得n个皇后之间不能相互“捕捉”？</li></ol><h1 id="应用于各种NP问题。"><a href="#应用于各种NP问题。" class="headerlink" title="应用于各种NP问题。"></a>应用于各种NP问题。</h1><p>计算量呈指数时间复杂度的应用，均可考虑此算法。</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>匈牙利算法</title>
      <link href="/2022/04/03/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95/"/>
      <url>/2022/04/03/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6939746079759974944/">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>匈牙利算法（Hungarian algorithm），其核心就是寻找增广路径，是一种用增广路径求二分图最大匹配的算法。</p><p>匈牙利算法是一种在P问题内（多项式时间内）求解任务分配问题的组合优化算法。它推动了后来的原始对偶方法。</p><p>匈牙利算法是美国数学家哈罗德·库恩于1955年提出的。此算法之所以被称作匈牙利算法，是因为算法很大一部分是基于以前匈牙利数学家Dénes Kőnig和Jenő Egerváry的工作之上创建起来的。</p><h1 id="二、名词解释"><a href="#二、名词解释" class="headerlink" title="二、名词解释"></a>二、名词解释</h1><h1 id="1、二分图与匹配"><a href="#1、二分图与匹配" class="headerlink" title="1、二分图与匹配"></a>1、二分图与匹配</h1><p>二分图（Bipartite graph）又称作二部图，是图论中的一种特殊模型。 设G&#x3D;(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集(A,B)，A、B内部的点不相交，并且图中的每条边（i，j）所关联的两个顶点i和j分别属于这两个不同的顶点集(i∈A, j∈B)，则称图G为一个二分图。</p><p>如图2-1左边图转换成一个二分图。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2ad50b9d65ca4624b317c801920d8701?from=pc" alt="流行算法：匈牙利算法"></p><p>图2-1</p><p>一个图为二分图的充分必要条件是至少有两个点，并且如果存在回路的话，那么回路的长度(长度指的是该回路连接的点的数目)必须为偶数。</p><p>二分图的匹配：</p><p>给定一个二分图G，在G的一个子图M中，M的边集{E}中的任意两条边都不依附于同一个顶点，则称M是一个匹配。</p><p>极大匹配是指在当前已完成的匹配下,无法再通过增加未完成匹配的边的方式来增加匹配的边数。</p><p>最大匹配是所有极大匹配当中边数最大的一个匹配。选择这样的边数最大的子集称为图的最大匹配问题。匈牙利算法就是找出这样一个最大匹配的边数。对于图来说，最大匹配不是唯一的，但是最大匹配的大小是唯一的。</p><p>如果一个匹配中，图中的每个顶点都和图中某条边相关联，则称此匹配为完全匹配，也称作完美匹配。图2-2中图d就是二分图的一个完全匹配（同时也是最大匹配），但是最大匹配不总是完全匹配。</p><h1 id="例子1-如图2-2所示"><a href="#例子1-如图2-2所示" class="headerlink" title="例子1 如图2-2所示"></a>例子1 如图2-2所示</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/94b4c835aabd4602aa1be6c0c90d21c9?from=pc" alt="流行算法：匈牙利算法"></p><p>图2-2</p><p>图a，图b，图c，图d中任意两条边的连接的顶点都没有相同的(换句话说，n条边必须连接2 * n个不相同的顶点)。所以他们都是图G的匹配。</p><h1 id="例子2-如图2-3所示"><a href="#例子2-如图2-3所示" class="headerlink" title="例子2 如图2-3所示"></a>例子2 如图2-3所示</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e293607bba904c5ba990203fd215fa5b?from=pc" alt="流行算法：匈牙利算法"></p><p>图2-3</p><p>红线部分代表匹配或完美匹配。</p><h1 id="2、增广路径"><a href="#2、增广路径" class="headerlink" title="2、增广路径"></a>2、增广路径</h1><p>交替路：从一个未匹配点出发，依次经过非匹配边、匹配边、非匹配边…形成的路径叫交替路。</p><p>增广路径：若P是图G中一条连通两个未匹配顶点的路径，并且属于M的边和不属于M的边(即已匹配和待匹配的边)在P上交替出现，则称P为相对于M的一条增广路径。</p><p>换一个说法，从一个未匹配点出发，走交替路，如果途径另一个未匹配点，则这条交替路称为增广路。如图2-4所示，红色结点代表匹配结点， 红色边代表匹配边，黑色边代表非匹配边。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c180006e1543456db0408c7ad2bcea91?from=pc" alt="流行算法：匈牙利算法"></p><p>图2-4</p><p>增广路径：1 -&gt; 2-&gt;3-&gt;4-&gt;5-&gt;6。1、6非匹配点，中间结点2、3、4、5是匹配点。黑色标记 1—&gt;2、3—&gt;4、5—&gt;6是非匹配边，红色标记2-3、4-5是匹配边。</p><p>增广路径的首尾是非匹配点。因此，增广路径的第一条和最后一条边，必然是非匹配边；同时它的第二条边（如果有）和倒数第二条边（如果有），必然是匹配边；以及第三条边（如果有）和倒数第三条边（如果有），一定是非匹配边。</p><p>增广路径从非匹配边开始，匹配边和非匹配边依次交替，最后由非匹配边结束。这样一来，增广路径中非匹配边的数目会比匹配边大 1。</p><p>如果我们置换增广路径中的匹配边和非匹配边，由于增广路径的首尾是非匹配点，其余则是匹配点，这样的置换不会影响原匹配中的匹配点，匹配中不包含在该路径中的其他匹配边也不受到影响，因而不会破坏匹配；增广路径的置换，可以得到比原有匹配更大的匹配（具体来说，匹配的边数增加了 1）。</p><p>增广路有一个重要特点：非匹配边比匹配边多一条。因此，研究增广路的意义是改进匹配。只要把增广路中的匹配边和非匹配边的身份交换即可。由于中间的匹配节点不存在其他相连的匹配边，所以这样做不会破坏匹配的性质。交换后，匹配边数目比原来多了 1 条。</p><p>由于二分图的最大匹配必然存在（比如，上限是包含所有顶点的完全匹配），所以，在任意匹配的基础上，如果我们有办法不断地搜寻出增广路径，直到最终我们找不到新的增广路径为止，我们就有可能得到二分图的一个最大匹配。这就是匈牙利算法的核心思想。</p><p>唯一的问题在于，在这种贪心的思路下，我们如何保证不存在例外的情况，即：当前匹配不是二分图的最大匹配，但已找不到一条新的增广路径。</p><p>我们从反证法考虑，即假设存在这样的情况。因为当前匹配不是二分图的最大匹配，那么在两个集合中，分别至少存在一个非匹配点。那么情况分为两种：</p><p>1)、这两个点之间存在一条边——那么我们找到了一条新的增广路径，产生矛盾；</p><p>2)、这两个点之间不存在直接的边，即这两个点分别都只与匹配点相连——那么：</p><p>a、如果这两个点可以用已有的匹配点相连，那么我们找到了一条新的增广路径，产生矛盾；</p><p>b、如果这两个点无法用已有的匹配点相连，那么这两个点也就无法增加匹配中边的数量，也就是我们已经找到了二分图的最大匹配，产生矛盾。</p><p>在所有可能的情况，上述假设都会产生矛盾。因此假设不成立，亦即贪心算法必然能求得最大匹配的解。</p><p>由增广路径的定义可以推出的三个结论：</p><p>①P的路径长度必定为奇数，第一条边和最后一条边都不属于M；</p><p>②P经过置换(取反)操作可以得到一个更大的匹配M；</p><p>③M为G的最大匹配当且仅当不存在相对于M的增广路径。</p><h1 id="增广路的应用"><a href="#增广路的应用" class="headerlink" title="增广路的应用"></a>增广路的应用</h1><ul><li>增广路用于证明最大匹配问题。</li><li>增广路主要应用于匈牙利算法中，用于求二分图最大匹配。</li></ul><h1 id="3、匈牙利树"><a href="#3、匈牙利树" class="headerlink" title="3、匈牙利树"></a>3、匈牙利树</h1><p>匈牙利树一般由 BFS 构造（类似于 BFS 树）。从一个未匹配点出发运行 BFS（唯一的限制是，必须走交替路），直到不能再扩展为止。如图2-5，由Fig.7，可以得到Fig.8的一棵 BFS 树。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/86591fe9815540c9aa10ed0e8c5f394f?from=pc" alt="流行算法：匈牙利算法"></p><p>图2-5</p><p>这棵树存在一个叶子结点为非匹配点（7号），但是匈牙利树要求所有叶子结点均为匹配点，因此这不是一棵匈牙利树（顺便说一句，Fig.8 中非匹配根节点2到非匹配叶结点7显然是一条增广路，沿这条增广路扩充后将得到一个完美匹配）。在Fig.9中，从2号结点出发就会得到一棵匈牙利树。</p><p>匈牙利树要求所有叶子结点均为匹配点，它就是把存在的可连接的匹配点都列出来。</p><h1 id="4、二分图最大匹配数-x3D-最小点覆盖率"><a href="#4、二分图最大匹配数-x3D-最小点覆盖率" class="headerlink" title="4、二分图最大匹配数&#x3D;最小点覆盖率"></a>4、二分图最大匹配数&#x3D;最小点覆盖率</h1><p>二分图的最小点覆盖的理解：找到最少的一些点，使二分图所有的边都至少有一个端点在这些点之中。倒过来说就是，删除包含这些点的边，可以删掉所有边。</p><h1 id="三、匈牙利算法复杂度"><a href="#三、匈牙利算法复杂度" class="headerlink" title="三、匈牙利算法复杂度"></a>三、匈牙利算法复杂度</h1><p>设V为二分图左边的顶点数，E为二分图中边的数目。匈牙利算法的实现以与点集合 V 为基础，每次在集合V中选一个顶点 Vi 做增广路径的起点搜索增广路径。搜索增广路径需要遍历边及E内的所有边，遍历方法可以采用深度优先遍历（DFS），也可以采用广度优先遍历（BFS），无论什么方法，其时间复杂度都是 O(E)。<br>匈牙利算法每个顶点 Vi 只能选择一次，因此算法的整体时间复杂度是 O(V*E)，总的来说，是一个相当高效的算法。</p><h1 id="四、举例说明"><a href="#四、举例说明" class="headerlink" title="四、举例说明"></a>四、举例说明</h1><h1 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h1><p>假设在婚介公司的手上有4位剩男，4位剩女，还没见面只是互相看了资料，每个人都对多名异性有好感。婚介公司做了一张互有好感关系图。如图4-1所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ec9c344c222849e0a67f3d1690c6a95b?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-1</p><p>是否可能让所有男孩和女孩一对一配对，使得每对儿都互相喜欢呢？在图论中，这就是完美匹配问题。如果换一个说法：最多有多少互相喜欢的男孩&#x2F;女孩可以配对儿？这就是最大匹配问题。匈牙利算法就是为了求解最大匹配问题。</p><h1 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h1><p>第1步 初始为空匹配。如图4-2所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/bffbf8f9a762453db2d3813bd3e56c47?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-2</p><p>第2步 选择空匹配点Al、Alice，建立第1条增广路径Al—&gt;Alice, 并进行置换，创建第1个匹配：红色标记Al**—&gt;**Alice。如图4-3所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/88033fc474ab41dab2e6b3e0247df7bf?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-3</p><p>第3步 选择空匹配点Bob，建立第2条增广路径Bob–&gt;Carol, 并进行置换，创建第2个匹配：红色标记<strong>Bob—&gt;Carol</strong>。如图4-4所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7393e9824891468a8a8721b62c9d0800?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-4</p><p>第4步，选择空匹配点Christ，建立增广路径Christ—&gt;<strong>Alice—&gt;Al</strong>—&gt;Beatrice</p><p>（增广路径满足：头部Christ、尾部Beatrice都是非匹配点， 中间Alice、Al都是匹配点；蓝色标记Christ—&gt;Alice是非匹配边，红色标记<strong>Alice—&gt;Al</strong>是匹配边，蓝色标记Al—&gt;Beatrice是非匹配边）。如图4-5所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0f1d5095d81244618d06042d8d45b5b4?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-5</p><p>第5步，置换增广路径中的匹配边和非匹配边。建立新的匹配：Christ—&gt;Alice**—&gt;**Al—&gt;Beatrice</p><p>（红色标记<strong>Christ—&gt;Alice</strong>是匹配边，Alice—&gt;Al是非匹配边，红色标记<strong>Al—&gt;Beatrice</strong>是匹配边）。如图4-6所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/655849a26e694f2bb07fea89459440ae?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-6</p><p>第6步，选择空匹配点Dan，建立增广路径Dan—&gt;<strong>Carol—&gt;Bob</strong>—&gt;Danielle</p><p>（增广路径满足：头部Dan、尾部Danielle都是非匹配点， 中间Carol、Bob都是匹配点；蓝色标记Dan—&gt;Carol是非匹配边，红色标记<strong>Carol—&gt;Bob</strong>是匹配边，蓝色标记Bob—&gt;Danielle是非匹配边）。如图4-7所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/95e402fa9b0d4c6d84110a72a86f5b46?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-7</p><p>第7步，置换增广路径中的匹配边和非匹配边。建立新的匹配：Dan—&gt;Carol—&gt;Bob—&gt;Danielle</p><p>（红色标记Dan—&gt;Carol是匹配边，Carol—&gt;Bob是非匹配边，红色标记Bob—&gt;Danielle是匹配边）。如图4-8所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/de9e1b0205264fbfb515c611e700b064?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-8</p><p>已经无法添加新的增广路径了，算法结束。男女双方达到最大配对，红色标记最大配对边:{Al—&gt;Beatrice，Bob—&gt;Danielle，Christ—&gt;Alice, Dan—&gt;Carol}。</p><h1 id="解法二"><a href="#解法二" class="headerlink" title="解法二"></a>解法二</h1><p>第1步 初始为空匹配。。如图4-9所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7a0a3f5c7c3f4cf39f05e6cec62a419b?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-9</p><p>第2步 选择空匹配点Al、Alice，建立第1条增广路径Al—&gt;Alice, 并进行置换，创建第1个匹配：红色标记Al**—&gt;**Alice。如图4-10所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e867fd9454334d0f89e396e8db80936a?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-10</p><p>第3步，选择空匹配点Christ，建立增广路径Christ—&gt;<strong>Alice—&gt;Al</strong>—&gt;Carol</p><p>（增广路径满足：头部Christ、尾部Carol都是非匹配点， 中间Alice、Al都是匹配点；蓝色标记Christ—&gt;Alice是非匹配边，红色标记<strong>Alice—&gt;Al</strong>是匹配边，蓝色标记Al—&gt;Carol是非匹配边）。如图4-11所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/033084fbf6624821b7850f8f91e35378?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-11</p><p>第4步，置换增广路径中的匹配边和非匹配边。建立新的匹配：Christ—&gt;Alice**—&gt;**Al—&gt;Carol</p><p>（红色标记<strong>Christ—&gt;Alice</strong>是匹配边，黑色标记Alice—&gt;Al是非匹配边，红色标记<strong>Al—&gt;Carol</strong>是匹配边）。如图4-12所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/d16f1a1129c3416e89fdc915902c45e8?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-12</p><p>第5步，选择空匹配点Dan，建立增广路径Dan—&gt;<strong>Carol—&gt;Al</strong>—&gt;Beatrice</p><p>（增广路径满足：头部Dan、尾部Beatrice都是非匹配点， 中间Carol、Al都是匹配点；蓝色标记Dan—&gt;Carol是非匹配边，红色标记<strong>Carol—&gt;Al</strong>是匹配边，蓝色标记Al—&gt;Beatrice是非匹配边）。如图4-13所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0dea5d76ce1b46289343731165354b6f?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-13</p><p>第6步，置换增广路径中的匹配边和非匹配边。建立新的匹配：Dan—&gt;Carol—&gt;Al—&gt;Beatrice</p><p>（红色标记Dan—&gt;Carol是匹配边，Carol—&gt;Al是非匹配边，红色标记Al—&gt;Beatrice是匹配边）。如图4-14所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/6fc700dcb9064edc942cf311d9186d98?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-14</p><p>第7步，选择空匹配点Bob，建立匹配边<strong>Bob–&gt;Danielle</strong>。如图4-15所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0593bdd9ab294a34a37f1612423b34ed?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-15</p><p>已经无法添加新的增广路径了，算法结束。男女双方达到最大配对，红色标记最大配对边:{Al—&gt;Beatrice，Bob—&gt;Danielle，Christ—&gt;Alice, Dan—&gt;Carol}。</p><h1 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h1><p>匈牙利算法的核心是：不断寻找增广路，并扩展增广路。不断重复这一过程直到找不到增广路为止。如图4-16所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4c9198ba1c8b47bcb82ec6a496f2e077?from=pc" alt="流行算法：匈牙利算法"></p><p>图4-16</p><p>具体步骤：</p><ol><li>首先从左边第一个点出发，寻找增广路黑色标记 1-&gt;2(没错，这个也是增广路)，然后翻转关系变为红色标记1-&gt;2。</li><li>从左边第二个点出发，寻找增广路3-&gt;2-&gt;1-&gt;4,然后翻转关系变为红色标记3-&gt;2-&gt;1-&gt;4。</li><li>从左边第三个点出发，寻找增广路5-&gt;4-&gt;1-&gt;2-&gt;3-&gt;6,然后翻转关系5-&gt;4-&gt;1-&gt;2-&gt;3-&gt;6。</li><li>至此，我们已经找到二分图G的最大匹配数，同时这个也是完全匹配。</li></ol><h1 id="五、应用"><a href="#五、应用" class="headerlink" title="五、应用"></a>五、应用</h1><h1 id="例子1-矩阵游戏"><a href="#例子1-矩阵游戏" class="headerlink" title="例子1 矩阵游戏"></a>例子1 矩阵游戏</h1><p>**题目描述</p><p>**小Q是一个非常聪明的孩子，除了国际象棋，他还很喜欢玩一个电脑益智游戏――矩阵游戏。矩阵游戏在一个 N×N黑白方阵进行（如同国际象棋一般，只是颜色是随意的）。每次可以对该矩阵进行两种操作：<br>行交换操作：选择矩阵的任意两行，交换这两行（即交换对应格子的颜色）<br>列交换操作：选择矩阵的任意两列，交换这两列（即交换对应格子的颜色）<br>游戏的目标，即通过若干次操作，使得方阵的主对角线(左上角到右下角的连线)上的格子均为黑色。<br>对于某些关卡，小Q百思不得其解，以致他开始怀疑这些关卡是不是根本就是无解的！于是小Q决定写一个程序来判断这些关卡是否有解。</p><p><strong>解：</strong></p><p>解题思路：我们把矩阵转化为二分图（左侧集合代表各行，右侧集合代表各列，某位置为1则该行和该列之间有边）。我们想进行一系列交换操作，使得X1连上Y1，X2连上Y2，……</p><p>假设N&#x3D;4, 建立4×4的01矩阵。如图5-1所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b8bceb46896c4647a6a0620fddd78687?from=pc" alt="流行算法：匈牙利算法"></p><p>图5-1</p><p>原问题变为将矩阵转化为二分图，求最大匹配。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/32e259aca2274eeeaa515f8651f53d1d?from=pc" alt="流行算法：匈牙利算法"></p><p>图5-2</p><p>如图5-2所示，采用匈牙利算法，红色标记最大匹配为｛X1—&gt;Y4, X2—&gt;Y2, X3—&gt;Y1, X4—&gt;Y3｝。</p><p>结果矩阵为(如图5-3所示)：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a280f92209754ce79739a420c6949311?from=pc" alt="流行算法：匈牙利算法"></p><p>图5-3</p><h1 id="例子2-柯南开锁"><a href="#例子2-柯南开锁" class="headerlink" title="例子2 柯南开锁"></a>例子2 柯南开锁</h1><p>面对OIBH组织的嚣张气焰, 柯南决定深入牛棚, 一探虚实.<br>他经过深思熟虑, 决定从OIBH组织大门进入………..<br>OIBH组织的大门有一个很神奇的锁.<br>锁是由M*N个格子组成, 其中某些格子凸起(灰色的格子). 每一次操作可以把某一行或某一列的格子给按下去.</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/29ef47800b2d4a9c8d074e29cb9e1245?from=pc" alt="流行算法：匈牙利算法"></p><p>图5-4</p><p>如果柯南能在组织限定的次数内将所有格子都按下去, 那么他就能够进入总部. 但是OIBH组织不是吃素的, 他们的限定次数恰是最少次数.<br>请您帮助柯南计算出开给定的锁所需的最少次数。</p><p><strong>解：</strong></p><p><strong>解题思路</strong></p><p>按下一行或一列，其实就是删掉与某个点相连的所有边。现在要求最少的操作次数，想想看，这不就是求最小点覆盖数吗？ 由Kőnig定理：二分图最小点覆盖数 &#x3D; 最大匹配数，也就是求二分图的最大匹配，用匈牙利算法可解决此问题。</p><p><strong>具体步骤</strong></p><p>第1步，先将图5-4中的矩阵行与列分别设定了二分图中的集合X与Y，标出灰色格子对应的连接边。</p><p>第2步，采用匈牙利算法求出该二分图的最大匹配：{X2—&gt;Y4, X4—&gt;Y2}。如图5-5所示。</p><p>第3步，在图5-4中找到最大匹配所对应的灰色格子，灰色格子数就是开锁所需的最小次数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2d162555b8a9443dbdabd590d4f6d3ef?from=pc" alt="流行算法：匈牙利算法"></p><p>图5-5</p><h1 id="六、附录"><a href="#六、附录" class="headerlink" title="六、附录"></a>六、附录</h1><h1 id="附录1-相关概念与定理"><a href="#附录1-相关概念与定理" class="headerlink" title="附录1 相关概念与定理"></a>附录1 相关概念与定理</h1><p>最大匹配数：最大匹配的匹配边的数目。</p><p>最小点覆盖数：选取最少的点，使任意一条边至少有一个端点被选择。</p><p>最大独立数：选取最多的点，使任意所选两点均不相连。</p><p>最小路径覆盖数：对于一个 DAG（有向无环图），选取最少条路径，使得每个顶点属于且仅属于一条路径。路径长可以为 0（即单个点）。</p><p>最大匹配数&#x3D;最大流&#x3D;最小割&#x3D;最小点集覆盖</p><p>定理1：最大匹配数 &#x3D; 最小点覆盖数（这是Knig 定理）</p><p>定理2：最大匹配数量 &#x3D; 顶点数 - 最大独立数</p><p>定理3：最小路径覆盖数 &#x3D; 顶点数 - 最大匹配数</p><h1 id="附录2-Konig定理及其证明"><a href="#附录2-Konig定理及其证明" class="headerlink" title="附录2 Kőnig定理及其证明"></a>附录2 Kőnig定理及其证明</h1><p><strong>Kőnig定理：二分图中的最大匹配数等于这个图中的最小点覆盖数。</strong></p><p>证明：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/59311857eb4f41cdbdd708dd818f4f8b?from=pc" alt="流行算法：匈牙利算法"></p><p>图6-1</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/36151513ac8a44adbcaf20dee57d5393?from=pc" alt="流行算法：匈牙利算法"></p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>马尔可夫链蒙特卡洛法(MCMC)</title>
      <link href="/2022/03/21/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95-MCMC/"/>
      <url>/2022/03/21/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95-MCMC/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6955086840999182861/">今日头条</a></p><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>马尔可夫链蒙特卡洛方法(Markov Chain Monte Carlo)，简称MCMC。其产生于20世纪50年代早期，是在贝叶斯理论框架下，通过计算机进行模拟的蒙特卡洛方法(Monte Carlo)。该方法将马尔可夫(Markov)过程引入到Monte Carlo模拟中，实现抽样分布随模拟的进行而改变的动态模拟，弥补了传统的蒙特卡罗积分只能静态模拟的缺陷。</p><p>Metropolis等人在1953年首次提出了基于马尔可夫链的蒙特卡罗方法，即 Metropolis 算法，并在最早的计算机上编程实现。Metropolis算法是首个普适的采样方法，并启发了一系列 MCMC 方法，所以人们把它视为随机模拟技术腾飞的起点。 Metropolis算法这篇论文[1]被收录在《统计学中的重大突破》中，《Computing in Science &amp; Engineering》尝试列出了对20世纪科学与工程的发展和实践影响最大的十种算法：</p><ul><li>Metropolis Algorithm for Monte Carlo</li><li>Simplex Method for Linear Programming</li><li>Krylov Subspace Iteration Methods</li><li>The Decompositional Approach to Matrix Computations</li><li>The Fortran Optimizing Compiler</li><li>QR Algorithm for Computing Eigenvalues</li><li>Quicksort Algorithm for Sorting</li><li>Fast Fourier Transform</li><li>Integer Relation Detection</li><li>Fast Multipole Method</li></ul><p>Metropolis Algorithm for Monte Carlo 被列为十大算法之首。 用于蒙特卡洛的Metropolis算法定义了一个收敛的马尔可夫链，其极限就是所需的概率分布。Metropolis算法及其推广算法已被称为蒙特卡洛马尔可夫链技术(MCMC)，因为这些算法模拟了一个马尔可夫链，从极限分布中获取抽样。</p><p>搞自然科学研究的人，MCMC应该是基本装备。所谓平生不识MCMC, 便称英雄也枉然。</p><h1 id="二、MCMC的两大贴身护卫"><a href="#二、MCMC的两大贴身护卫" class="headerlink" title="二、MCMC的两大贴身护卫"></a>二、MCMC的两大贴身护卫</h1><p>MCMC由两个MC组成，即马尔可夫链(Markov Chain ，也简称MC)和蒙特卡洛法（Monte Carlo Simulation，简称MC）。要弄懂MCMC的原理我们首先得搞清楚马尔科夫链的原理和蒙特卡罗法。</p><h1 id="1、马尔可夫链及其平稳分布"><a href="#1、马尔可夫链及其平稳分布" class="headerlink" title="1、马尔可夫链及其平稳分布"></a>1、马尔可夫链及其平稳分布</h1><h1 id="1）定义"><a href="#1）定义" class="headerlink" title="1）定义"></a>1）定义</h1><p>马尔可夫链的命名来自俄国数学家安德雷.马尔可夫以纪念其首次提出马尔可夫链和对其收敛性质所做的研究。</p><p>马尔可夫链是一组具有马尔可夫性质的离散随机变量的集合。所谓马尔可夫性质是指某一时刻状态转移的概率只依赖于它的前一个状态。</p><p>具体地，马氏链的数学定义:</p><p>对概率空间内的离散随机变量集合</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/f70eeb511dc64b14ba8a37f85f1b3bd4?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>若随机变量的取值都在可数集S内：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4da605b2917244b4968f4354124c28ea?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>且随机变量的条件概率满足如下关系</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5240ef7a06924833a8b3e8b207359070?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则随机变量集合X被称为马尔可夫链，可数集S被称为状态空间，马尔可夫链在状态空间内的取值称为状态。</p><p>马氏链某一时刻状态转移的概率只依赖于它的前一个状态，这样定义，可以大大简化模型的复杂度，因此马氏链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。</p><p>马尔可夫过程通常分为3类：</p><ul><li>时间、状态都是离散的，称为马尔可夫链。</li><li>时间连续、状态离散的，称为连续时间的马尔可夫链。</li><li>时间连续、状态连续的，称为马尔可夫过程。</li></ul><p>可见马氏链是时间、状态都是离散的马尔可夫过程。</p><h1 id="2）马氏链转移概率矩阵和收敛现象"><a href="#2）马氏链转移概率矩阵和收敛现象" class="headerlink" title="2）马氏链转移概率矩阵和收敛现象"></a>2）马氏链转移概率矩阵和收敛现象</h1><p>我们先来看马氏链的一个具体的例子。社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1、2、3分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是0.65，属于中层收入的概率是0.28，属于上层收入的概率是0.07。事实上，从父代到子代，收入阶层的变化的转移概率如图2-1和图2-2</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4fdddf5dd5db4a14aa37d78b76da9647?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图2-1</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5043ca2c5dea4522bba13b8e01583224?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图2-2</p><p>设状态空间S&#x3D;{1,2,3} (1、2、3分别代表人的收入阶层：下层、中层、上层), 使用矩阵的表示方式，转移概率矩阵记为:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/24e3c81dc5d9485885ff9d35a099e1d3?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/bda3eace45a84a63b8f33631ee0c2e5d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>那么他们的子女的分布比例将是</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/2d972b13347f490691ea22659cf60e9e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>他们的孙子代的分布比例将是</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d9d3e4d915ad41b49a2f33e71a71ef7e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>第n代子孙的收入分布比例将是</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/37c9b5bee23e421eb17f2d42b63b8418?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>假设初始概率分布为π0&#x3D;[0.21,0.68,0.11]，则我们可以计算前n代人的分布状况如下</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ed06c2ce7cc84c83a372c9c66036b917?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？我们换一个初始概率分布π0&#x3D;[0.75,0.15,0.1]试试看，继续计算前n代人的分布状况如下:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/294f8d8299784a5496aa0e5e11c8eb85?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们发现，到第9代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 π&#x3D;[0.286,0.489,0.225]，也就是说收敛的行为和初始概率分布 π0无关。这说明这个收敛行为主要是由概率转移矩阵P决定的。我们计算一下 P的n阶矩阵为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9c6b3f73357540a3984482ef8aab8818?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们发现，当 n 足够大的时候，这个P的n阶矩阵的每一行都是稳定地收敛到π&#x3D;[0.286,0.489,0.225]这个概率分布。这个收敛现象并非是我们这个马氏链独有的，而是绝大多数马氏链的共同行为。这个性质同样不光是离散状态，连续状态时也成立。</p><h1 id="3）马氏链定理"><a href="#3）马氏链定理" class="headerlink" title="3）马氏链定理"></a>3）马氏链定理</h1><p>马氏链定理： 如果一个非周期马氏链具有转移概率矩阵P, 状态空间S, i,j∈S且它的任何两个状态是连通的，那么</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9e2c1b9139854c8da861d19a9f0012f7?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这就是马氏链的收敛定理。</p><p>由该定理可以得出如下结论：</p><p>&lt;1&gt;</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/757d1dad015c45cab90de6249160f8c6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>&lt;2&gt;</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/0cbe59f17b4149a3bf3a96b81563ebc9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>&lt;3&gt; π 是方程 πP&#x3D;π的唯一非负解。其中,</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/053cf1f72cc843e395afe6744e6cee29?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>π称为马氏链的平稳分布。</p><p>请注意两种表示方式的不同：</p><ul><li>表示状态空间S中的各个状态1,2,j…的概率：</li></ul><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/52a519e2b3b94f84a5e9b6d72d2be9d3?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><ul><li>表示不同时刻的概率分布</li></ul><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ea6abf3c431c4f79bda0d40e7798b8f6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这个马氏链的收敛定理非常重要，所有的 MCMC方法都是以这个定理作为理论基础的。定理的证明相对复杂，一般的随机过程课本中也不给证明，所以我们就不用纠结它的证明了，直接用这个定理的结论就好了。我们对这个定理的内容做一些解释说明：</p><ul><li>非周期的马尔科夫链：这个主要是指马尔科夫链的状态转化不是循环的，如果是循环的则永远不会收敛。幸运的是我们遇到的马尔科夫链一般都是非周期性的；</li><li>任何两个状态是连通的：这个指的是从任意一个状态可以通过有限步到达其他的任意一个状态，不会出现条件概率一直为0导致不可达的情况；</li><li>马尔科夫链的状态数可以是有限的，也可以是无限的。因此可以用于连续概率分布和离散概率分布；</li><li>π通常称为马尔科夫链的平稳分布。</li></ul><p>我们仅证明定理的第二个结论</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/8d74eb7e6d7249ce806f00f712d92dc9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>证明：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4a71af08a5d84e1fb1914249eb8e5e9f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>上式两边取极限就得到</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/52d306a3398a45f1b98bea66c83f2029?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>证明完毕。</p><h1 id="4-马氏链的平稳分布与细致平衡条件"><a href="#4-马氏链的平稳分布与细致平衡条件" class="headerlink" title="4) 马氏链的平稳分布与细致平衡条件"></a>4) 马氏链的平稳分布与细致平衡条件</h1><p>给定一个马尔可夫链，若在其状态空间S存在概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/95b953f7b1fb41058f727a050c0083fb?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则π是该马尔可夫链的平稳分布。</p><p><strong>细致平衡条件（Detailed Balance Condition）：</strong>给定一个马尔科夫链，概率分布π和概率转移矩阵P，∀i，j∈S, (S是状态空间)，如果下面等式成立:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/794dc96c205f4578a8017cca689a4ac2?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则此马尔科夫链具有一个平稳分布（Stationary Distribution)。</p><p>需要注意，这个细致平衡条件仅是一个充分条件，而不是必要条件，也就是说存在具有平稳分布的马尔科夫链不一定满足此细致平衡条件。</p><p>证明如下：</p><p>由条件推出</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/61228d2f14f64617b1e45fa8a4e3323c?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>得</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/0b36177d3d1c43ad82ce365f326a7e8e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>所以此马尔科夫链具有一个平稳分布。</p><p>从初始概率分布π0(x)出发，我们在马氏链上做状态转移，记Xi的概率分布为πi(x)，则有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/7a51ab1b786742e5a7158b4e42b4be25?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>由马氏链收敛的定理, 概率分布πi(x)将收敛到平稳分布 π(x)。假设到第n步的时候马氏链收敛，则有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/97bbdef7a20a4c54829d95eb188ecbea?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>所以</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/c4ad2723ee9f4ef1bb33078ae503cd71?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>也就是说无论我们的起始状态是何种概率分布，在状态转移矩阵 P 的作用下，经过足够大的 n 步转移之后，最后都收敛到一个平稳分布，且该分布是唯一不变的。即平稳分布仅由状态转移矩阵P决定。</p><p>设马尔可夫链的状态空间S&#x3D;{ω1，ω2，…, ωi，…ωn}（可以理解为样本空间在随机过程中的表达），如果我们从一个具体的初始状态 x0∈S（x0可能是ω2，也可能是ωi）开始,沿着马氏链按照概率转移矩阵做跳转，那么我们可以得到一个转移状态序列 ，马氏链终于露出了它的庐山真面：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/7a55071c272a44c5b132e5753e2fd15a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>尽管马尔可夫链最终会收敛到所需的分布，但是初始样本可能会遵循非常不同的分布，尤其是在起点位于低密度区域的情况下。结果，通常需要一个老化期，所以我们会舍弃前面的老化样本，得到我们需要的平稳样本集</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/baf866d7917f4711ac9f11f022fa77dc?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这就是平稳分布 π的状态样本序列。最后平稳分布的样本集可能达到几万、几十万甚至更大的规模。统计各个状态出现的频次，归一化后画出统计直方图，就可以模拟目标分布。</p><h1 id="5）基于马尔可夫链的采样过程"><a href="#5）基于马尔可夫链的采样过程" class="headerlink" title="5）基于马尔可夫链的采样过程"></a>5）基于马尔可夫链的采样过程</h1><p>1&gt; 输入马尔可夫链状态转移矩阵P, 状态空间S，设定状态转移收敛次数为n1, 迭代次数为n2;</p><p>2&gt; 从任意简单概率分布采样得到初始状态值x0∈S;</p><p>3&gt; for t&#x3D;0 to n1+n2-1: 从条件概率分布p(x|x(t))中采样，得到样本x &#x3D; x(t+1). 去掉前面的老化样本，最终的样本集</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a1991254bbc849cc9fd7c642e44cf10e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>即为我们需要的平稳分布对应的样本集。</p><p>这里大家会问：</p><ul><li>如何进行简单概率分布采样和条件概率分布采样？</li></ul><p>简单概率分布可选择正态分布或均值分布，正态分布根据Box–Muller变换法或接受-拒绝采样法采样，均值分布可直接计算机采样；条件概率分布可由输入转移矩阵P得到概率数据，然后根据离散随机变量逆变换法采样。这些采样方法后面有详细介绍。</p><ul><li>怎么证明得到的样本集就是平稳分布对应的样本集？</li></ul><p>两种采样方法来理解：</p><p>第1种方法，从简单概率分布中采样 10000个的初始状态样本，用最简单粗暴的方法对每一个初始样本都执行一次n(n&gt;n1)步转移，状态之间的转移概率从该马尔科夫链的状态转移矩阵获取，最后依如下概率落入状态空间中的一个状态。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/7f85c64a90024871af4848f161819d66?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>当初始状态样本足够多时，由大数定律，最终落入到平稳马氏链的各个状态的数量所占的比例逼近各个状态的平稳分布概率。但是这样一来计算量相当大，每次只利用了采样过程中的最后一个状态，且要运行 10000次进入平稳状态的转移过程，数据的利用率很低，浪费也很大。</p><p>第2种方法，只需要对一个初始状态进行状态转移，就可以实现整个采样过程。我们把它进入稳态的那一刻记作t0，当 t0 时刻的状态向下一个时刻t1 转移时，它依照稳态分布概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/0a99677ae03e45afba72977ddf44ec40?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>（由n-步转移概率的性质Chapman–Kolmogorov等式可知，n-步转移矩阵是其之前所有转移矩阵的连续矩阵乘法，证明见附录1）</p><p>进入到状态 1、状态2 或者状态j当中的任意一个。以此类推，对于后面的所有转移时刻：t2、t3……tn，当前状态都能依照稳态分布中的概率进入到状态空间的任何其它状态。进行N次转移后得到N个采样结果，N个采样结果中各个状态的数量所占的比例和平稳分布是一致的。这种方法巧妙地用时间轴的采样分布等价替代了空间轴的采样分布，省时省力。</p><p>图2-3示意了进入平稳期的采样全过程（假设总共3个状态）：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/53153e2b368f4c7481345cbcec5cb0be?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图2-3</p><h1 id="6）马尔可夫链的性质"><a href="#6）马尔可夫链的性质" class="headerlink" title="6）马尔可夫链的性质"></a>6）马尔可夫链的性质</h1><p>马尔可夫链的4个性质：不可约性、常返性、周期性和遍历性。</p><ul><li>不可约是指当马尔可夫链为任何状态时，其访问其余所有状态的概率都是正的，即随机变量可以在任意状态间转移。</li><li>常返性指给定一个状态则经过有限的时间它一定会重新回到这个状态。</li><li>非周期指马尔可夫链的状态的迁移不会陷入小的闭环。</li><li>遍历性指马尔可夫链的的所有状态互通且均为非周期的正常返状态。</li><li>平稳马尔可夫链也被称为齐次马尔可夫链。即转移概率不随时间而变。</li><li>满足细致平衡条件的马氏链具有一个平稳分布。</li><li>由极限定理可知，遍历链是平稳马尔可夫链。</li><li>有限状态马尔可夫链转移概率的极限分布一定是平稳分布。</li><li>若初始概率是平稳分布，则任意时刻的绝对概率分布等于初始分布，也即为平稳分布。</li></ul><h1 id="7-马尔可夫链的作用"><a href="#7-马尔可夫链的作用" class="headerlink" title="7) 马尔可夫链的作用"></a>7) 马尔可夫链的作用</h1><p>马尔可夫链在处理随机动力学时对问题建模的强大作用。</p><p>马尔可夫链预测理论在教育学、经济学、金融投资、生物学、农作物栽培、地质灾变，…特别是水资源科学中都得到了极为广泛地应用。</p><p>例如，马尔可夫链在排队理论、统计数据、生物种群进化建模、隐马尔可夫模型信息理论、语音识别、音字转换、词性标注、蒙特卡洛采样等都有应用。</p><h1 id="2、蒙特卡洛法"><a href="#2、蒙特卡洛法" class="headerlink" title="2、蒙特卡洛法"></a>2、蒙特卡洛法</h1><h1 id="1）定义-1"><a href="#1）定义-1" class="headerlink" title="1）定义"></a>1）定义</h1><p>蒙特卡洛(Monte Carlo，MC)方法, 又称随机抽样法，统计试验法或随机模拟法。</p><p>蒙特卡罗法是由冯·诺伊曼等人在20世纪40年代为研制核武器的需要而首先提出并以摩纳哥的赌城Monte Carlo来命名的一种计算方法。</p><p>蒙特卡洛法是数学建模十大算法之一，是思想和技巧的艺术品。它的魔力在于，对于那些规模极大的问题，求解难度随着问题的维数（自变量个数）的增加呈指数级别增长、出现所谓的“维数的灾难”的问题，传统方法无能为力，而蒙特卡洛方法却可以独辟蹊径，基于随机仿真的过程给出近似的结果。</p><h1 id="2）主要步骤"><a href="#2）主要步骤" class="headerlink" title="2）主要步骤"></a>2）主要步骤</h1><p>蒙特卡洛法是一类通过随机抽样过程来求取最优解的方法的总称，如果建立蒙特卡洛模型的过程没有出错，那么抽样次数越多，得到的答案越精确。蒙特卡洛法可以归纳为如下三个步骤：</p><p><a> 构造或描述概率过程，将欲求解问题转化为概率过程。</p><p>对于本身就具有随机性质的问题，主要是正确描述和模拟这个概率过程，对于本来不是随机性质的确定性问题，比如计算定积分，就必须事先构造一个人为的概率过程，它的某些参量正好是所要求问题的解。即要将不具有随机性质的问题转化为随机性质的问题。</p><p><strong><b> 实现从已知概率分布抽样。</strong></p><p>构造了概率模型以后， 按照这个概率分布抽取随机变量。这一般可以直接由软件工具包（见附录2，3，4）调用，或抽取均匀分布的随机数构造。这成为实现蒙特卡洛方法模拟实验的基本手段，也是蒙特卡洛方法被称为随机抽样的原因。</p><p><strong><c> 通过样本计算各类统计量，此类统计量就是欲求问题的解。</strong></p><p>一般说来，构造了概率模型并能从中抽样后（即实现模拟实验后），我们就要确定一个随机变量，作为所要求的问题的解，我们称它为无偏估计。建立各种估计量，相当于对模拟实验的结果进行考察和登记，从中得到问题的解。</p><h1 id="3）组成部分"><a href="#3）组成部分" class="headerlink" title="3）组成部分"></a>3）组成部分</h1><ul><li>概率密度函数(pdf)：必须给出描述一个物理系统的一组概率密度函数;</li><li>随机数产生器：能够产生在区间[a,b]上满足概率分布的随机数;</li><li>抽样规则：随机抽取服从给定的pdf的随机变量;</li><li>模拟结果记录：记录一些感兴趣的量的模拟结果;</li><li>误差估计：必须确定统计误差（或方差）随模拟次数以及其它一些量的变化;</li><li>减少方差的技术：利用该技术可减少模拟过程中计算的次数;</li><li>并行和矢量化：可以在先进的并行计算机上运行的有效算法。</li></ul><h1 id="4）优点"><a href="#4）优点" class="headerlink" title="4）优点"></a>4）优点</h1><ul><li>对于具有统计性质问题可以直接进行解决；</li><li>对于连续性的问题不必进行离散化处理；</li><li>能够比较逼真地描述具有随机性质的事物的特点及物理实验过程；</li><li>受几何条件限制小；</li><li>方法的误差与问题的维数无关；</li><li>收敛速度与问题的维数无关；</li><li>具有同时计算多个方案与多个未知量的能力；</li><li>误差容易确定；</li><li>程序结构简单，易于实现。</li></ul><h1 id="5）应用"><a href="#5）应用" class="headerlink" title="5）应用"></a>5）应用</h1><p>蒙特卡洛方法应用非常广泛，其领域包括统计物理、天体物理、物理化学、数学、计算生物、统计学、经济学、金融证券、社会科学等等。</p><p>蒙特卡洛方法在解决物理和数学问题时时常被使用。当十分困难或不可能得到解析表达式，或不可能应用确定性算法时，该方法显得尤其重要和最为有用。</p><p>蒙特卡洛方法主要用于两个不同类别的问题：</p><ul><li>确定性的数学问题</li></ul><p>无理数、不规则图形的面积、多重积分、求逆矩阵、解线性代数方程组、解积分方程、解某些偏微分方程边值问题和计算代数方程组、计算微分算子的特征值等等。</p><ul><li>随机性问题</li></ul><p>仿真、概率分布的生成（MCMC算法）。</p><h1 id="应用之一-求无理数π"><a href="#应用之一-求无理数π" class="headerlink" title="应用之一 求无理数π"></a>应用之一 求无理数π</h1><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/b0561f6f3e9645d6a2dd709422827ee8?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>解题思路：</p><p>构造一个单位正方形和一个单位圆的1&#x2F;4，往整个区域内随机投入点，根据点到原点的距离判断点是落在1&#x2F;4的圆内还是在圆外，从而根据落在两个不同区域的点的数目，求出两个区域的比值。如此一来，就可以求出1&#x2F;4单位圆的面积，从而求出圆周率π。</p><p>解题步骤：</p><p>第1步，建模，构造概率过程，选择区域内随机投入点作为随机变量；</p><p>第2步，选择二维均匀分布Y &#x3D; sqrt(h^2+v^2), h、v~U[0,1], 作为已知概率分布抽样；</p><p>第3步，根据落入正方形样本数和1&#x2F;4圆样本数，求出结果。</p><p>演示代码：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from <span class="built_in">random</span> <span class="keyword">import</span> <span class="built_in">random</span></span><br><span class="line">from math <span class="keyword">import</span> <span class="built_in">sqrt</span></span><br><span class="line">N = <span class="number">1000000</span></span><br><span class="line">hits = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i in <span class="title function_">range</span>(<span class="number">1</span>, N):</span><br><span class="line">    x, y = <span class="built_in">random</span>(), <span class="built_in">random</span>();</span><br><span class="line">    <span class="built_in">dist</span> = <span class="built_in">sqrt</span>(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">dist</span> &lt;= <span class="number">1.0</span>:</span><br><span class="line">        hits = hits + <span class="number">1</span></span><br><span class="line">pi = <span class="number">4</span> * (hits/N)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pi的值是 %s&#x27;</span> % pi)</span><br></pre></td></tr></table></figure><h1 id="应用之二-求定积分"><a href="#应用之二-求定积分" class="headerlink" title="应用之二 求定积分"></a>应用之二 求定积分</h1><p>例子1</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/941a0b78288844598f53179757c0d6ba?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>解题思路如下：</p><p>假定随机变量具有密度函数</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/842e34e82e59464ea601d2ae805586b1?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>根据数学期望公式得</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fe9d9ce0938b4f1399f87c48db5e2924?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>构造统计数</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/1ae88d04aaf342b992d041a97684f75d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>根据大数定律</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/13e1e977f945488d9a7cc525af1f3f94?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>最终求出了定积分。</p><p>例子2 求定积分</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d07e429b8d4a48dabbf66dd6a1451ced?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>解题思路：</p><p>将原式变形为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a6dae99640e74713bd7e61b7b59d019d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>解：采用蒙特卡洛方法， MATLAB模拟，</p><p>N&#x3D;500;</p><p>x&#x3D;unifrnd(0,2,N,1);</p><p>mean(2<em>exp(3</em>x.^2))</p><p>第一条语句设定了停止条件，共做N次Monte Carlo 模拟；</p><p>第二条语句实现了在积分区间[0,2]均匀产生N个随机数；</p><p>第三条语句求样本均值，实现蒙特卡洛计算方法的面积逼近。</p><h1 id="应用之三-仿真中子逸出"><a href="#应用之三-仿真中子逸出" class="headerlink" title="应用之三 仿真中子逸出"></a>应用之三 仿真中子逸出</h1><p>图2-4是一个中子穿过用于中子屏蔽的铅墙示意图。铅墙的高度远大于左右厚度。设中子是垂直由左端进入铅墙，在铅墙中运行一个单位距离然后与一个铅原子碰撞。碰撞后，任意改变方向，并继续运行一个单位后与另一个铅原子碰撞。这样下去，如果中子在铅墙里消耗掉所有的能量或者从左端逸出就被视为中子被铅墙挡住，如果中子穿过铅墙由右端逸出就视为中子逸出。如果铅墙厚度为5个单位，中子运行7个单位后能量耗尽，求中子逸出的几率。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/59d495a4df05469c91913849aabdd04f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图2-4</p><p>解题思路：</p><p>这个问题并不复杂，但不容易找到一个解析表达式。而用模拟仿真的方法求解却可以有满意的结果。解题步骤如下：</p><p>第1步，建模，构造概率过程，选择中子碰撞后的角度作为随机变量；</p><p>第2步，选择角度的均匀分布作为已知概率分布抽样；</p><p>第3步，根据假设的总中子样本数和逸出的中子样本数，求出结果。</p><p>解：具体做法如下：</p><p>我们关心的是一次碰撞后，中子在水平方向行进了多少，所以行进方向是正负θ的结果是一样的，我们就只考虑θ是正的情形。由于中子运行的方向θ是随机的，我们用计算机抽取在0到π间均衡分布的随机数，模拟1000000个中子在铅墙里行进的情形，看看这些中子与铅原子碰撞7次后，有多少超过了铅墙的右端。</p><p>实现伪代码</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n = 1000000;</span><br><span class="line">m = 0;</span><br><span class="line">flag = 1;</span><br><span class="line"><span class="keyword">for</span> <span class="attribute">i</span>=1:n</span><br><span class="line">    <span class="attribute">x</span>=1;</span><br><span class="line">    <span class="keyword">for</span> <span class="attribute">j</span>=1:7</span><br><span class="line">        angle = pi * rand;</span><br><span class="line">        x = x + cos(angle);</span><br><span class="line">        <span class="keyword">if</span> x&lt;0</span><br><span class="line">            <span class="attribute">k</span>=0;</span><br><span class="line">            <span class="attribute">flag</span>=0;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    <span class="keyword">if</span> x&gt;5 &amp; <span class="attribute">flag</span>==1</span><br><span class="line">        <span class="attribute">k</span>=1;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        <span class="attribute">k</span>=0;</span><br><span class="line">    end</span><br><span class="line">    m = m + k;</span><br><span class="line">    flag = 1;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>运行结果：m&#x2F;n &#x3D; 1.3%</p><p>我们运行程序得出逸出铅墙的中子的可能性约为1.3%，有了这个数字，我们可以报告安全部门，如果数字不能达到安全要求，我们则要加厚铅墙。</p><h1 id="3、马尔可夫链与蒙特卡洛方法的灵魂附体"><a href="#3、马尔可夫链与蒙特卡洛方法的灵魂附体" class="headerlink" title="3、马尔可夫链与蒙特卡洛方法的灵魂附体"></a>3、马尔可夫链与蒙特卡洛方法的灵魂附体</h1><p>对于给定的概率分布p(x)，我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布，于是一个很漂亮的想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移状态序列</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9dc9ec34772a4c008663a2c5debefc86?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果马氏链在第n步已经收敛了，于是我们就得到了平稳分布p(x)的样本集</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fd51470c608342f4b281618c7b416f7e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这个绝妙的想法在1953年被 Metropolis等人想到了，为了研究粒子系统的平稳性质， Metropolis 考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现。</p><h1 id="三、MCMC有什么用？"><a href="#三、MCMC有什么用？" class="headerlink" title="三、MCMC有什么用？"></a>三、MCMC有什么用？</h1><h1 id="1、贝叶斯推断中很多问题都是MCMC方法的应用"><a href="#1、贝叶斯推断中很多问题都是MCMC方法的应用" class="headerlink" title="1、贝叶斯推断中很多问题都是MCMC方法的应用"></a>1、贝叶斯推断中很多问题都是MCMC方法的应用</h1><p>从贝叶斯(Bayesian)的观点来看，模型中的观测变量和参数都是随机变量，因此，样本x与参数θ的联合分布可以表示为：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/50a50c85dcc84b59a722861d3a520d09?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>根据贝叶斯理论，可以通过样本x的信息对θ的分布的进行更新，后验概率为：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/b1ea055c416640e2902afae5999827cf?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其中π(θ)先验概率；f(x|θ)似然函数（即样本的概率分布）；分母是标准化常量。后验概率 &#x3D; (似然函数<em>先验概率)&#x2F;标准化常量 &#x3D; 标准似然度</em>先验概率。</p><p>贝叶斯学习中经常需要进行三种积分运算：</p><ul><li>归范化（normalization)</li><li>边缘 化（marginalization)</li><li>数学期望（expectation)</li></ul><p>1&gt; 后验概率计算中需要归范化计算：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/7e888bc76bc043a8b946e7d68372a96a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>2&gt; 如果有隐变量z∈Z，后验概率的计算需要边缘化计算：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ba0cb9a1a29b4c38a9a84173102a97c6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>3&gt; 如果有一个函数g(θ)，可以计算该函数的关于后验概率分布的数学期望：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/afe0c001d5d643eaac1c1244d6fb9cb0?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>此积分值为样本x的函数，因此可以对g(θ)进行推断。</p><p>该期望估计</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/b997c25128894bddbc07c8e67dcce0ea?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>当维数很高时，直接进行这三类积分是非常困难的， MCMC方法就是为此目的而诞生，为这些计算提供了一个通用的有效解决方案。</p><h1 id="2、MCMC的作用总结"><a href="#2、MCMC的作用总结" class="headerlink" title="2、MCMC的作用总结"></a>2、MCMC的作用总结</h1><h1 id="1）在贝叶斯推理中主要用于从后验分布的“非标准化部分”中直接生成样本集，避免复杂计算。"><a href="#1）在贝叶斯推理中主要用于从后验分布的“非标准化部分”中直接生成样本集，避免复杂计算。" class="headerlink" title="1）在贝叶斯推理中主要用于从后验分布的“非标准化部分”中直接生成样本集，避免复杂计算。"></a>1）在贝叶斯推理中主要用于从后验分布的“非标准化部分”中直接生成样本集，避免复杂计算。</h1><p>对很多贝叶斯推断问题来说，有时候后验分布过于复杂，使得积分没有显示结果，数值方法也很难应用；有时候需要计算多重积分（比如后验分布是多元分布时）。这些都会带来计算上的很大困难。这也是在很长的时期内，贝叶斯统计得不到快速发展的一个原因。1990年代MCMC计算方法引入到贝叶斯统计学之后，一举解决了这个计算的难题。MCMC方法的最新发展使计算大型分层模型成为可能，这些模型需要对数百到数千个未知参数进行积分。可以说，近年来贝叶斯统计的蓬勃发展，特别是在各个学科的广泛应用和MCMC方法的使用有着极其密切的关系。</p><h1 id="2）从任意给定的概率分布或复杂分布中获取随机样本集"><a href="#2）从任意给定的概率分布或复杂分布中获取随机样本集" class="headerlink" title="2）从任意给定的概率分布或复杂分布中获取随机样本集"></a>2）从任意给定的概率分布或复杂分布中获取随机样本集</h1><p>MCMC算法是一个普适的采样方法，它的出现带来了随机模拟技术的腾飞。在随机变量生成技术中，MCMC是一种相当高级的方法，可以从一个非常困难的概率分布中获得样本。更出乎意料的是，可以用MCMC从一个未经标准化的分布中获得样本，这来自于定义马尔可夫链的特定方式，马尔可夫链对这些归一化因子并不敏感。</p><p>对高维（多个自变量）积分的数值近似计算是MCMC方法的一种重要的应用，很多统计问题，比如计算概率、矩（期望、方差等）都可以归结为定积分的计算。</p><p>MCMC的精髓在于构造合适的马氏链。</p><p>MCMC是一种简单有效的计算方法，在很多领域到广泛的应用，如统计学、贝叶斯(Bayes)问题、计算机问题等。</p><h1 id="四、MCMC生成的样本集长啥样？"><a href="#四、MCMC生成的样本集长啥样？" class="headerlink" title="四、MCMC生成的样本集长啥样？"></a>四、MCMC生成的样本集长啥样？</h1><p>MCMC背后的基本思想是构造一个遍历的马尔可夫链，使得其平稳的、收敛的极限分布成为人们所需要的目标概率分布。</p><p>设马尔可夫链的状态空间S&#x3D;{ω1，ω2，…ωn}（可以理解为样本空间在随机过程中的表达），从某个初始状态x0出发，用构造的马尔可夫链随机游走，产生状态样本序列:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/2820f14d7e144d77ab4cb85b3d1b48e6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>舍弃前面的老化(不稳定)的样本集合</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/6b4cbec88d944a9eb620d811c17a8807?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>应用遍历定理，确定正整数m和n, 得到平稳分布的状态样本集合：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ddfb61393f344060b26974a73aada48e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>该样本集可能达到几万、几十万、几千万甚至更大的规模。统计该状态样本集中对应的各个状态ω1，ω2…出现的频次，归一化后画成直方图。如图4-1所示，绿色部分就是状态概率分布（即抽样概率分布），红色部分是目标概率分布。t趋近∞时，抽样概率分布可以无限逼近目标概率分布。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/388ac2ef2d71460f9bfb17bedca22f89?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图4-1</p><h1 id="五、MCMC与随机采样法"><a href="#五、MCMC与随机采样法" class="headerlink" title="五、MCMC与随机采样法"></a>五、MCMC与随机采样法</h1><p>平时我们接触比较多的场景是，给定一堆样本数据，求出这堆样本的概率分布p(x)。而采样刚好是个逆命题：给定一个概率分布p(x)，如何生成满足条件的样本？</p><h1 id="1、为什么需要采样样本？"><a href="#1、为什么需要采样样本？" class="headerlink" title="1、为什么需要采样样本？"></a>1、为什么需要采样样本？</h1><p>通过样本可以计算各类统计量（如均值、方差、矩等），而此类统计量就是欲求问题的近似解。例如求积分，根据大数定律，相当于求某概率分布采样样本的均值。</p><h1 id="2、采样必须满足的条件"><a href="#2、采样必须满足的条件" class="headerlink" title="2、采样必须满足的条件"></a>2、采样必须满足的条件</h1><h1 id="1）足够地随机"><a href="#1）足够地随机" class="headerlink" title="1）足够地随机"></a>1）足够地随机</h1><p>蒙特卡罗模拟有一个危险的缺陷：如果必须输入一个模式中的随机数并不像设想的那样是随机数，而却构成一些微妙的非随机模式，那么整个的模拟和预测结果都可能是错的。</p><p>对于给定的概率分布来说，采样必须是随机的，这个非常重要。随机采样对于人来说，<strong>是不能也，非不为也</strong>。依靠人来采样很难得到真正的随机样本，人为地通过概率分布函数计算得到一批样本注定是徒劳的。</p><p>蒙特卡罗模拟要取决于可靠的无穷尽的随机数目来源。产生随机数要靠随机数发生器，随机数发生器主要包括真（物理性）随机数发生器和伪随机数发生器。</p><p>真随机数生成器（True Random Number Generator, TRNG）是一种通过物理过程而不是计算机程序来生成随机数字的设备。通常是基于一些能生成随机的“噪声”信号的微观现象，如热力学噪声、光电效应和量子现象等，其物理过程在理论上是完全不可预测的，并且这种不可预测性要经过实验检验。真随机数生成器通常由换能器、放大器和模拟数字转换器组成。其中换能器用来将物理过程中的某些效果转换为电信号，放大器及其电路用来将随机扰动的振幅放大到宏观级别，而模拟数字转换器则用来将输出变成数字，通常是二进制的零和一。通过重复采样这些随机的信号，一系列的随机数得以生成。</p><p>硬件随机数发生器通常每秒仅产生有限数量的随机位。为了增加可用的输出数据速率，通常将它们用于生成“种子”，以用于更快的密码安全伪随机数生成器，然后生成器以更高的数据速率生成伪随机输出序列。</p><p>伪随机数发生器是依靠计算机按照一定算法模拟产生的，其结果是确定的，是可见的。通过这种方式产生的“随机数”并不随机，是伪随机数。贝尔实验室的里德博士告诫人们记住伟大的诺依曼的忠告:“任何人如果相信计算机能够产生出真正的随机的数序组都是疯子。”</p><p>伪随机数存在两大问题</p><ul><li>递推公式和初始值确定后，整个随机数序列便被唯一确定，不满足随机数相互独立的要求。</li><li>随机数序列会出现周期性的循环现象。</li></ul><p>真随机数的生成对技术的要求比较高，在实际应用中往往使用伪随机数就够了。在使用伪随机数时，要尽量克服其存在的两大问题。对于第一个问题，不能从本质上加以改变，但只要递推公式选的比较好，随机数间的相互独立性是可以近似满足的；对于第二个问题，因为用蒙特卡罗方法解任何具体问题时，所使用的随机数的个数总是有限的，只要所用随机数的个数不超过伪随机数序列出现循环现象时的长度就可以了。</p><h1 id="2）足够多的采样样本"><a href="#2）足够多的采样样本" class="headerlink" title="2）足够多的采样样本"></a>2）足够多的采样样本</h1><p>我们通过采样获取足够多的样本， 然后统计样本空间中的样本出现的频次，来模拟和逼近目标概率分布的。采样样本越多，越逼近目标概率分布。如图5-1所示。</p><p>几万或几十万的样本集是小目标；几百万或几千万的样本集很常见；在AI和大数据时代，目标概率分布有时会涉及到几百个、几千个参数， 几百亿的样本集都出现了。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d9dcba0ecae64d19904b9caac121b9e9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-1</p><h1 id="3）-尽量覆盖样本空间"><a href="#3）-尽量覆盖样本空间" class="headerlink" title="3） 尽量覆盖样本空间"></a>3） 尽量覆盖样本空间</h1><p>这与算法和采样策略有关。</p><h1 id="4）-能模拟给定的目标分布"><a href="#4）-能模拟给定的目标分布" class="headerlink" title="4） 能模拟给定的目标分布"></a>4） 能模拟给定的目标分布</h1><p>这与算法有关，要从理论上证明。</p><h1 id="3、常见的随机采样方法"><a href="#3、常见的随机采样方法" class="headerlink" title="3、常见的随机采样方法"></a>3、常见的随机采样方法</h1><h1 id="1）-U-0，1均匀分布采样法（Uniform-Sampling）"><a href="#1）-U-0，1均匀分布采样法（Uniform-Sampling）" class="headerlink" title="1） U~[0，1均匀分布采样法（Uniform Sampling）"></a>1） U~[0，1均匀分布采样法（Uniform Sampling）</h1><p>例如：线性同余伪随机数生成器</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/f03b99dbbb7b44c799cd132564adf1bd?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3edb5ba2328346a8bda4fc03ad468097?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>X是伪随机序列；</p><p>m, m&gt;0表示模量；</p><p>a, 0&lt;a&lt;m表示乘数；</p><p>c, 0≤c&lt;m表示增量；</p><p>X0， 0≤X0&lt;m表示初始值；</p><p>如果c&#x3D;0, 称为乘法同余发生器，如果c!&#x3D;0, 称为混合同余发生器；</p><p>Yn则是 [0, 1)内服从均匀分布的随机变量。</p><h1 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h1><p>均匀分布随机数可以直接由计算机模拟产生。</p><h1 id="2）-Box–Muller变换法"><a href="#2）-Box–Muller变换法" class="headerlink" title="2） Box–Muller变换法"></a>2） Box–Muller变换法</h1><p>Box–Muller 变换是一种利用均匀分布快速产生符合标准正态分布伪随机数对的一种方法。基本思想是先得到服从均匀分布的随机数，再将服从均匀分布的随机数转变为服从正态分布。</p><h1 id="I-标准形式的-Box–Muller-变换"><a href="#I-标准形式的-Box–Muller-变换" class="headerlink" title="I 标准形式的 Box–Muller 变换"></a>I 标准形式的 Box–Muller 变换</h1><p>假设变变量 U1 和变量 U2 是（0，1]均匀分布的随机数，</p><p>且 U1 和 U2 彼此独立，令：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/369714742cfa42cbae6b5d8e2253f054?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则 Z0 和 Z1 就是服从 N（0，1）的标准正态分布随机数，并且 Z0 和 Z1 相互独立。</p><h1 id="II-极坐标形式的-Box–Muller-变换"><a href="#II-极坐标形式的-Box–Muller-变换" class="headerlink" title="II 极坐标形式的 Box–Muller 变换"></a>II 极坐标形式的 Box–Muller 变换</h1><p>假设变量 u 和变量 v 是在[-1，1]上的均匀分布随机量，</p><p>u 和 v 相互独立，令：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/0017dca313a2495bacb864543644e45a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>故而，随机数 z0 和 z1 计算后得出如下结果：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/8669d49dfd674403bb499cd6453052d5?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>z0 和 z1 是服从分布 N（0，1）的随机数，并且 z0 和 z1 相互 独立。</p><h1 id="3）-逆变换法（Inverse-Transform-Method）"><a href="#3）-逆变换法（Inverse-Transform-Method）" class="headerlink" title="3） 逆变换法（Inverse Transform Method）"></a>3） 逆变换法（Inverse Transform Method）</h1><p>逆变换法就是利用累积分布函数求逆，将所求概率分布的抽样转化为随机均匀分布的抽样。</p><h1 id="I-连续随机变量逆变换法"><a href="#I-连续随机变量逆变换法" class="headerlink" title="I 连续随机变量逆变换法"></a>I 连续随机变量逆变换法</h1><p>设所求连续分布采样的连续随机变量X的概率密度函数为f (x) 。</p><p>第1步，求概率累积分布函数F(x)</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e5813a53997c4b33983ee3dd8dda912a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>F(x)是单调递增的连续函数，令</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/34e1ddc6c2b04ce49a697060ea69c410?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则U也是随机变量，且U∈[0, 1]。</p><p>第2步，求反函数</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d53a36985a434d71a14411846bc8757e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>第3步，随机产生均匀分布U[0,1]，求出对应的X，就可得到所求概率分布的采样。</p><h1 id="II-离散随机变量逆变换法"><a href="#II-离散随机变量逆变换法" class="headerlink" title="II 离散随机变量逆变换法"></a>II 离散随机变量逆变换法</h1><p>设所求离散分布采样的离散随机变量的概率密度为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/f5c30da3ebce405198989d64b1df94d0?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>第1步，生成均匀分布U[0,1]随机数。</p><p>第2步，求出对应的X</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/6c09aa902f6d4fd0b031559605a25bc8?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>也就是如下分段式：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/108c536bcbe3483b9692c456a928d25d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>X就是所求概率分布的采样。</p><h1 id="III-轮盘赌采样"><a href="#III-轮盘赌采样" class="headerlink" title="III 轮盘赌采样"></a>III 轮盘赌采样</h1><p>任意离散分布都可以画在轮盘上，采样只需要随机地旋转轮盘即可。如图5-2所示。</p><p>该轮盘模拟了累积分布函数，随机转动轮盘相当于随机均匀采样，最后轮盘的停止位置就是采样值。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fe769819655b47c6bf1e9a47683aaf39?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-2</p><h1 id="小结：-1"><a href="#小结：-1" class="headerlink" title="小结："></a>小结：</h1><p>优点：随机数可以直接由均匀分布导出。</p><p>缺点：很多复杂累积分布函数求逆困难。</p><h1 id="4）接受-拒绝采样法（Acceptance-Rejection-Sampling）"><a href="#4）接受-拒绝采样法（Acceptance-Rejection-Sampling）" class="headerlink" title="4）接受-拒绝采样法（Acceptance-Rejection Sampling）"></a>4）接受-拒绝采样法（Acceptance-Rejection Sampling）</h1><p>实际随机分布抽样中会遇到如下问题：</p><ul><li>概率密度函数p(x) 太复杂无法直接采样；</li><li>逆变换中一些累积概率分布函数无法求逆。</li></ul><p>接受-拒绝采样法就派上了用场。基本思想是选择一个容易抽样的随机分布函数q(x)和常数k, 使得k*q(x)≥p(x)（这里p(x)可以进行非归一化，乘以常数Z，以下省略），然后按照一定的方法拒绝某些抽样，达到接近p(x)分布的目的。</p><p>如图5-3所示：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/60c9af6850634d0eb9ef5f8b93e90f5f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4a6b87f8a74f4f979dcf002d5b559a9e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-3</p><p>假设我们选取已知的高斯分布q(x)&#x3D;Gassian(1.4, 1.2)，我们按照一定的方法拒绝某些样本，达到接近p(x)分布的目的。</p><p>具体步骤如下：</p><p>第1步，选择已知的分布函数q(x)，确定常量k，使得p(x)总在kq(x)的下方。</p><p>第2步，在x轴上，从q(x)分布抽样取得a。但是a不一定留下，会有一定的几率被拒绝。</p><p>第3步，在y轴上，从均匀分布(0,1)中抽样得到u。如果u.kq(a)&gt;p(a)，就是落到了灰色的区域中，拒绝；否则接受这次抽样。</p><p>第4步，重复2、3步。</p><p>模拟结果如图5-4所示：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/c794d82299ef4d04ba9d7e1f28246e4d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-4</p><h1 id="小结：-2"><a href="#小结：-2" class="headerlink" title="小结："></a>小结：</h1><p>在高维（多个自变量）分布的情况下，接受-拒绝采样会出现如下一些问题：</p><p>1&gt; 合适的提议分布q(x)比较难以找到，并且当kq(x)与p(x)相差太多时，会导致拒绝率很高，无用计算增加。</p><p>2&gt; 很难确定一个合理的常数 k 值, 并且k值往往会很大；</p><p>3&gt; 所需的样本量随空间维数增加而指数增长。</p><h1 id="5）-重要性采样法（Importance-Sampling）"><a href="#5）-重要性采样法（Importance-Sampling）" class="headerlink" title="5） 重要性采样法（Importance Sampling）"></a>5） 重要性采样法（Importance Sampling）</h1><p>重要性采样是统计学中估计某一分布性质时使用的一种方法，该方法从与原分布不同的新分布中采样，而对原分布的性质进行估计。重要性采样常用于蒙特卡洛积分。</p><h1 id="作用一-降方差"><a href="#作用一-降方差" class="headerlink" title="作用一 降方差"></a>作用一 降方差</h1><p>重要性采样是蒙特卡洛方法中的一个重要策略。该方法不改变统计量，只改变概率分布。重要性采样算法就是在有限的采样次数内，尽量让采样点覆盖对积分贡献很大的点或者以一种受控的方式改变仿真，以便增加稀少事件的数目。它是一种降方差的仿真方法。</p><h1 id="作用二-将复杂的无法抽样的分布转化为简单的容易抽样的分布，间接求一些统计量f-x"><a href="#作用二-将复杂的无法抽样的分布转化为简单的容易抽样的分布，间接求一些统计量f-x" class="headerlink" title="作用二 将复杂的无法抽样的分布转化为简单的容易抽样的分布，间接求一些统计量f(x)"></a>作用二 将复杂的无法抽样的分布转化为简单的容易抽样的分布，间接求一些统计量f(x)</h1><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/bf3d7751196249789a9405fd66fee5d4?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其中， q(x)为提议分布(proposal distribution)，p(x) &#x2F; q(x)可以看做 importance weight。</p><h1 id="小结：-3"><a href="#小结：-3" class="headerlink" title="小结："></a>小结：</h1><p>优点： 如果我们无法从分布p(x)中抽样, 或者从中抽样的成本很高, 那么我们还可以求E[f(x)]吗？答案是可以的, 我们可以从一个简单的分布q(x)中进行抽样得到x, 求f(x)p(x)&#x2F;q(x)的期望，进而可以求得函数积分的近似解。</p><p>缺点：在高维空间里找到一个这样合适的q(x)非常难。</p><h1 id="6）-MCMC采样法"><a href="#6）-MCMC采样法" class="headerlink" title="6） MCMC采样法"></a>6） MCMC采样法</h1><h1 id="（1）MCMC思想的来源"><a href="#（1）MCMC思想的来源" class="headerlink" title="（1）MCMC思想的来源"></a>（1）MCMC思想的来源</h1><p>拒绝采样法和重要性采样法面临两个问题：</p><p>1&gt; 如果提议分布q(x)与目标分布p(x)差距过大，模拟采样效果就不太好；</p><p>2&gt; 构造一个与目标分布p(x)相似的提议分布q(x)很困难。</p><p>我们知道，对于任意的目标分布，我们可以设计一种接受-拒绝的概率，从而使得所有接受的点都是该分布的样本。拒绝采样法和重要性采样法采用了提议函数q(x)和接受概率实现了这种思想，然而问题在于，由于其提议分布q(x)是固定的，这对q(x)的要求很高，试想下，如果q与真实p的差距过大，那么几乎每个样本都会被拒绝掉，效率很低。那如何设计更好的q呢？一个巧妙的办法是设计一个自适应的提议分布 q(x’|x)，它以上一次接受的样本作为条件，然后经过转移propose一个新的样本，如果我们能够约束这个转移不会改变目标分布，那么我们就能快速的找到目标分布p(x)的样本。如图5-5所示，这也正是MCMC的思想。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/6e508995c9ba4c5db5f477a944dc9074?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-5</p><p>另一方面，大多数拒绝采样法和重要性采样法都遭受“维数的诅咒”，其中拒绝的概率随维数的增加呈指数增长。 MCMC方法都没有这个“维数诅咒”的问题，当要采样的分布维数很大时，它通常是唯一可用的解决方案。结果，MCMC方法成为了从分层贝叶斯模型和当今许多学科中使用的其它高维统计模型中生成样本的首选方法。MCMC采样是一种普适采样方法，它可以解决其它采样方法无法搞定的很多问题。</p><h1 id="（2）MCMC中的提议分布q"><a href="#（2）MCMC中的提议分布q" class="headerlink" title="（2）MCMC中的提议分布q(.)"></a>（2）MCMC中的提议分布q(.)</h1><p>提议分布q(x, x’) &#x3D; q(x’|x)通常要满足以下三个条件：</p><p>1&gt; 对于固定的x, q(.)是一个概率密度函数;</p><p>2&gt; 对于∀x, x’∈S, q(x, x’)的值要能计算;</p><p>3&gt; 对于固定的x, 能够方便地从q(x,x’)中产生随机数。</p><p>从理论上讲，提议函数q(.)的选取是任意的，但在实际计算中，提议函数的选取对于算法效率的影响是相当大的。一般认为提议函数与目标分布越接近，模拟的效果越好。</p><p>q(.)函数的选择一般有两种：</p><p>1&gt; 对称: 如高斯分布，均匀分布，这种情况下称为随机游走;</p><p>2&gt; 非对称：如 log-normal，采样值倾向于选择高概率的点，因为我们不希望采样点来回震荡。</p><h1 id="（3）-MCMC采样原理"><a href="#（3）-MCMC采样原理" class="headerlink" title="（3） MCMC采样原理"></a>（3） MCMC采样原理</h1><p>MCMC采样法的关键是构建马氏链，马氏链的收敛性质主要由状态转移矩阵P决定, 所以基于马氏链做采样的关键问题是如何构造状态转移矩阵P, 使得马氏链具有一个平稳分布，且该平稳分布π(x)恰好是我们要的分布p(x)。如何能做到这一点呢？</p><p>答案的关键在细致平衡条件</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/bc07f2457ce440d8b2ac87b81a3f3fe7?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其中i，j∈S (S是状态空间)，P是状态转移矩阵。</p><p>不幸的是，对于平稳分布π(x)，即我们要的分布p(x)，要找到满足细致平衡条件的状态矩阵P很困难。没关系，我们随机找一个已知的马尔科夫链状态转移矩阵Q，其中Q(i,j)&#x3D;Q(j|i)是从状态i转移到状态j的提议分布条件概率，i,j∈状态空间S，它可能一开始并不满足细致平衡条件，即：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3e532f0e9266489fbe68bec8aa8a9e9a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们可以对上式做一个改造，使细致平衡条件成立。方法是引入一个α(i,j) ,使上式可以取等号，即：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a410e9633e1c4d498663e180e04e8a33?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>要使该等式成立，只要满足下面两式即可</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/26be701f1a1d46de9537b0c3f6210f99?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这样，我们就得到了平稳分布π(x)对应的马尔科夫链状态转移矩阵P</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9cdbb3e96cad4eecb492737a2bb17f82?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们有一般称α(i,j)为接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9e0ca613399e40058892b489e2871568?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其取值在(0,1)之间，可以理解为一个概率值。</p><p>也就是说，基于平稳分布的状态转移矩阵P可以通过任意一个马尔科夫链状态转移矩阵Q以一定的接受概率获得。在接受-拒绝采样法中，那里是以一个常见分布通过一定的接受-拒绝概率得到目标概率分布，这里是以一个常见的马尔科夫链状态转移矩阵Q通过一定的接受-拒绝概率得到目标转移矩阵P，两者解决问题的思路是类似的。</p><p>由于α(i,j) &#x3D;π(j)Q(j, i)&lt;1, 通常是一个比较小的数，因此在一次采样中很容易拒绝跳转，从而导致采样的时间成本、计算成本很高。</p><p>因为α(i,j)&lt;1和α(j,i)&lt;1, 我们可以将两者同时放大但不超过1，且不破环等式成立。</p><p>1&gt; 当α(i,j) ≤ α(j,i)时，则α(i,j)按照α(j,i)放大到1（概率不可能大于1）的比例等比例放大:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3d576847cba846b59d28059c6e98ba96?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>2&gt; 当α(i,j) &gt; α(j,i)时，则α(j,i)按照α(i,j)放大到1的比例等比例放大:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4831c27cef29434893ddd4df1e0c12b3?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>所以将两者同时放大有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/0adf6756e47a4ce987296bb75a661093?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>此时α(i,j)就是Metropolis-Hasting算法的接受概率。其中π(x)&#x3D;p(x), Q(i,j)&#x3D;Q(j|i)是提议分布条件概率，α(i,j)在取到1的情况下能实现链的满概率跳转，否则以放大1&#x2F;(π(i)Q(i,j))倍的概率进行跳转。</p><p>当提议分布对称时，即Q(j, i) &#x3D; Q(i, j)， 接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e72c4a0fe0aa4f21bd80b6a3b42c018e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>此时α(i,j)就是Metropolis算法的接受概率。其中π(x)&#x3D;p(x)。</p><p>由任选的提议分布Q(i,j)和接受概率α(j,i), 可以构造一个满足细致平衡条件的马氏链， 该马氏链具有一个平稳分布。构造平稳分布的马氏链的过程就是我们实现MCMC采样的过程。</p><h1 id="（4）MCMC几种基本算法"><a href="#（4）MCMC几种基本算法" class="headerlink" title="（4）MCMC几种基本算法"></a>（4）MCMC几种基本算法</h1><p>MCMC算法包括Metropolis算法、Metropolis-Hasting 采样法、吉布斯采样法等。</p><h1 id="I、-Metropolis算法"><a href="#I、-Metropolis算法" class="headerlink" title="I、 Metropolis算法"></a>I、 Metropolis算法</h1><p>已知目标概率分布p(x)，如何对其进行采样？</p><p>我们分别采用接受-拒绝算法和Metropolis算法进行采样，并对两者做了对比。</p><p>接受-拒绝算法的具体步骤如下：</p><p>1&gt; 首先选择一个容易抽样的分布作为提议分布，记为q(x)，接着确定一个常数C，其中 C &gt; 1， 使得在x的定义域上都有p(x) ≤ Cq(x)。</p><p>2&gt; 生成服从提议概率密度函数q(x)的随机数y。</p><p>3&gt; 接着生成一个服从均匀分布U (0,1) 的随机数u。</p><p>4&gt; 计算接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/1b0b93d9ceea4d3c99347ad050a22f0f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果u &lt; α(y)，则接受y; 否则，拒绝。</p><p>5&gt; 不断重复2~ 4就可以生成一组服从目标分布p(x)的随机数序列。</p><p>Metropolis算法就是改进的接受-拒绝算法，具体步骤如下：</p><p>1&gt; 首先构造提议分布q(.|x(t)), 这里提议分布是对称的，即q(y|x)&#x3D;q(x|y)， 例如，我们可以选均值为u&#x3D;x(t)的正态分布为提议分布。设置t&#x3D;0和初始状态x0。</p><p>2&gt; 从提议概率分布q(.|x(t))中产生一个随机数y作为下一个状态。</p><p>3&gt; 接着生成一个服从均匀分布U (0,1) 的随机数u。</p><p>4&gt; 计算接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/eff4daa36ab44fae8c4efabdb40b9fcc?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果u ≤ α，则接受候选随机数y, 赋值x(t+1)&#x3D;y，y作为提议分布的新状态，意味着此操作将鼓励跳转到目标分布的高密度区。</p><p>如果u &gt; α，则拒绝候选随机数，赋值x(t+1)&#x3D;x(t)，表明f(x)在此y状态为低密区, 以概率的形式被拒绝。</p><p>5&gt; 增量设置t &#x3D; t+1。</p><p>6&gt; 重复步骤2~步骤5，我们就可以得到一组仅依赖于上一个随机数并且与前面随机数无关的随机序列</p><p>Metropolis算法与接受-拒绝算法不同之处在于：</p><p>1&gt; Metropolis算法的提议分布是自适应的条件概率分布，产生的随机数是由上一次的状态值转移得到；接受-拒绝算法的提议分布是固定的，没有状态转移的概念。</p><p>2&gt; 两者的接受概率定义不同。</p><p>Metropolis算法中唯一的限制是提议分布必须是对称的，满足条件的这类分布有正态分布、柯西分布、t分布和均匀分布等。为了能够使用非对称分布，或者目标分布的支持集是不对称的，如x∈(0,+∞), 我们将考虑使用Metropolis-Hastings算法。</p><p><strong>例子</strong> 已知： (贝叶斯推断: 一个简单的投资模型) 假设有5种股票被跟踪记录了250的交易日每天的表现, 在每一个交易日, 收益最大的股票被标记出来。用Xi表示股票i在250个交易日中胜出的天数, 则记录到得的频数(x1,x2,… , x5)为随机变量(X1,X2,…,X5) 的观测。基于历史数据, 假设这5种股票在任何给定的一个交易日能胜出的先验机会比率为1 : (1 − β) : (1 − 2β) : 2β : β, 这里β∈(0, 0.5)是一个未知的参数。在有了当前这250个交易日的数据后, 使用Bayes方法对此比例进行更新。</p><p>求：根据观测的数据求β目标分布随机抽样的样本集，进而估计β？</p><p>解：根据前述，(X1,X2,…,X5)在给定β的条件下服从多项分布, 概率向量为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4910ed0d0cce42688e437a7624cdc44d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>因此β服从后验分布为（根据联合密度的多项分布公式，把x1,x2,…,x5看作常数）</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/26f507b48013446193d8f1b4bd4b449b?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>我们不能直接从此后验分布中产生随机数。一种估计β的方法是产生一个链, 使其平稳分布为此后验分布, 然后从产生的链中抽样来估计β.</p><p>我们这里使用随机游动Metropolis算法, 可以初始化β&#x3D;0.2, 提议分布为均匀分布U[0,1], 收敛次数burn&#x3D;1000, 迭代次数m&#x3D;5000, 接受的概率为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/db66e81c110d4831b599edd92d53b7d1?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>因为后验分布即为所求的目标分布f(x), 可得</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e794bad27f724900994297c7d87e7147?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>观测数据(x1, x2, x3, x4, x5)是5只股票服从概率向量p在250天中胜出的频次。</p><h1 id="R语言的演示代码："><a href="#R语言的演示代码：" class="headerlink" title="R语言的演示代码："></a>R语言的演示代码：</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">b &lt;- .2 #actual value of beta</span><br><span class="line">w &lt;- .25 #width of the uniform support <span class="built_in">set</span></span><br><span class="line">m &lt;- 5000 #length of the chain 迭代次数</span><br><span class="line">burn &lt;- 1000 #burn-in time 收敛次数</span><br><span class="line">days &lt;- 250</span><br><span class="line">x &lt;- numeric(m) #the chain</span><br><span class="line"><span class="comment"># generate the observed frequencies of winners</span></span><br><span class="line"><span class="comment"># 首先生成观测数据，即5只股票250天按β=0.2计算各自的概率产生的胜出频次。</span></span><br><span class="line">i &lt;- sample(1:5, <span class="attribute">size</span>=days, <span class="attribute">replace</span>=<span class="literal">TRUE</span>,</span><br><span class="line"><span class="attribute">prob</span>=c(1, 1-b, 1-2<span class="number">*b</span>, 2<span class="number">*b</span>, b))</span><br><span class="line">win &lt;- tabulate(i)</span><br><span class="line"><span class="built_in">print</span>(win)</span><br><span class="line"><span class="comment"># 计算分布概率值</span></span><br><span class="line">prob &lt;- function(y, win) &#123;</span><br><span class="line"><span class="comment"># computes (without the constant) the target density</span></span><br><span class="line"><span class="keyword">if</span> (y &lt; 0 || y &gt;= 0.5)</span><br><span class="line">return (0)</span><br><span class="line">return((1/3)^win[1] *</span><br><span class="line">((1-y)/3)^win[2] * ((1-2*y)/3)^win[3] *</span><br><span class="line">((2*y)/3)^win[4] * (y/3)^win[5])</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># Random Walk Metropolis algorithm</span></span><br><span class="line"><span class="comment"># 使用随机游动Metropolis算法产生随机数</span></span><br><span class="line">u &lt;- runif(m) #<span class="keyword">for</span> accept/reject <span class="keyword">step</span></span><br><span class="line">v &lt;- runif(m, -w, w) #proposal distribution</span><br><span class="line">x[1] &lt;- .25</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> 2:m) &#123;</span><br><span class="line">y &lt;- x[i-1] + v[i]</span><br><span class="line"><span class="comment">#计算接受概率</span></span><br><span class="line"><span class="keyword">if</span> (u[i] &lt;= prob(y, win) / prob(x[i-1], win))</span><br><span class="line">x[i] &lt;- y</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">x[i] &lt;- x[i-1]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 链的路径图显示链己经收敛到目标分布</span></span><br><span class="line">par(<span class="attribute">mfrow</span>=c(1,2))</span><br><span class="line">plot(x, <span class="attribute">type</span>=<span class="string">&quot;l&quot;</span>)</span><br><span class="line">abline(<span class="attribute">h</span>=b, <span class="attribute">v</span>=501, <span class="attribute">lty</span>=3)</span><br><span class="line">xb &lt;- x[- (1:501)]</span><br><span class="line">hist(xb, <span class="attribute">prob</span>=<span class="literal">TRUE</span>, <span class="attribute">xlab</span>=bquote(beta), <span class="attribute">ylab</span>=<span class="string">&quot;X&quot;</span>, <span class="attribute">main</span>=<span class="string">&quot;&quot;</span>)</span><br><span class="line">z &lt;- seq(min(xb), max(xb), <span class="attribute">length</span>=100)</span><br><span class="line">lines(z, dnorm(z, mean(xb), sd(xb))</span><br><span class="line"><span class="comment"># 从而产生的随机数在丢弃初始的burn-in部分后可以用来估计β.</span></span><br><span class="line"><span class="comment"># β的估计, 样本频率和MCMC估计的多项分布概率由如下代码给出</span></span><br><span class="line"><span class="built_in">print</span>(win)</span><br><span class="line"><span class="built_in">print</span>(round(win/days, 3))</span><br><span class="line"><span class="built_in">print</span>(round(c(1, 1-b, 1-2<span class="number">*b</span>, 2<span class="number">*b</span>, b)/3, 3))</span><br><span class="line">xb &lt;- x[(burn+1):m]</span><br><span class="line"><span class="built_in">print</span>(mean(xb))</span><br><span class="line"><span class="built_in">print</span>(sd(xb))</span><br></pre></td></tr></table></figure><h1 id="小结：-4"><a href="#小结：-4" class="headerlink" title="小结："></a>小结：</h1><p>Metropolis算法相邻样本是相关的，无法正确反映分布。这意味着，如果我们想要一组独立的样本，最后还必须丢弃一些样本。可以通过增加跳跃宽度选取样本来降低自相关。</p><h1 id="II、-Metropolis-Hastings算法"><a href="#II、-Metropolis-Hastings算法" class="headerlink" title="II、 Metropolis-Hastings算法"></a>II、 Metropolis-Hastings算法</h1><p>Metropolis算法的应用受到了提议概率分布的对称形式的限制，因此，Hastings把提议概率分布从对称分布形式推广到一般情况，形成了Metropolis-Hastings算法，简称M-H算法，也称之为通用梅特罗波利斯算法。</p><h1 id="lt-1-gt-、M-H算法对于一维目标概率分布的采样"><a href="#lt-1-gt-、M-H算法对于一维目标概率分布的采样" class="headerlink" title="&lt;1&gt;、M-H算法对于一维目标概率分布的采样"></a>&lt;1&gt;、M-H算法对于一维目标概率分布的采样</h1><h1 id="M-H算法具体步骤如下："><a href="#M-H算法具体步骤如下：" class="headerlink" title="M-H算法具体步骤如下："></a>M-H算法具体步骤如下：</h1><p>1&gt; 首先构造提议概率分布q(.|x(t))。设置t&#x3D;0和初始状态x0。</p><p>2&gt; 从提议分布q(.|x(t))中产生一个随机数y作为下一个状态。</p><p>3&gt; 接着生成一个服从均匀分布U (0,1) 的随机数u。</p><p>4&gt; 计算接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/f36cb81fb802449c91e75a7ffec0dbbf?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果u ≤ α，则接受候选随机数y, 赋值x(t+1)&#x3D;y;</p><p>如果u &gt; α，则拒绝候选随机数，赋值x(t+1)&#x3D;x(t)。</p><p>5&gt; 增量设置t &#x3D; t+1。</p><p>6&gt; 重复步骤2~步骤5，我们就可以得到一组仅依赖于上一个随机数并且与前面随机数无关的随机序列。</p><h1 id="采样演示如下："><a href="#采样演示如下：" class="headerlink" title="采样演示如下："></a>采样演示如下：</h1><p>第1次采样，选择初始状态x0</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/da5209b48105423e8027244407ea7e34?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-6a</p><p>第2次采样，接受候选样本x1</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a987e1e14c02467d93a1f279ec472032?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>第3次采样，接受候选样本x2</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3bb41ee6a98b422099b50715fac67a9f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-6c</p><p>第4次采样，拒绝候选样本x3</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e1f7073b3d18440281b4ff04bbf06442?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-6d</p><p>第5次采样，接受候选样本x4</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/bdaf1907e7264976966ae36635eac066?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-6e</p><h1 id="lt-2-gt-、M-H算法对于高维目标概率分布的采样"><a href="#lt-2-gt-、M-H算法对于高维目标概率分布的采样" class="headerlink" title="&lt;2&gt;、M-H算法对于高维目标概率分布的采样"></a>&lt;2&gt;、M-H算法对于高维目标概率分布的采样</h1><h1 id="满条件分布定义"><a href="#满条件分布定义" class="headerlink" title="满条件分布定义"></a>满条件分布定义</h1><p>马尔可夫链蒙特卡罗法的目标分布通常是多元联合概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d92dac17be134a7c892ce2cdeb60bacc?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果条件概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3270f5b82f6b4ffdb73ca9d3486d159a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>中所有 k个变量全部出现，其中</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/cf45c741b68c4e48aee6781e608887a4?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>那么称这种条件概率分布为满条件分布（full conditional distribution)。</p><p>满条件分布有以下性质：</p><p>1&gt; 对任意的x, x’和任意的I⊂K, 有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5ac2bf22be5441dc87585d94926916b4?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>2&gt; 对任意的x, x’和任意的I⊂K, 有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fedceeb0ce84477c8ff4ec552bcbbe4b?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>M-H算法中，可以利用上述性质，简化计算，提高计算效率。具体地，通过满条件分布概率的比</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/68fd026416be4c72a0e47fd45b53edc9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>代替计算联合概率的比</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e704492f0df549a8bf991faac3d60782?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>而前者更容易计算。</p><h1 id="单分量Metropolis-Hastings算法"><a href="#单分量Metropolis-Hastings算法" class="headerlink" title="单分量Metropolis- Hastings算法"></a>单分量Metropolis- Hastings算法</h1><p>在Metropolis-Hastings算法中，通常需要对多元变量分布进行抽样，有时对多元变量分布的抽样是困难的。</p><p>可以对多元变量的每一变量的条件分布依次分别进行抽样，从而实现对整个多元变量的一次抽样，这就是单分量Metropolis-Hastings (single-component Metropolis-Hastings）算法。</p><p>假设马尔可夫链的状态由k维随机变量表示</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/45bbc9030f534f7db1b1468d1e52f764?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>马尔可夫链在时刻i的状态</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4ca2e44ae0fb4af8b0d06234cf5a8af6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>为了生成容量为的样本集合</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e280bac42a354dc899f3365ce85924f3?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>单分量Metropolis-Hastings 算法的基本做法：假设有一个k维的随机向量，现想要构造一条k维向量n个样本的马尔可夫链序列，那么随机初始化一个k维向量，然后固定这个向量其中的k-1个元素，抽取剩下的那个元素，生成转移状态的随机数，这样循环k次，就把整个向量更新了一遍，也就是生成了一个新的k维向量样本，把这个整体重复n次就得到了一条马尔科夫链。</p><p>单分量M-H算法由下面的k步迭代实现M-H算法的一次迭代</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ddf18150a2be43bab6386119f40e680f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>马尔可夫链的转移概率为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fe69689716324eababd4b28cbd6f484f?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如图5-7所示，单分量M-H算法的迭代过程。目标是对含有两个变量的随机变量x进行抽样</p><p>如果变量x1或x2更新，那么在水平或垂直方向产生一 个移动，连续水平和垂直移动产生</p><p>一个新的样本点。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/26c65abb11f94fccb1e702c5af5f6a22?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-7</p><p><strong>例子 逻辑回归应用</strong></p><p>考虑54位老年人的智力测试成绩(Wechesler Adult Intelligence Scale, WAIS, 0-20分)。 研究的兴趣在于发现老年痴呆症。</p><p>我们采用如下简单的逻辑回归模型:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/c1250d1ccee641d283661ca5a869e967?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则似然函数为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3a628f89202042ee966aabb04bbef65e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>考虑β0, β1的先验分布π为独立的正态分布:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4bb176c7aab942c9872821191ea3fb7a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>从而我们需要从此分布中产生随机数。</p><p>我们考虑如下三种抽样方法.</p><h1 id="M-H算法-独立抽样"><a href="#M-H算法-独立抽样" class="headerlink" title="M-H算法: 独立抽样"></a>M-H算法: 独立抽样</h1><p>提议分布取为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/ad417bc0b7d34f5092c363e3b0378a46?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>则算法如下：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/db03da79aa3c4c719e1ab6157cdf3ffe?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><h1 id="R语言演示代码"><a href="#R语言演示代码" class="headerlink" title="R语言演示代码"></a>R语言演示代码</h1><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">wais-<span class="keyword">read</span>.<span class="keyword">table</span> (.wais. txt. ,header=TRUE)</span><br><span class="line">y&lt;-wais[,2]; x&lt;-wais[,1]</span><br><span class="line"><span class="keyword">m</span> &lt;- 55000 #length of chain</span><br><span class="line">mu.beta&lt;-c(0,0); sigma.beta&lt;-c(100,100)</span><br><span class="line"><span class="keyword">prop</span>.s&lt;-c(0.1,0.1) #proposal distribution standard variance</span><br><span class="line">beta &lt;- <span class="built_in">matrix</span>(nrow=<span class="keyword">m</span>, ncol=2)</span><br><span class="line">acc.<span class="keyword">prob</span> &lt;- 0</span><br><span class="line">current.beta&lt;-c(0,0)</span><br><span class="line"><span class="keyword">for</span> (t <span class="keyword">in</span> 1:<span class="keyword">m</span>)&#123;</span><br><span class="line">    <span class="keyword">prop</span>.beta&lt;- rnorm(2, current.beta, <span class="keyword">prop</span>.s )</span><br><span class="line">    cur.eta&lt;-current.beta[1]+current.beta[2]*x</span><br><span class="line">    <span class="keyword">prop</span>.eta&lt;-<span class="keyword">prop</span>.beta[1]+<span class="keyword">prop</span>.beta[2]*x</span><br><span class="line">    loga &lt;-(<span class="built_in">sum</span>(y*<span class="keyword">prop</span>.eta-<span class="built_in">log</span>(1+<span class="built_in">exp</span>(<span class="keyword">prop</span>.eta)))</span><br><span class="line">    -<span class="built_in">sum</span>(y*cur.eta-<span class="built_in">log</span>(1+<span class="built_in">exp</span>(cur.eta)))</span><br><span class="line">    +<span class="built_in">sum</span>(dnorm(<span class="keyword">prop</span>.beta, mu.beta,s.beta,<span class="keyword">log</span>=TRUE))</span><br><span class="line">    -<span class="built_in">sum</span>(dnorm(current.beta,mu.beta,s.beta,<span class="keyword">log</span>=TRUE)))</span><br><span class="line">    <span class="keyword">u</span>&lt;-runif(1)</span><br><span class="line">    <span class="keyword">u</span>&lt;-<span class="built_in">log</span>(<span class="keyword">u</span>)</span><br><span class="line">    <span class="keyword">if</span>( <span class="keyword">u</span> &lt; loga) &#123;</span><br><span class="line">        current.beta&lt;-<span class="keyword">prop</span>.beta</span><br><span class="line">        acc.<span class="keyword">prob</span> &lt;- acc.<span class="keyword">prob</span>+1</span><br><span class="line">    &#125;</span><br><span class="line">    beta[t,]&lt;-current.beta</span><br><span class="line">&#125;</span><br><span class="line">acc.<span class="keyword">prob</span>&lt;-acc.<span class="keyword">prob</span>/<span class="keyword">m</span></span><br><span class="line">acc.<span class="keyword">prob</span></span><br><span class="line"># convergence diagnostics <span class="keyword">plot</span></span><br><span class="line">erg.<span class="keyword">mean</span>&lt;-function( x )&#123;     # compute ergodic <span class="keyword">mean</span></span><br><span class="line">    <span class="keyword">n</span>&lt;-<span class="built_in">length</span>(x)</span><br><span class="line">    result&lt;-cumsum(x)/cumsum(rep(1,<span class="keyword">n</span>))</span><br><span class="line">&#125;</span><br><span class="line">burnin&lt;-15000</span><br><span class="line">idx&lt;-seq(1,<span class="keyword">m</span>,50)</span><br><span class="line">idx2&lt;-seq(burnin+1,<span class="keyword">m</span>)</span><br><span class="line">par(mfrow=c(2,2))</span><br><span class="line"><span class="keyword">plot</span>(idx,beta[idx,1],<span class="keyword">type</span>=<span class="string">&quot;l&quot;</span>,xlab=<span class="string">&quot;Iterations&quot;</span>,ylab=<span class="string">&quot;Values of beta0&quot;</span>)</span><br><span class="line"><span class="keyword">plot</span>(idx,beta[idx,2],<span class="keyword">type</span>=<span class="string">&quot;l&quot;</span>,xlab=<span class="string">&quot;Iterations&quot;</span>,ylab=<span class="string">&quot;Values of beta1&quot;</span>)</span><br><span class="line">ergbeta0&lt;-erg.<span class="keyword">mean</span>(beta[,1])</span><br><span class="line">ergbeta02&lt;-erg.<span class="keyword">mean</span>(beta[idx2,1])</span><br><span class="line">ylims0&lt;-<span class="keyword">range</span>(c(ergbeta0,ergbeta02))</span><br><span class="line">ergbeta1&lt;-erg.<span class="keyword">mean</span>(beta[,2])</span><br><span class="line">ergbeta12&lt;-erg.<span class="keyword">mean</span>(beta[idx2,2])</span><br><span class="line">ylims1&lt;-<span class="keyword">range</span>(c(ergbeta1,ergbeta12))</span><br><span class="line"><span class="keyword">plot</span>(idx , ergbeta0[idx], <span class="keyword">type</span>=’<span class="keyword">l</span>’, ylab=’Values of beta0’, xlab=’Iterations’,</span><br><span class="line">main=’(c) Ergodic <span class="keyword">Mean</span> <span class="keyword">Plot</span> of beta0’, ylim=ylims0)</span><br><span class="line">lines(idx2, ergbeta02[idx2-burnin], col=2, lty=2)</span><br><span class="line"><span class="keyword">plot</span>(idx, ergbeta1[idx], <span class="keyword">type</span>=’<span class="keyword">l</span>’, ylab=’Values of beta1’, xlab=’Iterations’,</span><br><span class="line">main=’(<span class="keyword">d</span>) Ergodic <span class="keyword">Mean</span> <span class="keyword">Plot</span> of beta1’, ylim=ylims1)</span><br><span class="line">lines(idx2, ergbeta12[idx2-burnin], col=2, lty=2)</span><br><span class="line">apply(beta[(burnin+1):<span class="keyword">m</span>,],2,<span class="keyword">mean</span>)</span><br><span class="line">apply(beta[(burnin+1):<span class="keyword">m</span>,],2,sd)</span><br></pre></td></tr></table></figure><p>注意到在独立抽样方法产生的链中, β0和β1有很强的负相关性:</p><p>cor(beta[(burnin+1):m,1],beta[(burnin+1):m,2])&#x3D;-0.954.</p><p>这是链收敛很慢的原因. 在高度相关的空间上使用独立的提议分布导致链的混合效率低下。</p><h1 id="M-H算法-多元正态提议分布"><a href="#M-H算法-多元正态提议分布" class="headerlink" title="M-H算法: 多元正态提议分布"></a>M-H算法: 多元正态提议分布</h1><p>在独立抽样中, 我们使用的提议分布是相互独立的, 从而导致链的混合效率低下. 因此, 自然而然的我们 可以考虑非独立的提议分布. 提议分布相关阵应该和后验分布的相关阵类似. 为此, 可以考虑利用 Fisher信息阵H(β), 提议分布取为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/279fe28d5d704f35a60c81bcd9b4170c?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>由似然函数, 容易计算得到Fisher信息阵为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/e22c7ed716f449ab869d9efea897472c?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>算法如下：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a5ee1ff65b0a47fa88d5595a05b16b7c?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><h1 id="R语言演示代码-1"><a href="#R语言演示代码-1" class="headerlink" title="R语言演示代码"></a>R语言演示代码</h1><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">calculate.loglike&lt;-function(b,X=x,Y=y)&#123;</span><br><span class="line">    <span class="attribute">x&lt;-X; y&lt;-y</span></span><br><span class="line"><span class="attribute">    n&lt;-length(x); X&lt;-cbind( rep(1,n), x )</span></span><br><span class="line"><span class="attribute">    precision&lt;-700</span></span><br><span class="line"><span class="attribute">    eta&lt;-b[1]+b[2]*x</span></span><br><span class="line"><span class="attribute">    logq &lt;- log(1+exp(eta))</span></span><br><span class="line"><span class="attribute">    logq[eta&gt;precision]&lt;-eta[eta&gt;precision]</span></span><br><span class="line"><span class="attribute">    loglike&lt;- sum( y*eta - logq )</span></span><br><span class="line"><span class="attribute">    eta[eta&gt;precision]&lt;-precision</span></span><br><span class="line"><span class="attribute">    h &lt;- 1/((1+exp(-eta))*(1+exp(eta)))</span></span><br><span class="line"><span class="attribute">    H &lt;- t(X) %*% diag( h ) %*% X</span></span><br><span class="line"><span class="attribute">    return( list(loglike=loglike, H=H) )</span></span><br><span class="line"><span class="attribute">&#125;</span></span><br><span class="line"><span class="attribute">#---------------------------------------</span></span><br><span class="line"><span class="attribute">library(MASS)</span></span><br><span class="line"><span class="attribute">y=wais[,2]</span></span><br><span class="line"><span class="attribute">x=wais[,1]</span></span><br><span class="line"><span class="attribute">prop.sd=0.3</span></span><br><span class="line"><span class="attribute">m=2500</span></span><br><span class="line"><span class="attribute">beta0=c(0,0)</span></span><br><span class="line"><span class="attribute">n&lt;-length(y)</span></span><br><span class="line"><span class="attribute">X&lt;-cbind(rep(1,n), x )</span></span><br><span class="line"><span class="attribute">mu.beta&lt;-c(0,0)</span></span><br><span class="line"><span class="attribute">s.beta&lt;-c(100,100)</span></span><br><span class="line"><span class="attribute">c.beta&lt;- prop.sd</span></span><br><span class="line"><span class="attribute">beta &lt;- matrix(nrow=Iterations, ncol=2)</span></span><br><span class="line"><span class="attribute">acc.prob &lt;- 0</span></span><br><span class="line"><span class="attribute">current.beta&lt;-beta0</span></span><br><span class="line"><span class="attribute">for (t in 1</span>:m)&#123;</span><br><span class="line">    cur&lt;-calculate<span class="variable">.loglike</span>( current<span class="variable">.beta</span> )</span><br><span class="line">    cur<span class="variable">.T</span>&lt;-(1/c<span class="variable">.beta</span>^2)*(cur$H+diag(1/s<span class="variable">.beta</span>^2))</span><br><span class="line">    prop<span class="variable">.beta</span>&lt;- mvrnorm( 1, current<span class="variable">.beta</span>, solve(cur<span class="variable">.T</span>))</span><br><span class="line">    prop&lt;-calculate<span class="variable">.loglike</span>( prop<span class="variable">.beta</span> )</span><br><span class="line">    prop<span class="variable">.T</span> &lt;- (1/c<span class="variable">.beta</span>^2)* (prop$H+diag(1/s<span class="variable">.beta</span>^2))</span><br><span class="line">    loga &lt;-( prop$loglike-cur$loglike</span><br><span class="line">    +sum(dnorm(prop<span class="variable">.beta</span>,mu<span class="variable">.beta</span>,s<span class="variable">.beta</span>,log=TRUE))</span><br><span class="line">    -sum(dnorm(current<span class="variable">.beta</span>,mu<span class="variable">.beta</span>,s<span class="variable">.beta</span>,log=TRUE))</span><br><span class="line">    + as<span class="variable">.numeric</span>(0.5*log( det(prop<span class="variable">.T</span>) )</span><br><span class="line">    - 0.5 * t(current<span class="variable">.beta</span> - prop<span class="variable">.beta</span>) %*% prop<span class="variable">.T</span> %*% (current<span class="variable">.beta</span> - prop<span class="variable">.beta</span>))</span><br><span class="line">    - as<span class="variable">.numeric</span>(0.5*log( det(cur<span class="variable">.T</span> ) )</span><br><span class="line">    - 0.5 * t(prop<span class="variable">.beta</span> - current<span class="variable">.beta</span>) %*% cur<span class="variable">.T</span> %*% (prop<span class="variable">.beta-</span> current<span class="variable">.beta</span> )) )</span><br><span class="line">    u&lt;-runif(1)</span><br><span class="line">    u&lt;-log(u)</span><br><span class="line">    if( u &lt; loga ) &#123;</span><br><span class="line">        current<span class="variable">.beta</span>&lt;-prop<span class="variable">.beta</span></span><br><span class="line">        acc<span class="variable">.prob</span> &lt;- acc<span class="variable">.prob</span>+1</span><br><span class="line">    &#125;</span><br><span class="line">    beta[t,]&lt;-current<span class="variable">.beta</span></span><br><span class="line">&#125;</span><br><span class="line">print(acc<span class="variable">.prob</span>/m)</span><br></pre></td></tr></table></figure><p>在计算exp(η)时, 由于在R中, exp(710)&#x3D;Inf, 因此我们对η&gt;700, 取近似log(1 + exp(η)) ≈ η.</p><p>对链的收敛诊断可以看出, 链混合的效率很高。</p><h1 id="逐分量的M-H算法"><a href="#逐分量的M-H算法" class="headerlink" title="逐分量的M-H算法"></a>逐分量的M-H算法</h1><p>在M-H算法中, 按分量进行逐个更新, 其优势在于应用方便, 不需要考虑调节参数. 算法如下:</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/3769f43e49794f2b9e8759cca974b958?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><h1 id="R语言演示代码-2"><a href="#R语言演示代码-2" class="headerlink" title="R语言演示代码"></a>R语言演示代码</h1><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">y&lt;-wais<span class="selector-attr">[,2]</span></span><br><span class="line">x&lt;-wais<span class="selector-attr">[,1]</span></span><br><span class="line">m&lt;-<span class="number">10000</span></span><br><span class="line">beta0&lt;<span class="built_in">-c</span>(<span class="number">0</span>,<span class="number">0</span>) <span class="selector-id">#initial</span> value</span><br><span class="line">mu<span class="selector-class">.beta</span>&lt;<span class="built_in">-c</span>(<span class="number">0</span>,<span class="number">0</span>) # prior</span><br><span class="line">s<span class="selector-class">.beta</span>&lt;<span class="built_in">-c</span>(<span class="number">100</span>,<span class="number">100</span>) # prior</span><br><span class="line">prop<span class="selector-class">.s</span>&lt;<span class="built_in">-c</span>(<span class="number">1.75</span>,<span class="number">0.2</span>) # sd of proposal <span class="attribute">normal</span></span><br><span class="line">beta &lt;- <span class="built_in">matrix</span>(nrow=m, ncol=<span class="number">2</span>)</span><br><span class="line">acc<span class="selector-class">.prob</span> &lt;<span class="built_in">-c</span>(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">current<span class="selector-class">.beta</span>&lt;-beta0</span><br><span class="line">for (t in <span class="number">1</span>:m)&#123;</span><br><span class="line">    for (j in <span class="number">1</span>:<span class="number">2</span>)&#123;</span><br><span class="line">        prop<span class="selector-class">.beta</span>&lt;- current<span class="selector-class">.beta</span></span><br><span class="line">        prop<span class="selector-class">.beta</span><span class="selector-attr">[j]</span>&lt;- <span class="built_in">rnorm</span>( <span class="number">1</span>, current.beta[j], prop.s[j] )</span><br><span class="line">        cur<span class="selector-class">.eta</span> &lt;-current<span class="selector-class">.beta</span><span class="selector-attr">[1]</span>+current<span class="selector-class">.beta</span><span class="selector-attr">[2]</span>*x</span><br><span class="line">        prop<span class="selector-class">.eta</span>&lt;-prop<span class="selector-class">.beta</span><span class="selector-attr">[1]</span>+prop<span class="selector-class">.beta</span><span class="selector-attr">[2]</span>*x</span><br><span class="line">        <span class="built_in">if</span>(sum(prop.eta&gt;<span class="number">700</span>)&gt;<span class="number">0</span>) &#123;<span class="built_in">print</span>(t); stop;&#125;</span><br><span class="line">        <span class="built_in">if</span>(sum(cur.eta &gt;<span class="number">700</span>)&gt;<span class="number">0</span>) &#123;<span class="built_in">print</span>(t); stop;&#125;</span><br><span class="line">        loga &lt;<span class="built_in">-</span>(sum(y*prop.eta-log(<span class="number">1</span>+exp(prop.eta)))</span><br><span class="line">        <span class="built_in">-sum</span>(y*cur.eta-log(<span class="number">1</span>+exp(cur.eta)))</span><br><span class="line">        +<span class="built_in">sum</span>(dnorm(prop.beta,mu.beta,s.beta,log=TRUE))</span><br><span class="line">        <span class="built_in">-sum</span>(dnorm(current.beta,mu.beta,s.beta,log=TRUE)))</span><br><span class="line">        u&lt;<span class="built_in">-runif</span>(<span class="number">1</span>)</span><br><span class="line">        u&lt;<span class="built_in">-log</span>(u)</span><br><span class="line">        <span class="built_in">if</span>(u&lt; loga)&#123;</span><br><span class="line">            current<span class="selector-class">.beta</span>&lt;-prop<span class="selector-class">.beta</span></span><br><span class="line">        acc<span class="selector-class">.prob</span><span class="selector-attr">[j]</span> &lt;- acc<span class="selector-class">.prob</span><span class="selector-attr">[j]</span>+<span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    beta<span class="selector-attr">[t,]</span>&lt;-current<span class="selector-class">.beta</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">print</span>(acc.prob/m)</span><br></pre></td></tr></table></figure><p>链的收敛诊断显示了相比于独立的M-H算法, 混合的效率要高的多。</p><h1 id="III、-吉布斯采样-Gibbs-Sampling"><a href="#III、-吉布斯采样-Gibbs-Sampling" class="headerlink" title="III、 吉布斯采样(Gibbs Sampling)"></a>III、 吉布斯采样(Gibbs Sampling)</h1><p>吉布斯采样算法是Stuart Geman 和Donald Geman 两兄弟在1984年提出来的，以物理学家Josiah Willard Gibbs的名字命名。这个算法在现代贝叶斯分析中占据重要位置。</p><p>在大数据时代，数据特征非常的多，M-H采样由于接受概率α(i,j)的存在，在高维时需要的计算时间非常的可观，同时α(i,j)一般小于1，有时候辛苦计算出来却被拒绝了。能不能做到不拒绝转移呢？吉布斯采样算法就这样出场了。</p><p>吉布斯采样算法是单分量M-H算法的特殊情况，但是更容易实现，因而被广泛使用。</p><p>吉布斯采样特别适合于采样贝叶斯网络的后验分布，因为贝叶斯网络通常被指定为条件分布的集合。</p><h1 id="吉布斯采样算法是单分量M-H算法的特殊情况，主要有两点特殊："><a href="#吉布斯采样算法是单分量M-H算法的特殊情况，主要有两点特殊：" class="headerlink" title="吉布斯采样算法是单分量M-H算法的特殊情况，主要有两点特殊："></a>吉布斯采样算法是单分量M-H算法的特殊情况，主要有两点特殊：</h1><p>1&gt; 吉布斯采样算法的提议分布是当前变量xj，j &#x3D; 1,2,…，k的满条件概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a670efac45ae436d9b6a931fd5d7d748?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>2&gt; 吉布斯采样算法的接受概率α &#x3D; 1</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/6a3e834807fb4a35a540c4f705434807?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>一般的采样方法，生成一维随机变量并不困难，对于高维分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/c2a63dd01c744ebdb51c74a21384de8b?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>生成各分量非独立的随机向量就非常困难。吉布斯采样法很擅长解决这类问题，其广泛用于多元变量联合分布的抽样和估计。</p><p>吉布斯采样法把一次一个n维样本变为“n次一维”样本，但前提是我们事先知道完全条件概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/659fa0aba6ce4246bef98111551f948d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>数学上称为半共轭，为方便常简写为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/dfdc0313bee24b5fac78b931154ab66a?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>不同于M-H算法，该算法不引入新的提议分布q( . )，仅通过条件分布得到以给定分布p(x)为平稳分布的马氏链的转移概率。</p><p>把条件分布当作提议分布q( . )，则有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/271a6f3493404b62b40beca40344f9d2?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>接受概率</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/2b717effd6ae41a99a7094cfae1d25b6?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>以接受概率为1的方式实现链的满概率跳转。</p><h1 id="吉布斯采样构造的马氏链收敛到平稳分布的证明"><a href="#吉布斯采样构造的马氏链收敛到平稳分布的证明" class="headerlink" title="吉布斯采样构造的马氏链收敛到平稳分布的证明"></a>吉布斯采样构造的马氏链收敛到平稳分布的证明</h1><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/1a7b481f6a71413fbec09ce94a03d1d4?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-8</p><p>假设有一个二维概率分布p(x,y), 考察x坐标相同的两个点A(x1,y1), B(x1, y2)。如图5-8所示。</p><p>由条件概率公式推得</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/a90e80029ddf49ec848d0f9213b76e83?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>上面两式右边相等，所以</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/fee60942a28f4449beacdba4b251d396?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>故</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5ab8c591eb5f45ed8cd84d248f05a51d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>从上面等式，可以看出，在x&#x3D;x1这条平行于y轴的直线上，如果使用条件分布p(y|x1)作为任意两点之间的转移概率，那么任意两点之间的转移满足细致平稳条件。同样地，如果在y&#x3D;y1这条直线上任意取两个点A(x1,y1), C(x2, y1), 也有如下等式：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/46e2022f158041e78dc95747c00c2a4e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>于是，我们可以构造平面上任意两点之间的转移概率矩阵Q</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/caf20065cc4c46c39da6775c4982da82?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>根据上式，我们很容易验证对平面上任意两点X、Y (如图5-8的两点A、B), 满足细致平稳条件</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/118035fe4ae941d9a6d7010aa4afd194?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>因此，这个二维空间上的马氏链将收敛到平稳分布p(x,y)。</p><p>马氏链的状态转移只是轮换地沿着x轴和y轴做转移，于是得到样本(x0,y0), (x0,y1), (x1,y1), (x1, y2), (x2, y2), …马氏链收敛后，最终得到的样本就是p(x, y)的样本。坐标轴轮换采样是不强制要求的，也可以在x轴和y轴之间随机地选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。</p><p>我们很容易把二维推广到高维的情形，对于二维细致平稳条件</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/c59511cf1f4240829ad30c222cb6b178?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>如果x1变为多维X1，可以看出推导过程不变，所以多维细致平稳条件同样是成立的。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/be9682abfc2443158408c0f2cf32c423?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>此时转移矩阵Q由条件分布p(y|<strong>X1</strong>)定义。上式只是说明了一个坐标轴的情况，和二维情形类似，很容易验证对所有的坐标轴都有类似的结论。</p><p>所以，n维空间对于概率分布p(x1,x2,…,xn)可以定义如下转移矩阵</p><p>1&gt; 如果当前状态为(x1,x2,…,xn)，马氏链状态转移的过程中，只能沿着坐标轴做转移，转移概率定义为</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/62ea22f268b44287a7602d649369f2c0?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>2&gt; 其它无法沿着坐标轴进行的跳转，转移概率都设置为0。</p><p>于是，我们可以把Gibbs Sampling算法从二维的概率分布p(x,y)推广到n维的概率分布p(x1,x2,…,xn)。</p><h1 id="二维Gibbs采样算法步骤："><a href="#二维Gibbs采样算法步骤：" class="headerlink" title="二维Gibbs采样算法步骤："></a>二维Gibbs采样算法步骤：</h1><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/afe943545a554fbc9ed98bc9579ffbcf?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>例子 已知：贝叶斯后验分布有如下形式</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9b670eceb8d6462a8cf66d96677d9151?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>求：利用Gibbs采样法获取满足该后验分布的样本序列</p><p>解：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/751cfabf29e34d3a859018105a05b172?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/dc8ee3f47a74498698b1483ee454b7d9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>由上面两式，求得条件概率分布</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9d23c9f4bf7343b19eb80c159da08159?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>这里A(y), B(x)是关于y, x的函数，当指定y, x时，它们是常数。</p><p>求得的条件概率分布随机变量x, y都符合正态分布，即</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/8555e5d1506140368065599d9c0a3343?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>按已知正态分布f(x|y,data)和f(y|x,data)在两个坐标轴上不停轮换采样就可以得到后验分布的样本集。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/788207cb7a904be983f8349b3d511625?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>图5-9 Bibbs采样点轨迹图</p><p>从图5-9的实验结果可以看出采样点大都集中在高密区。</p><h1 id="小结：-5"><a href="#小结：-5" class="headerlink" title="小结："></a>小结：</h1><p>由于MCMC的马氏链的固有性质 - 采样点依赖前一个状态，这决定了相邻的采样点之间并不独立，存在自相关。自相关的存在将降低采样精度，为了减少样本的依赖性，我们在实验中可以采取如下措施：</p><ul><li>删减(Thinning)技术：对于生成的马氏链，每隔一段距离保留一个，其余舍弃；</li><li>块采样(Blocked Gibbs Sampling)技术；</li><li>折叠采样（Collapsed Gibbs Sampling)技术；</li><li>有序的过松弛(Ordered Overrelaxation)。</li></ul><h1 id="高维Gibbs采样算法步骤："><a href="#高维Gibbs采样算法步骤：" class="headerlink" title="高维Gibbs采样算法步骤："></a>高维Gibbs采样算法步骤：</h1><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/1cc9c4d872854441959fbbb04517b90d?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>当采样空间的维数比较高时，接受-拒绝采样和重要性采样都不适用，原因是当维数升高时很难找到适合的提议分布，采样效率比较差。没有比较就没有伤害，高维Gibbs采样算法的优势就体现出来了，其优点为：</p><ul><li>适用于广泛且困难的问题；</li><li>当联合分布不明确或很难直接采样，但是每个变量的条件分布是已知的并且很容易（或至少更容易）采样时，Gibbs采样更有效；</li><li>问题维数的增加通常不会降低其收敛速度或使其更复杂。</li></ul><h1 id="六、MCMC收敛诊断方法"><a href="#六、MCMC收敛诊断方法" class="headerlink" title="六、MCMC收敛诊断方法"></a>六、MCMC收敛诊断方法</h1><p>MCMC方法依赖于模拟的收敛性，下面介绍三种常用的收敛性诊断方法。</p><h1 id="1、同时产生多条马尔科夫链"><a href="#1、同时产生多条马尔科夫链" class="headerlink" title="1、同时产生多条马尔科夫链"></a>1、同时产生多条马尔科夫链</h1><p>这种方法的思路是选取多个不同的初值，同时产生多条马尔科夫链，经过一段时间后，若这几条链稳定下来，则说明算法收敛了。在实际操作中，可在同一个二维图中画出这些不同的马尔科夫链产生的后验样本值对迭代次数的散点图，如果经过若干次迭代后，这些散点图基本稳定，重合在一起，则可判断其算法收敛。</p><h1 id="2、比率诊断法"><a href="#2、比率诊断法" class="headerlink" title="2、比率诊断法"></a>2、比率诊断法</h1><p>这种方法的思路是选取多个不同的初值，同时产生T条马尔科夫链，记第j条链的标准差估计为Sj，链内方差的均值为W，链间方差为B，则：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/9ebb269a3d624b5b9627434f0bad5ba2?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>R的值趋近1，则表明MCMC模拟收敛，且比较稳定。</p><h1 id="3、GewekeTest法"><a href="#3、GewekeTest法" class="headerlink" title="3、GewekeTest法"></a>3、GewekeTest法</h1><p>GewekeTest由一系列的Z检验组成，其基本思路是：先对所有样本(假设有M个)做一次Z检验，然后去掉最前面的c个样本，用剩余的M-c个样本重复上述检验，再继续去掉最前面的c个样本，用剩余的M-2c个样本重复上述检验，依次类推，重复K次这样的检验，直到M-Kc。</p><h1 id="七、附录"><a href="#七、附录" class="headerlink" title="七、附录"></a>七、附录</h1><h1 id="附录1-证明n-步转移矩阵等于其之前所有转移矩阵的连续矩阵相乘"><a href="#附录1-证明n-步转移矩阵等于其之前所有转移矩阵的连续矩阵相乘" class="headerlink" title="附录1 证明n-步转移矩阵等于其之前所有转移矩阵的连续矩阵相乘"></a>附录1 证明n-步转移矩阵等于其之前所有转移矩阵的连续矩阵相乘</h1><p>该证明内容我们用定理来表述</p><p>定理： 对于∀n≥0，∀i，j∈S, (S是状态空间)，则有</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/019b2b1c5e764cf895f0462cd55f16a9?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>证明：</p><p>该定理可通过数学归纳法和Chapman-Kolmogorov等式来证明。</p><p>大家都知道，对于矩阵乘法，下面式子显然成立。</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/650dd2fbc7ee414a889e28d90db0f73e?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其中n≥0, m≥0.</p><p>Chapman-Kolmogorov等式</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/5ce6e40090914e5c892723b15054b017?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>其中n≥0, m≥0， i，j∈S，S是状态空间。</p><p>下面只需证明Chapman-Kolmogorov等式， 然后就可以用数学归纳法证明我们的定理了。</p><p>证明方法一：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/d7a0f4ee02304c229298d017c8902b39?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>证明方法二：</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/4eecb3be3b6748898c19ef005c68d75b?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p>根据该等式用数学归纳法证明定理略。</p><h1 id="附录2-WinBUGS-用MCMC进行贝叶斯推断的专用软件包"><a href="#附录2-WinBUGS-用MCMC进行贝叶斯推断的专用软件包" class="headerlink" title="附录2 WinBUGS - 用MCMC进行贝叶斯推断的专用软件包"></a>附录2 WinBUGS - 用MCMC进行贝叶斯推断的专用软件包</h1><p>WinBUGS(Bayesian inference Using Gibbs Sampling)是由英国的Imperial College和MRC联合开发的MCMC进行贝叶斯推断的专用软件包。在网站<br><a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/%E5%8F%AF%E4%BB%A5%E5%85%8D%E8%B4%B9%E4%B8%8B%E8%BD%BD%E3%80%82%E4%BD%BF%E7%94%A8WinBUGS%E5%8F%AF%E4%BB%A5%E5%BE%88%E6%96%B9%E4%BE%BF%E5%9C%B0%E5%AF%B9%E8%AE%B8%E5%A4%9A%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%86%E5%B8%83%E8%BF%9B%E8%A1%8CGibbs%E6%8A%BD%E6%A0%B7%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%80%85%E4%B8%8D%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E5%8F%82%E6%95%B0%E7%9A%84%E5%85%88%E9%AA%8C%E5%AF%86%E5%BA%A6%E6%88%96%E4%BC%BC%E7%84%B6%E7%9A%84%E7%B2%BE%E7%A1%AE%E8%A1%A8%E8%BE%BE%E5%BC%8F%EF%BC%8C%E5%8F%AA%E8%A6%81%E8%AE%BE%E7%BD%AE%E5%A5%BD%E5%8F%98%E9%87%8F%E7%9A%84%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83%E5%B9%B6%E5%AF%B9%E6%89%80%E7%A0%94%E7%A9%B6%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E4%B8%80%E8%88%AC%E6%80%A7%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E5%B0%B1%E8%83%BD%E5%BE%88%E5%AE%B9%E6%98%93%E5%AE%9E%E7%8E%B0%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E6%9E%90%EF%BC%8C%E8%80%8C%E4%B8%8D%E9%9C%80%E8%A6%81%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BC%96%E7%A8%8B%E3%80%82%E5%9C%A8WinBUGS%E4%B8%AD%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E6%9C%89%E5%90%91%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E7%9B%B4%E8%A7%82%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E7%BC%96%E5%86%99%E6%A8%A1%E5%9E%8B%E7%A8%8B%E5%BA%8F%EF%BC%8C%E5%B9%B6%E7%BB%99%E5%87%BA%E5%8F%82%E6%95%B0%E7%9A%84">www.mrc-bsu.cam.ac.uk/software/bugs/可以免费下载。使用WinBUGS可以很方便地对许多常用的模型和分布进行Gibbs抽样，使用者不需要知道参数的先验密度或似然的精确表达式，只要设置好变量的先验分布并对所研究的模型进行一般性的描述，就能很容易实现对模型的贝叶斯分析，而不需要复杂的编程。在WinBUGS中可以使用有向图模型方式对模型进行直观的描述，也可以直接编写模型程序，并给出参数的</a> Gibbs抽样动态图、用smoothing方法得到的后验分布的核密度估计图、抽样值的自相关图及均数和置信区间的变换图等，使抽样结果更直观、可靠。Gibbs抽样收敛后，可以得到参数的后验分布的平均值、标准差、95%置信区间和中位数等信息。</p><h1 id="附录3-MCNP-基于蒙特卡罗方法的核物理方面的通用软件包"><a href="#附录3-MCNP-基于蒙特卡罗方法的核物理方面的通用软件包" class="headerlink" title="附录3 MCNP - 基于蒙特卡罗方法的核物理方面的通用软件包"></a>附录3 MCNP - 基于蒙特卡罗方法的核物理方面的通用软件包</h1><p>MCNP(Monte Carlo N Particle Transport Code)是由美国洛斯阿拉莫斯国家实验室(LosAlamos National Laboratory)开发的基于蒙特卡罗方法的用于计算三维复杂几何结构中的中子、光子、电子或者耦合中子&#x2F;光子&#x2F;电子输运问题的通用软件包，也具有计算核临界系统（包括次临界和超临界系统）本征值问题的能力。</p><p>目前，MCNP以其灵活、通用的特点以及强大的功能被广泛应用于辐射防护与射线测定、辐射屏蔽设计优化、反应堆设计、（次）临界装置实验、医学以及检测器设计与分析等学科领域，并得到一致认可。</p><h1 id="附录4-R语言，S-PLUS，MATLBA-模拟MCMC的软件工具"><a href="#附录4-R语言，S-PLUS，MATLBA-模拟MCMC的软件工具" class="headerlink" title="附录4 R语言，S-PLUS，MATLBA - 模拟MCMC的软件工具"></a>附录4 R语言，S-PLUS，MATLBA - 模拟MCMC的软件工具</h1><h1 id="附录5-马尔可夫发展历史"><a href="#附录5-马尔可夫发展历史" class="headerlink" title="附录5 马尔可夫发展历史"></a>附录5 马尔可夫发展历史</h1><p><strong>1906年</strong> Andrey Andreyevich Markov 引入了马尔可夫链的概念</p><p>Markov, A. A. (1906). Rasprostranenie zakona bol’shih chisel na velichiny, zavisyaschie drug ot druga. Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete, 15(135-156), 18</p><p><strong>1953年</strong> 梅特罗波利斯将蒙特卡洛方法引入马尔可夫链</p><p>Metropolis, Nicholas; Rosenbluth, Arianna W.; Rosenbluth, Marshall N.; Teller, Augusta H.; Teller, Edward (1953). “Equation of State Calculations by Fast Computing Machines”.</p><p><strong>1957年</strong> 马尔可夫决策过程被提出</p><p>Bellman, R. (1957). A Markovian decision process. Journal of Mathematics and Mechanics, 679-684.</p><p><strong>1966年</strong> Leonard E. Baum等学者提出了隐马尔可夫模型（Hidden Markov Model，HMM）</p><p>Baum, L. E.; Petrie, T. (1966).Statistical Inference for Probabilistic Functions of Finite State Markov Chains. The Annals of Mathematical Statistics. 37 (6): 1554–1563.</p><p><strong>1975年</strong> Baker将HMM用于语音识别</p><p>Baker, J.(1975). The DRAGON system—An overview.IEEE Transactions on Acoustics, Speech, and Signal Processing. 23: 24–29.</p><p><strong>1980年</strong> 《Markov random fields and their applications》出版，详细描述了马尔可夫随机场的理论和应用</p><p>Kindermann, R., &amp; Snell, J. L. (1980). Markov random fields and their applications (Vol. 1). American Mathematical Society.</p><p><strong>1984年</strong> Stuart和Donald Geman兄弟描述了Gibbs抽样算法</p><p>Geman, S.; Geman, D.(1984). Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.IEEE Transactions on Pattern Analysis and Machine Intelligence. 6 (6): 721–741.</p><p><strong>1988年</strong> Judea Pearl提出马尔可夫毯的概念</p><p>Pearl, J. (2014). <em>Probabilistic reasoning in intelligent systems: networks of plausible inference</em>. Morgan Kaufmann.</p><p><strong>1989年</strong> James D. Hamilton 1989年机应用了制转换模型</p><p>Hamilton, J. (1989). A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica. 57 (2): 357–84.</p><p><strong>20世纪80年代</strong> HMM 开始用于分析生物序列（DNA）</p><p>Bishop, M. and Thompson, E. (1986). Maximum Likelihood Alignment of DNA Sequences.Journal of Molecular Biology. 190 (2): 159–165.</p><p><strong>1991年</strong> Lovejoy 研究了部分可观测马尔可夫决策过程（POMDP）</p><p>Lovejoy, W. S. (1991).A survey of algorithmic methods for partially observed Markov decision processes.Annals of Operations Research. 28(1):47–65.</p><p><strong>1995年</strong> D. P. Bertsekas 和 J. N. Tsitsiklis讨论了一类用于不确定条件下的控制和顺序决策的动态规划方法</p><p>Bertsekas D. P. and Tsitsiklis, J. N. (1995). Neuro-dynamic programming: an overview.Proceedings of 1995 34th IEEE Conference on Decision and Control. 1: 560-564.</p><p><strong>1998年</strong> S.Brin和L.Page提出PageRank算法</p><p>Page, L. (1998). The pagerank citation ranking : bringing order to the web. Stanford Digital Libraries Working Paper, 9(1), 1-14.</p><p><strong>2017年</strong> Yoshua Bengio 等研究者提出了 GibbsNet</p><p>Lamb, A. et al. (2017).GibbsNet: Iterative Adversarial Inference for Deep Graphical Models.arXiv:1712.04120.</p><p><strong>2017年</strong> Jiaming Song, Shengjia Zhao和Stefano Ermon研究了生成对抗的训练方法来对马尔可夫链（Markov chain）的转移算子（transition operator）进行学习</p><p>Song, J.; Zhao, S.; Ermon, S. (2017).GENERATIVE ADVERSARIAL LEARNING OF MARKOV CHAINS. ICLR.</p><h1 id="附录6-MCMC发展历史"><a href="#附录6-MCMC发展历史" class="headerlink" title="附录6 MCMC发展历史"></a>附录6 MCMC发展历史</h1><p><strong>1953年6月</strong> Metropolis等人发表了MCMC方法的开篇之作&lt;&lt;通过快速计算器计算状态方程&gt;&gt;(Equations of state calculations by fast computing machines), 论文的主要关注点是计算积分公式(类似于贝叶斯的后验分布)</p><p><img src="https://p6.toutiaoimg.com/origin/pgc-image/6d899e68e81140848103c2860e492949?from=pc" alt="流行算法：马尔可夫链蒙特卡洛法(MCMC)"></p><p><strong>1970年</strong> Hastings完成了MCMC方法史上里程碑的论文-Monte Carlo Sampling Methods Using Markov Chains and their Applications &lt;&lt;马尔可夫链蒙特卡洛抽样方法及其应用&gt;&gt;.解决了核物理研究中碰到的粒子分布高纬计算问题，攻克了常规蒙特卡洛方法遇到的维度瓶颈问题，将Metropolis算法一般化，并推广为一种统计模拟工具，形成了Metropolis-Hastings算法。</p><p>M-H算法相对于Metropolis方法而言，看起来更像专业的统计模拟工具。最重要的是，M-H算法不要求“提议分布函数”必须是对称的，从而应用起来更加灵活方便。Hastings在文章中列举了三个例子：第一个目标分布是泊松分布，采用的提议分布是随机游走；第二个目标分布是高斯分布，提议分布是均匀随机游走，但此均匀分布的中心不是马氏链的当前值θ， 而是-θ;第三个目标分布是多元分布，Hastings引进了Gibbs抽样的策略，每次只更新其中一个变量，这样的转移矩阵同样满足平稳分布。</p><p><strong>1973年</strong> Hastings的唯一的博士生Peskun发表了题为Optimum Monte-Carlo Sampling Using Markov Chains &lt;&lt;最优马尔可夫链蒙特卡洛抽样方法&gt;&gt;的文章，比较了Metropolis和Barker的接受概率的形式，也证明了在离散情形下Metropolis是最优的选择，这里的最优性可以通过经验平均值的渐进方差来表示。</p><p><strong>1984年</strong> Geman S和Geman D发表了在MCMC方法史上具有重要突破性的文章-Stochastic Relaxation,Gibbs Distributions and the Bayesian Restoration of Images. 该文基于随机松弛(Stochastic relaxation)算法，采用Gibbs分布对图像的贝叶斯恢复进行了研究，提出了Gibbs采样的概念并将其引入到统计应用领域，Robert和Casella将此文称为“革命的种子”，吹响了MCMC方法革命的号角。</p><p><strong>1986年</strong> Adrian Smith 做了关于分层模型的系列学术演讲。</p><p><strong>1987年</strong> Tanner和Wong在论文中采用基于多个条件分布进行模拟的方法，这种思路等价于从联合分布进行模拟，基本具备了Gibbs采样的雏形。</p><p><strong>1989年</strong> Adrian Smith第一次详细阐释了Gibbs抽样的本质。</p><p><strong>1990年</strong>Gelfand和Smith发表论文Sampling-based approaches to calculation marginal densities.&lt;&lt;基于抽样的边际分布计算方法&gt;&gt; ，将Gibbs抽样的本质阐述得更为深刻和完整，成为主流统计学界大规模使用MCMC方法的真正起点。</p><p><strong>1995年</strong> Green提出得逆跳的马尔可夫链蒙特卡洛模拟(Reversible Jump Markov Chain Monte Carlo, RJMCMC)，可被视为MCMC方法第二次革命的开端，所构建的马氏链不仅可以在一个模型的参数空间内进行转移，还可以在不同模型(维度可以不同)之间跳跃，从而为贝叶斯模型选择提供了强大工具。</p><p><strong>二十世纪九十年代及以后</strong> Richardson和Green将RJMCMC应用到混合阶估计中；Brooks，Giudici和Roberts提出了提高RJMCMC转移效率的方法；Marin和Robert将RJMCMC应用到变量选择中。由于MCMC方法得到的并非独立样本，具有自相关性，因此不能直接用于参数估计，尤其是不能直接用于直接估计参数的标准误差。为了克服自相关性，Hobert等人提出在原始马氏链转移链基础上间隔抽取构成新的样本序列，从而克服自相关性，可以得到参数估计以及估计误差。</p><h1 id="八、参考文献"><a href="#八、参考文献" class="headerlink" title="八、参考文献"></a>八、参考文献</h1><p>[1] N. Metropolis et al., “Equations of state calculations by fast computing machines”,《the Journal of Chemical Physics》,21,1087(1953).</p><p>[2] <a href="http://www.random.org/">www.random.org</a></p><p>[3] en.wikipedia.org&#x2F;wiki&#x2F;Gibbs_sampling</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>动态规划算法</title>
      <link href="/2022/03/04/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/"/>
      <url>/2022/03/04/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6938026841311592998/">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a><strong>一、定义</strong></h1><p>动态规划（英语：Dynamic Programming，简称DP）是运筹学的一个分支，是通过把原问题分解为相对简单的子问题的方式求解复杂问题的一种方法。20世纪50年代初，美国数学家贝尔曼（R.Bellman）等人在研究多阶段决策过程的优化问题时，提出了著名的最优化原理，从而创立了动态规划[1] 。</p><p>这里Programming不是编程的意思，而是决策。但这种决策不是一下就出来的，而是一步步多阶段(multistage)积累出来的。换句话说，我们需要一个决策，但这个决策太大了，我们做不了，所以需要把它递归到我们可以简单做出决策的状态，然后从这些状态开始，慢慢的“动态地”演进到最终的决策。</p><p>把一个问题看作是一个前后关联具有链状结构的多阶段过程就称为多阶段决策过程，这种问题称为多阶段决策问题。在多阶段决策问题中，各个阶段采取的决策，一般来说是与时间有关的，决策依赖于当前状态，又随即引起状态的转移，一个决策序列就是在变化的状态中产生出来的，故有“动态”的含义，称这种解决多阶段决策最优化的过程为动态规划方法。</p><p>动态规划没有统一的处理方法，必须根据问题的各种性质并结合一定的技巧来处理。动态规划问题一直是大厂面试时最频繁出现的算法题，主要原因在于此类问题灵活度高，思维难度大，没有很明显的套路做法。没有放之四海皆准的计算动态规划解决方案的公式。</p><p>动态规划算法是国际大学生程序设计竞赛（ACM）用得最多的算法之一，深入学习动态规划很重要。</p><h1 id="二、基本思想"><a href="#二、基本思想" class="headerlink" title="二、基本思想"></a><strong>二、基本思想</strong></h1><p>动态规划算法与分治法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。</p><p>动态规划算法的核心是记住已经求过的解，记住求解的方式有两种：</p><p>①通过具有记忆功能的迭代自顶而下求解;</p><p>②一般采用迭代法通过自底而上求解。</p><p>动态规划算法的关键在于解决冗余，这是动态规划算法的根本目的。动态规划实质上是一种以空间换时间的技术，它在实现的过程中，不得不存储产生过程中的各种状态，所以它的空间复杂度要大于其他的算法。选择动态规划算法是因为它可以用空间换取时间，特别是规模比较大时，大幅减少搜索与计算的时间。</p><h1 id="三、动态规划分类"><a href="#三、动态规划分类" class="headerlink" title="三、动态规划分类"></a><strong>三、动态规划分类</strong></h1><p>动态规划一般可分为四类：</p><ul><li>线性动态规划</li></ul><p>例如：拦截导弹,合唱队形,挖地雷,建学校,剑客决斗等；</p><ul><li>区域动态规划</li></ul><p>例如：石子合并, 加分二叉树,统计单词个数,炮兵布阵等；</p><ul><li>树形动态规划</li></ul><p>例如：皇宫守卫,贪吃的九头龙,二分查找树,聚会的欢乐,数字三角形等；</p><ul><li>背包动态规划</li></ul><p>例如：01背包问题,完全背包问题,分组背包问题,装箱问题,挤牛奶等。</p><h1 id="四、适用条件"><a href="#四、适用条件" class="headerlink" title="四、适用条件"></a>四、适用条件</h1><p>能采用动态规划求解的问题，通常要具备3个性质：</p><h1 id="1-问题中的状态必须满足最优化原理"><a href="#1-问题中的状态必须满足最优化原理" class="headerlink" title="(1) 问题中的状态必须满足最优化原理"></a><strong>(1) 问题中的状态必须满足最优化原理</strong></h1><p>最优化原理的定义：作为整个过程的最优策略具有这样的性质，无论过去的状态和决策如何，对先前决策所形成的状态而言，余下的诸决策必构成最优决策。简而言之，一个最优化策略的子策略总是最优的。</p><p>最优化原理用数学语言描述：假设为了解决某一优化问题，需要依次作出n个决策D1，D2，…，Dn，如若这个决策序列是最优的，对于任何一个整数k，1 &lt; k &lt; n，不论前面k个决策是怎样的，以后的最优决策只取决于由前面决策所确定的当前状态，即以后的决策Dk+1，Dk+2，…，Dn也是最优的。</p><p>最优化原理是动态规划的基础。任何一个问题，如果失去了这个最优化原理的支持，就不可能用动态规划方法计算。一个问题满足最优化原理又称其具有最优子结构性质。最优子结构是指原问题的最优解包含子问题的最优解。证明见附录1。</p><h1 id="2-问题中的状态必须满足无后效性"><a href="#2-问题中的状态必须满足无后效性" class="headerlink" title="(2) 问题中的状态必须满足无后效性"></a><strong>(2) 问题中的状态必须满足无后效性</strong></h1><p>所谓的无后效性是指，下一时刻的状态只与当前状态有关，而和当前状态之前的状态无关，当前的状态是对以往决策的总结。</p><h1 id="3-子问题重叠性"><a href="#3-子问题重叠性" class="headerlink" title="(3) 子问题重叠性"></a><strong>(3) 子问题重叠性</strong></h1><p>子问题重叠性即子问题之间是不独立的，一个子问题在下一阶段决策中可能被屡次使用到。</p><p>动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。</p><p>子问题重叠不是使用动态规划的必要条件，但是问题存在子问题重叠的特性更能够充分彰显动态规划的优势。若是没有这条性质，动态规划算法同其它算法相比就不具有优点。</p><h1 id="五、动态规划解题思路"><a href="#五、动态规划解题思路" class="headerlink" title="五、动态规划解题思路"></a>五、动态规划解题思路</h1><h1 id="1、解题步骤"><a href="#1、解题步骤" class="headerlink" title="1、解题步骤"></a>1、解题步骤</h1><h1 id="1-划分阶段"><a href="#1-划分阶段" class="headerlink" title="1) 划分阶段"></a><strong>1) 划分阶段</strong></h1><p>按照问题的时间或空间特征，把问题分为若干个阶段。在划分阶段时，注意划分后的阶段必定要是有序的或者是可排序的，不然问题就没法求解。</p><h1 id="2-确定状态和状态变量"><a href="#2-确定状态和状态变量" class="headerlink" title="2) 确定状态和状态变量"></a><strong>2) 确定状态和状态变量</strong></h1><p>将问题发展到各个阶段时所处于的各类客观状况用不一样的状态表示出来。</p><p>DP状态的确定主要有两大原则：</p><ul><li>最优子结构</li><li>无后效性</li></ul><p>例如：图5-1求得一条从A到G的最短路径？</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c149ad2c8d4b4c65a8f1c148590df34f?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图5-1</p><p>可以用Dijkstra算法求得， Dijkstra算法符合动态规划的这一特性：待求解的问题分解为若干个子问题，前一子问题的解，为后一子问题的求解提供了有用的信息。动态规划主要是解决多阶段决策问题，并且阶段间存在着相互影响。该例子有7个阶段，初始阶段状态为A, 第2阶段状态有两个{B1，B2}, 第三阶段状态有4个{C1,C2,C3,C4}，等等。一个阶段可以有多个状态。</p><h1 id="3-确定决策并写出状态转移方程"><a href="#3-确定决策并写出状态转移方程" class="headerlink" title="3) 确定决策并写出状态转移方程"></a><strong>3) 确定决策并写出状态转移方程</strong></h1><p>由于决策和状态转移有着自然的联系，状态转移就是根据上一阶段的状态和决策来导出本阶段的状态。因此若是确定了决策，状态转移方程也就可写出。</p><p>我们只需要用分类讨论的思想来枚举所有小状态向大状态转移的可能性即可推出DP转移方程。</p><h1 id="4-初始条件和边界状况"><a href="#4-初始条件和边界状况" class="headerlink" title="4) 初始条件和边界状况"></a><strong>4) 初始条件和边界状况</strong></h1><p>给出的状态转移方程是一个递推式，需要一个递推的终止条件或边界条件。即设置初始值，考虑边界状况。</p><h1 id="2、动态规划决策过程示意图"><a href="#2、动态规划决策过程示意图" class="headerlink" title="2、动态规划决策过程示意图"></a>2、动态规划决策过程示意图</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/18738fde5a324aaeb392cd0b52f64655?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图5-2</p><h1 id="六、应用"><a href="#六、应用" class="headerlink" title="六、应用"></a><strong>六、应用</strong></h1><p>动态规划的应用极其广泛，包括工程技术、经济、工业生产、军事以及自动化控制等领域，并在背包问题、生产经营问题、资金管理问题、资源分配问题、最短路径问题和复杂系统可靠性问题等中取得了显著的效果 。</p><h1 id="例子1-0-1背包问题"><a href="#例子1-0-1背包问题" class="headerlink" title="例子1 0-1背包问题"></a><strong>例子1 0-1背包问题</strong></h1><p>现有n件物品和一个容量为c的背包。第i件物品的重量是重量为w[i]，价值是v[i]。已知对于一件物品必须选择取（用1表示）或者不取（用0表示），且每件物品只能被取一次（这就是“0-1”的含义）。求放置哪些物品进背包，可使这些物品的重量总和不超过背包容量，且价值总和最大。</p><h1 id="1-gt-穷举法"><a href="#1-gt-穷举法" class="headerlink" title="1&gt; 穷举法"></a><strong>1&gt; 穷举法</strong></h1><h1 id="有两种方法："><a href="#有两种方法：" class="headerlink" title="有两种方法："></a><strong>有两种方法：</strong></h1><h1 id="第一种方法，计算从n件物品中，取任意数量的物品有多少种取法？"><a href="#第一种方法，计算从n件物品中，取任意数量的物品有多少种取法？" class="headerlink" title="第一种方法，计算从n件物品中，取任意数量的物品有多少种取法？"></a><strong>第一种方法，计算从n件物品中，取任意数量的物品有多少种取法？</strong></h1><p>利用高中的排列组合知识，分类取：</p><p>取0个物品，有1种取法；</p><p>取1个物品，有n种取法；</p><p>取2个物品，有n*(n-1)&#x2F;2种取法；</p><p>…</p><p>取n个物品，有1种取法；</p><p>总共的取法为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/64751d9595de48959134e4d500ea25cb?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>共有<strong>2ⁿ</strong>种取法，然后分别计算<strong>2ⁿ</strong>种取法的物品的总重量和总价值，选出其中满足条件（物品的重量总和不超过背包容量，且价值总和最大）的一种取法。</p><h1 id="第二种方法，将n个物品排成一行，对于一件物品必须选择取（用1表示）或者不取（用0表示），想象成n个0、1组成的序列，如"><a href="#第二种方法，将n个物品排成一行，对于一件物品必须选择取（用1表示）或者不取（用0表示），想象成n个0、1组成的序列，如" class="headerlink" title="第二种方法，将n个物品排成一行，对于一件物品必须选择取（用1表示）或者不取（用0表示），想象成n个0、1组成的序列，如"></a><strong>第二种方法，将n个物品排成一行，对于一件物品必须选择取（用1表示）或者不取（用0表示），想象成n个0、1组成的序列，如</strong></h1><p>00010…010；</p><p>01100…100;</p><p>10001…110;</p><p>…</p><p>利用高中的排列组合知识, 这样的序列有2<em>2</em>2*2…*2 &#x3D; 2^n种。</p><p>0-1背包问题中的“0-1”就是这个意思。</p><p>每一个序列代表一种取法，选出其中满足条件（物品的重量总和不超过背包容量，且价值总和最大）的一种取法。</p><p><strong>显然，穷举法的运行时间是O(2ⁿ)，当n很大时真的是慢如蜗牛。</strong></p><h1 id="2、回溯法"><a href="#2、回溯法" class="headerlink" title="2、回溯法"></a><strong>2、回溯法</strong></h1><p>用回溯法实现就是要在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根结点出发深度探索解空间树。具体方法如下： </p><p>①对每一个物品i，对该物品只有选与不选两种决策，有n个物品，可以形成一棵 深度为n的决策树； </p><p>②遍历这棵树，以枚举所有情况，最后进行判断，若重量不超过背包容量，且总价值最大，该方案就是最优的。</p><p>假设你是一个小偷，背着一个可装下6磅东西的背包，你可以偷窃的商品如下：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a2d810a1d13f466bae7d37963158e9eb?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-1</p><p>为了让偷窃的商品价值最高，你该选择哪些商品？</p><p>初始的状态是（6，0），表示背包容量为6磅，背包内物品总价值为0美元。接下来，我们要开始做选择了。每加一件物品，有两种选择，选或者不选。选择一个物品时，背包的容量会减少，里面的物品总价值会增加。背包问题变成了决策树，如图6-1所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/792902c241d64cbaba805c36e30e907e?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-2</p><p>子树叶蓝色标记是按照回溯法枚举的选取物品的剩余容量和总价值，红色标记是能偷窃的商品的最高价值4200美元。</p><h1 id="小结："><a href="#小结：" class="headerlink" title="小结："></a><strong>小结：</strong></h1><p>优化方法： </p><p>剪枝一：当重量大于背包的容量时，没有必要对剩下的物品进行决策； </p><p>剪枝二：将剩下的所有物品都选取，其总价值也没有目前所求得的总价值还大的话，就可以返回。</p><p>回溯法枚举所有的解空间，时间复杂度是O(2ⁿ)。</p><p><strong>3、动态规划法</strong></p><p>与回溯法同样的例子，假设你是一个小偷，背着一个可装下6磅东西的背包，你可以偷窃的商品如下：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/595da1e51b3f490dbb17f0050504f67e?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-3</p><p>为了让偷窃的商品价值最高，你该选择哪些商品？</p><p>第1步，我们从一个网格开始，将背包问题网格化。网格的行对应的是商品，列对应的是不同容量(1~6磅)的背包。如图6-4所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/923d96b22e69425796cfc8bd9adef71b?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-4</p><p>第2步，我们先来一步一步做。首先来看第1行，加入2磅的吉他。第1个单元格表示背包的的容量为1磅。吉他的重量是2磅，这意味着它不能装入背包！第1行的其它单元格的背包的容量≥2磅，可以装入吉他，总价值是1500美元。如图6-5所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4616b165d37942d2b301d5ede692e24a?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-5</p><p>第3步，再加入4磅的电脑。在第2行中的各单元格的总价值的计算公式如下：</p><p>S(2,j)&#x3D;max {v[2]+S(1,c[j]-w[2]),S(1,j)}</p><p>S(2, j)—表示第2行，第j列的单元格的总价值；</p><p>v[2] — 表示第2行的商品(即电脑)的价值；</p><p>w[2] — 表示第2行的商品(即电脑)的重量；</p><p>c[j] — 表示第j列的小背包的容量，一般c[j]&#x3D;j；(例如，第1列背包的容量是1磅， 第2列的背包的容量是2磅， 第3列的背包的容量是3磅，…， 第6列的背包的容量是6磅)</p><p>c[j]-w[2] — 表示加入电脑后，第j列的背包的容量减去电脑重量后的剩余容量，刚好对应着以前的列号。</p><p>S(1, j) — 表示第1行第j列的单元格的总价值。</p><p><strong>对计算公式第一种理解方法：</strong></p><p>加入当前商品（电脑）后，当前行中的各单元格的总价值等于①与②的最大值。</p><p>①　当前加入的商品（电脑）的价值 + 上一行中剩余背包容量（当前行的各单元格对应的背包容量减去当前加入的商品的重量）所在的列对应的单元格的总价值；</p><p>②　同一列(即同一背包的容量)的上一行单元格(即上一次计算)的已经加入的商品的总价值。</p><p><strong>对计算公式第二种理解方法：</strong></p><p>如图中红色标记部分， 第2行第6列的计算总价值 &#x3D; max { 第1行第2列的单元格的总价值$1500 + 当前电脑的价值$2000, 第1行第6列的单元格的总价值$1500 }， 即两者取最大值。 为什么是第1行第2列呢？因为第6列的容量是6磅，减去加入的电脑的重量4磅，剩余的容量为2磅，对应的就是第2列。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/94070f7e6dd642a2a9516d37c8a53809?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-6</p><p>第4步，再加入5磅的音响。在第3行中的各单元格的总价值的计算公式如下：</p><p>S(3, j) &#x3D; max {v[3]+S(2,c[j]-w[3]),S(2, j)}</p><p>S(3, j)—表示第3行，第j列的单元格的总价值；</p><p>v[3] — 表示第3行的商品(即音响)的价值；</p><p>w[3] — 表示第3行的商品(即音响)的重量；</p><p>c[j] — 表示第j列的小背包的容量，一般c[j]&#x3D;j；(例如，第1列背包的容量是1磅，第2列的背包的容量是2磅， 第3列的背包的容量是3磅，…，第6列的背包的容量是6磅）</p><p>c[j]-w[3] — 表示加入音响后，第j列的背包的容量减去音响重量后的剩余容量，刚好对应着以前的列号。</p><p>S(2, j) — 表示第2行第j列的单元格的总价值。</p><p><strong>对计算公式第一种理解方法：</strong></p><p>加入当前商品（音响）后，当前行中的各单元格的总价值等于①与②的最大值。</p><p>③　当前加入的商品（音响）的价值 + 上一行中剩余背包容量（当前行的各单元格对应的背包容量减去当前加入的商品的重量）所在的列对应的单元格的总价值；</p><p>④　同一列(即同一背包的容量)的上一行单元格(即上一次计算)的已经加入的商品的总价值。</p><p><strong>对计算公式第二种理解方法：</strong></p><p>如图中红色标记部分， 第3行第6列的计算总价值 &#x3D; max { 第2行第1列的单元格的总价值0 + 当前音响的价值 $3000, 第2行第6列的单元格的总价值$3500 }， 即两者取最大值。 为什么是第2行第1列呢？因为第6列的容量是6磅，减去加入的音响的重量5磅，剩余的容量为1磅，对应的就是第1列。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/402d069fd26844619f043b9a89a095b7?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-7</p><p>经过第4步加入手机、第5步加入MP3后，计算得到满的网格数据，红色标记的最后一行就是对应的各容量能够获取的商品的最大总价值。例如：1磅的容量能获取$1200, 2磅的容量能获取$1700, 3磅的容量能获取$2700, 4磅的容量能获取$3200, 5磅的容量能获取$3200, 6磅的容量能获取$4200。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0e24d758f50140e4be2ae101c76b496d?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-8</p><p><strong>小结：</strong></p><p>问题1：再新加一件商品，沿着往下走时，最大价值有可能降低吗？</p><p>答案：不可能。每次迭代时，你都存储当前的最大价值。最大价值不可能比以前低。</p><p>问题2：行的排列顺序发生变化时结果将如何？答案会随之变化吗？</p><p>假设你按如下顺序填充各行：手机、音响、电脑、MP3、吉他。网格将会是什么样的？</p><p>答案：没有变化。也就是说，各行的排列顺序无关紧要。</p><p>问题3：可以逐列而不是逐行填充网格吗？</p><p>答案：就这个背包问题而言，这没有任何影响，但对于其他问题，可能有影响。</p><p>动态规划先解决子问题，再逐步解决大问题。 对于背包问题，你先解决小背包（子背包）问题，再逐步解决原来的问题。</p><p>0-1背包问题用二维矩阵的网格解决，矩阵的行表示n个物品，列表示背包逐步增长的容量, 是从1增长到c，也就是从小背包容量增长到待求背包容量。</p><p><strong>其状态转移函数：S(i, j) &#x3D; max { v[i] + S(i-1, j- w[i]), S(i-1, j)} 1≤i≤n, 1≤j≤c</strong></p><p>S(i, j)—表示第i行，第j列的单元格的总价值；</p><p>v[i] — 表示第i行的物品的价值；</p><p>w[i] — 表示第i行的物品的重量；</p><p>j — 既表示列号，又表示第j列的小背包的容量;</p><p>j-w[i] — 表示加入物品后，第j列的小背包的容量减去该物品重量后的剩余容量，刚好对应着以前的列号。</p><p>S(i-1, j) — 表示第i-1行第j列的单元格的总价值。</p><p>0-1背包问题的空间复杂度与时间复杂度均为O(n*c)，其中n为物品数，c为背包容量。</p><h1 id="例子2-查字典-最长公共子串与最长公共子序列-2"><a href="#例子2-查字典-最长公共子串与最长公共子序列-2" class="headerlink" title="例子2 查字典 - 最长公共子串与最长公共子序列[2]"></a><strong>例子2 查字典 - 最长公共子串与最长公共子序列[2]</strong></h1><p>已知：假设你管理着字典网站。用户在该网站输入单词时，你需要给出其定义。 但如果用户拼错了，你必须猜测他原本要输入的是什么单词。例如，Alex想查单词fish，但不小心输入了hish。</p><p>求：Alex输入了hish，那他原本要输入的是fish还是vista呢？</p><p><strong>解答：</strong></p><h1 id="1-gt-最长公共子串"><a href="#1-gt-最长公共子串" class="headerlink" title="1&gt; 最长公共子串"></a><strong>1&gt; 最长公共子串</strong></h1><p>如何将这个问题划分为子问题呢？你可能需要比较子串：不是比较hish和fish，而是先比较his和fis。每个单元格都将包含这两个子串的最长公共子串的长度。这也给你提供了线索，让坐标轴很可能是这两个单词。</p><p>第1步 建立网格，单元格全部初始化为0。如图6-9所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cdb6e5991f654823bf13ff2e699bff8d?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-9</p><p>第2步，计算单元格的值，如果两个字母不相同，值为0；如果两个字母相同，值为左上角邻居的值加1。如图6-10所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/98348b4388e843df81c5f195d0924b49?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-10</p><p>第3步，查找单词Fish和Hish的最长公共子串时，网格如图，最长公共子串为ish。如图6-11所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4aec45d6bd814f6b94cb9bf944c0a385?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-11</p><p>查找单词hish和vista的最长公共子串时，网格如图，最长公共子串为is。如图6-12所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/be225340ce49431493813199b6da80ef?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-12</p><p>所以，Alex输入了hish，那他原本要输入的是fish而不是vista。</p><p><strong>伪代码实现如下：</strong></p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (word_a<span class="comment">[i]</span> == word_b<span class="comment">[j]</span>)     //两个字母相同</span><br><span class="line">    cell<span class="comment">[i]</span><span class="comment">[j]</span> = cell<span class="comment">[i-1]</span><span class="comment">[j-1]</span> + 1;</span><br><span class="line">else                          //两个字母不相同</span><br><span class="line">    cell<span class="comment">[i]</span><span class="comment">[j]</span> = 0;</span><br></pre></td></tr></table></figure><h1 id="2-gt-最长公共子序列"><a href="#2-gt-最长公共子序列" class="headerlink" title="2&gt; 最长公共子序列"></a><strong>2&gt; 最长公共子序列</strong></h1><p>假设Alex不小心输入了fosh，他原本想输入的是fish还是fort呢？</p><p>我们使用最长公共子串公式来比较它们。如图6-13所示。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/713e74e4948243f1a41f46fe0a28b3f3?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-13</p><p>最长公共子串的长度相同，都包含两个字母！但fosh与fish更像。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5d919b989a4241aba6ed5340f38e4ca1?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-14</p><p>这里比较的是最长公共子串，但其实应比较最长公共子序列：两个单词中都有的序列包含的</p><p>字母数。如何计算最长公共子序列呢？</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/76283630314040d4aa2fb388a7e1bf32?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-15</p><p>最终的网格如下。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/efcf9dd76b964957ba69398aea066257?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图6-16</p><p>伪代码实现如下。</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (word_a<span class="comment">[i]</span> == word_b<span class="comment">[j]</span>)</span><br><span class="line">    cell<span class="comment">[i]</span><span class="comment">[j]</span> = cell<span class="comment">[i-1]</span><span class="comment">[j-1]</span> + 1;</span><br><span class="line">else</span><br><span class="line">    cell<span class="comment">[i]</span><span class="comment">[j]</span> = max(cell<span class="comment">[i-1]</span><span class="comment">[j]</span>, cell<span class="comment">[i]</span><span class="comment">[j-1]</span>);</span><br></pre></td></tr></table></figure><h1 id="小结：-1"><a href="#小结：-1" class="headerlink" title="小结："></a><strong>小结：</strong></h1><ul><li>生物学家根据最长公共序列来确定DNA链的相似性，进而判断度两种动物或疾病有多相似。最长公共序列还被用来寻找多发性硬化症治疗方案。</li><li>你使用过诸如git diff等命令吗？它们指出两个文件的差异，也是使用动态规划实现的。</li><li>比较字符串的相似程度。编辑距离（levenshtein distance）指出了两个字符串的相似程度，也是使用动态规划计算得到的。编辑距离算法的用途很多，从拼写检查到判断用户上传的资料是否是盗版，都在其中。</li><li>你使用过诸如Microsoft Word等具有断字功能的应用程序吗？它们如何确定在什么地方断字以确保行长一致呢？使用动态规划！</li></ul><h1 id="七、动态规划算法的局限性"><a href="#七、动态规划算法的局限性" class="headerlink" title="七、动态规划算法的局限性"></a><strong>七、动态规划算法的局限性</strong></h1><p>动态规划对于解决多阶段决策问题的效果是明显的，但是动态规划也有一定的局限性。首先，它没有统一的处理方法，必须根据问题的各种性质并结合一定的技巧来处理；另外当变量的维数增大时，总的计算量及存贮量急剧增大。</p><p>动态规划的维数就是指状态变量的维数，而不是指决策变量或策略的维数。状态变量的维数过大，在应用动态规划的方法求解时将大大增加电子计算机的内存量。解题所需要的内存量一般按指数速度增加的。</p><p>因而，受计算机的存贮量及计算速度的限制，当今的计算机仍不能用动态规划方法来解决较大规模的问题，这就是“维数障碍”</p><h1 id="八、附录"><a href="#八、附录" class="headerlink" title="八、附录"></a><strong>八、附录</strong></h1><h1 id="附录1-证明最优子结构中原问题的最优解包含子问题的最优解"><a href="#附录1-证明最优子结构中原问题的最优解包含子问题的最优解" class="headerlink" title="附录1 证明最优子结构中原问题的最优解包含子问题的最优解"></a><strong>附录1 证明最优子结构中原问题的最优解包含子问题的最优解</strong></h1><p><strong>以0-1背包问题为例证明。</strong></p><p>0-1背包问题： 给定n中物品和一个背包，物品i的重量是wi,其价值为vi,背包的容量为C。如何选择装入背包的物品，使得装入背包中物品的总价值最大？</p><p>0-1背包问题等价于一个整数规划问题：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/6a032515d85348d48a11da6bb134d0a2?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图8-1</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2f8968f70b9e422dabee49b8dc04b93e?from=pc" alt="流行算法：竞赛必备-动态规划算法 &lt;一&gt;"></p><p>图8-2</p><h1 id="九、参考资料"><a href="#九、参考资料" class="headerlink" title="九、参考资料"></a><strong>九、参考资料</strong></h1><p>[1] Bellman的自传书 <Eye of the Hurricane: An Autobiography></p><p>[2]《算法图解》Aditya Bhargava著，袁国忠译</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>动态规划最短路径-维特比算法</title>
      <link href="/2022/02/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84-%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/"/>
      <url>/2022/02/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84-%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6936529283917824519/?log_from=6396cd54cb25a_1651649299705">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a><strong>一、定义</strong></h1><p>维特比(Viterbi)算法说白了就是动态规划实现最短路径。由安德鲁·维特比(Andrew Viterbi)于1967年提出，用于在数字通信链路中解卷积以消除噪音。</p><p>所谓动态规划，其核心就是“动态”的概念，把大的问题细分为多个小的问题，基于每一步的结果再去寻找下一步的策略，通过每一步走过之后的局部最优去寻找全局最优。</p><h1 id="二、求篱笆网络（Lattice）的最短路径问题"><a href="#二、求篱笆网络（Lattice）的最短路径问题" class="headerlink" title="二、求篱笆网络（Lattice）的最短路径问题"></a><strong>二、求篱笆网络（Lattice）的最短路径问题</strong></h1><p>篱笆网络有向图的特点，如图2-1所示：</p><ul><li>有一个开始结点，有一个终点，带有方向；</li><li>同一列结点有多个，并且和上一列结点交错地连接起来；</li><li>不构成有向回路。</li></ul><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/7e335cac9e0f4830b68b4648555b3ce1?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图2-1</p><p><strong>题目：如图2-1，求A到E的最短路径？</strong></p><h1 id="1、穷举法找最短路径"><a href="#1、穷举法找最短路径" class="headerlink" title="1、穷举法找最短路径"></a><strong>1、穷举法找最短路径</strong></h1><p><strong>从A到C列 有3*3&#x3D; 9条路径：</strong></p><p>A B1 C1</p><p>A B1 C2</p><p>A B1 C3</p><p>A B2 C1</p><p>A B2 C2</p><p>A B2 C3</p><p>A B3 C1</p><p>A B3 C2</p><p>A B3 C3</p><p><strong>从A到D列 有3*3*3 &#x3D; 27条路径：</strong></p><p>A B1 C1 D1</p><p>A B1 C1 D2</p><p>A B1 C1 D3</p><p>A B1 C2 D1</p><p>A B1 C2 D2</p><p>A B1 C2 D3</p><p>A B1 C3 D1</p><p>A B1 C3 D2</p><p>A B1 C3 D3</p><p>A B2 C1 D1</p><p>A B2 C1 D2</p><p>A B2 C1 D3</p><p>A B2 C2 D1</p><p>A B2 C2 D2</p><p>A B2 C2 D3</p><p>A B2 C3 D1</p><p>A B2 C3 D2</p><p>A B2 C3 D3</p><p>A B3 C1 D1</p><p>A B3 C1 D2</p><p>A B3 C1 D3</p><p>A B3 C2 D1</p><p>A B3 C2 D2</p><p>A B3 C2 D3</p><p>A B3 C3 D1</p><p>A B3 C3 D2</p><p>A B3 C3 D3</p><p>假如整个网络长度是N, 宽度是D, 穷举法有D<em>D</em>D*D…D， D的N次幂条路径, 总的计算时间复杂度为o（D^N）即D的N次幂。</p><p>(不包括起始点和终点, N代表水平方向有的结点数，D代表竖直方向的结点数，本例子是N&#x3D;3, D&#x3D;3）</p><h1 id="2、维特比法找最短路径"><a href="#2、维特比法找最短路径" class="headerlink" title="2、维特比法找最短路径"></a><strong>2、维特比法找最短路径</strong></h1><p><strong>第1步，从A到C列有3*3条路径，计算量是3*3</strong></p><p>A B1 C1 6+5</p><p>A B2 C1 7+4</p><p><strong>A B3 C1 5+4 最短为9</strong></p><p>可以看出A到C1， A B3 C1 路径最短， 把这条路径记录下来。</p><p>A B1 C2 6+6</p><p><strong>A B2 C2 7+3 最短为10</strong></p><p>A B3 C2 5+6</p><p>可以看出A到C2， A B2 C2 路径最短， 把这条路径记录下来。</p><p>A B1 C3 6+9</p><p>A B2 C3 7+7</p><p><strong>A B3 C3 5+6 最短为11</strong></p><p>可以看出A到C3， A B3 C2 路径最短， 把这条路径记录下来。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/5f18b0c887164198a5b9a2995130bbf3?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图2-2</p><p><strong>第2步，从C到D列有3*3条路径，计算量是3*3</strong></p><p><strong>我们只把到A到C的最短的路径（红色部分A-B3-C1、A-B2-C2、A-B3-C3）参与到D的路径计算中，其它路径就不用计算了。</strong></p><p>A B3 C1 D1 9+7</p><p><strong>A B2 C2 D1 10+5&#x3D;15 最短为15</strong></p><p>A B3 C3 D1 11+5</p><p>可以看出A到D1， A B2 C2 D1 路径最短， 把这条路径记录下来。</p><p>A B3 C1 D2 9+8</p><p><strong>A B2 C2 D2 10+4&#x3D;14 最短为14</strong></p><p>A B3 C3 D2 11+7&#x3D;18</p><p>可以看出A到D2， A B2 C2 D2 路径最短， 把这条路径记录下来。</p><p><strong>A B3 C1 D3 9+3&#x3D;12 最短为12</strong></p><p>A B2 C2 D3 10+3&#x3D;13</p><p>A B3 C3 D3 11+6&#x3D;17</p><p>可以看出A到D3， A B3 C1 D3 路径最短， 把这条路径记录下来。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/1b14e31b81a14929b264e975812cf69c?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图2-3</p><p>第3步，从D到E有3条路径</p><p>A B2 C2 D1 E 15+4&#x3D;19</p><p>A B2 C2 D2 E 14+8&#x3D;22</p><p><strong>A B3 C1 D3 E 12+5&#x3D;17 最短为17</strong></p><p>假如整个网络长度是N,宽度是D,这样总的计算时间复杂度只有o((N-1)*D^2)。</p><p>(不包括起始点和终点,N代表水平方向有的结点数，D代表竖直方向的结点数，本例子是N&#x3D;3, D&#x3D;3）</p><h1 id="3、小结"><a href="#3、小结" class="headerlink" title="3、小结"></a><strong>3、小结</strong></h1><p>假如整个网络长度是N(不包括起始点和终点), 宽度是D, 维特比法的计算计算时间复杂度是o((N-1)*D^2); 穷举法的计算时间复杂度是o（D^N）即D的N次幂。维特比法的计算量比穷举法的计算量少很多，特别是当N和D都比较大时，效果特别明显。如穷举法要一个月才能计算完，维特比法只要几分钟就计算完了。这就是好的算法的巨大威力。</p><h1 id="三、从动态规划的角度理解维特比算法"><a href="#三、从动态规划的角度理解维特比算法" class="headerlink" title="三、从动态规划的角度理解维特比算法"></a><strong>三、从动态规划的角度理解维特比算法</strong></h1><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/e3526d6f2b824f66b1e22aae031812e5?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图3-1</p><p>如图3-1，求S到E的最短路径？</p><h1 id="动态规划四部曲"><a href="#动态规划四部曲" class="headerlink" title="动态规划四部曲"></a><strong>动态规划四部曲</strong></h1><h1 id="确定状态-（a-最后一步；b-化为子问题）"><a href="#确定状态-（a-最后一步；b-化为子问题）" class="headerlink" title="确定状态 （a. 最后一步；b. 化为子问题）"></a><strong>确定状态 （a. 最后一步；b. 化为子问题）</strong></h1><p>设f(n, j)代表s到第t&#x3D;n列，第j层结点o(tn,j)的最短距离。</p><p><strong>最后一步求</strong>：</p><p>f(E) &#x3D; min{f(n, 1), f(n, 2), f(n, 3)}</p><p>(S到E点的最短距离就是S到第t&#x3D;n列各结点o(tn,1)、o(tn,2)、o(tn,3)最短距离的最小值)</p><p>前一步是求t&#x3D;n-1列各结点（最短距离为f(n-1, 1), f(n-1,2), f(n-1,3)）到t&#x3D;n列各结点的最短距离。</p><p><strong>原问题是求S到E的最短距离转化为子问题:</strong></p><p>已知S到第t&#x3D;i列各结点的最短距离，求t&#x3D;i列各结点到t&#x3D;i+1列各结点的最短距离。</p><h1 id="转移方程"><a href="#转移方程" class="headerlink" title="转移方程"></a><strong>转移方程</strong></h1><p>f(n,1) &#x3D; min{f(n-1, 1)+A(n-1,1; n, 1,), f(n-1,2)+A(n-1,2; n, 1,)，f(n-1,3)+A(n-1,3;n,1)}</p><p>f(n,2) &#x3D; min{f(n-1, 1)+A(n-1,1; n, 2,), f(n-1,2)+A(n-1,2; n, 2,)，f(n-1,3)+A(n-1,3;n,2)}</p><p>f(n,3) &#x3D; min{f(n-1, 1)+A(n-1,1; n, 3,), f(n-1,2)+A(n-1,2; n, 3,)，f(n-1,3)+A(n-1,3;n,3)}</p><p>其中A为第n-1列各结点到第n列各结点的加权距离。</p><h1 id="开始和边界条件"><a href="#开始和边界条件" class="headerlink" title="开始和边界条件"></a><strong>开始和边界条件</strong></h1><p>f(1,1) &#x3D;f(0, 1)，f(0,1)表示S到结点O(t1, 1)的距离；</p><p>f(1,2) &#x3D;f(0, 2)，f(0,2)表示S到结点O(t1, 2)的距离；</p><p>f(1,3) &#x3D;f(0, 3)，f(0,3)表示S到结点O(t1, 2)的距离。</p><h1 id="计算顺序"><a href="#计算顺序" class="headerlink" title="计算顺序"></a><strong>计算顺序</strong></h1><p>f(1,1)，f(1,2)，f(1,3)，f(2,1)，f(2,2)，f(2,3)，…，f(n-1,1)，f(n-1,2)，f(n-1,3)，f(n,1)，f(n,2)，f(n,3)。</p><h1 id="四、维特比算法的理解可以概括成三点"><a href="#四、维特比算法的理解可以概括成三点" class="headerlink" title="四、维特比算法的理解可以概括成三点"></a><strong>四、维特比算法的理解可以概括成三点</strong></h1><p>1）如果最短路径P经过某个点，比如图3-1中的O(t2,2)，那么这条路径上的起始点S到O(t2,2)的这段子路径Q，一定是S到O(t2,2)之间的最短路径。否则，用S到O(t2,2)的最短路径R替代Q，便构成一条比P更短的路径，这显然是矛盾的。证明了满足最优化原理。</p><p>2）从S到E的路径必定经过第i列的某个结点，假定第i列有k个结点，那么如果记录了从S到第i列的所有k个结点的最短路径，最终的最短路径必经过其中一条，这样，只要考虑非常有限的最短路即可。</p><p>3）结合以上两点，假定当我们从i列进入i+1列时，从S到第i列上各个结点的最短路径已经找到，并且记录在这些结点上，那么在计算从起点S到第i+1列的某个结点Xi+1的最短路径时，只要考虑从S到前一列i所有的k个结点的最短路径，以及从这些结点到Xi+1的距离即可。</p><h1 id="五、应用"><a href="#五、应用" class="headerlink" title="五、应用"></a><strong>五、应用</strong></h1><p>维特比算法被广泛应用于CDMA和GSM数字蜂窝网络、拨号调制解调器、卫星、深空通信和802.11无线网络中解卷积码。</p><p>维特比算法用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中。维特比算法之所以重要，是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词、地图导航等。——《数学之美》</p><h1 id="先介绍隐马尔可夫模型的基础知识"><a href="#先介绍隐马尔可夫模型的基础知识" class="headerlink" title="先介绍隐马尔可夫模型的基础知识"></a><strong>先介绍隐马尔可夫模型的基础知识</strong></h1><p>隐马尔可夫模型(HMM)问题可由下面五个元素描述：</p><p>观测序列（observations）：实际观测到的现象序列<br>隐含状态（states）：所有的可能的隐含状态<br>初始概率（start_probability）：每个隐含状态的初始概率<br>转移概率（transition_probability）：从一个隐含状态转移到另一个隐含状态的概率<br>发射概率（emission_probability）：某种隐含状态产生某种观测现象的概率</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/d2979acee4c3433fba08845a25a8000b?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-1</p><p>图5-1中展现的隐马尔科夫模型中的状态序列，其中一共包含5种隐含状态，状态序列的长度为7，那么图中很明显横轴是时间轴，纵轴是隐含状态轴。</p><h1 id="例子1-医生看病"><a href="#例子1-医生看病" class="headerlink" title="例子1 医生看病"></a><strong>例子1 医生看病</strong></h1><p>想象一个乡村诊所。村民有着非常理想化的特性，要么健康要么发烧。他们只有问诊所的医生的才能知道是否发烧。 聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。</p><p>假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种“健康”和“发烧”，但医生不能直接观察到，这意味着状态对他是“隐含”的。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。整个系统为一个隐马尔可夫模型(HMM)。</p><p>医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。 换句话说，医生知道隐马尔可夫模型的参数。</p><p>隐含状态 &#x3D; (‘健康’, ‘发烧’)<br>观测序列 &#x3D; (‘正常’, ‘冷’, ‘头晕’)<br>初始概率 &#x3D; {‘健康’: 0.6, ‘发烧’: 0.4}<br>转移概率 &#x3D; {<br>‘健康’ : {‘健康’: 0.7, ‘发烧’: 0.3},<br>‘发烧’ : {‘健康’: 0.4, ‘发烧’: 0.6},<br>}<br>发射概率 &#x3D; {<br>‘健康’ : {‘正常’: 0.5, ‘冷’: 0.4, ‘头晕’: 0.1},<br>‘发烧’ : {‘正常’: 0.1, ‘冷’: 0.3, ‘头晕’: 0.6},<br>}</p><p>其对应的状态转移图如图5-2所示：</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/fb7204ccd8bf4e68ba1aa74c0125a92b?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-2</p><p>题目：</p><p>已知：医生知道隐马尔可夫模型的参数；阿华连续三天的身体感觉依次是正常、冷、头晕。</p><p>求：阿华这三天的身体健康状态变化的过程是怎么样的？</p><p>解：横轴观测序列的长度为 3（第一天 正常、第二天 冷、第三天 头晕），纵轴隐含状态个数为2（健康、发烧）。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/316a7e4c21934a93bd03c8acd771fbb9?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/8adef5788aef461ab800cb7ade01ecfa?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/b261d151b77746a5ba134fbbcb0c0308?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/37cf06429b0b4fa993aa20bdbec689e1?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/003977e62e644ecca0b37524d6677679?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/a2ce999e8e624d58b1e62b31d5da5338?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>维特比算法揭示了观察结果 [‘正常’, ‘冷’, ‘发晕’] 最有可能由状态序列 [‘健康’, ‘健康’, ‘发烧’]产生。 换句话说，对于观察到的活动, 病人第一天感到正常，第二天感到冷时都是健康的，而第三天发烧了。</p><p>维特比算法的计算过程可以直观地由上面系列图表示。黑色加粗的是维特比路径。</p><h1 id="例子2-通过拼音识别汉字"><a href="#例子2-通过拼音识别汉字" class="headerlink" title="例子2 通过拼音识别汉字"></a><strong>例子2 通过拼音识别汉字</strong></h1><p>这是维特比算法求解隐马尔可夫模型问题，用维特比算法针对隐马尔可夫模型三大问题中的解码问题(给定模型和观测序列，如何找到与此观测序列最匹配的状态序列的问题)进行求解。</p><p>首先，我们已经知道状态序列X会产生观测序列O {wo, ai, zhong, guo}:</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/49561b4502f84b9b9353dad70663864b?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-3</p><p>观测序列O对应的状态序列X有很多种可能，对应的概率如图5-4所示，比如ai可能有三种可能：哎、爱、挨。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/7a4cc02401054e2a8e1c71a5bbcb983c?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-4</p><p>观测序列O求状态序列问题就变成了最大概率问题，把概率最大理解为路径最短，转换为了求解最短路径问题。</p><h1 id="1、模型化为篱笆网络。如图5-6所示。"><a href="#1、模型化为篱笆网络。如图5-6所示。" class="headerlink" title="1、模型化为篱笆网络。如图5-6所示。"></a><strong>1、模型化为篱笆网络。如图5-6所示。</strong></h1><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/7141c099e37245a1877702dc4545a160?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-6</p><h1 id="2、利用维特比算法求得最短路径。如5-7所示。"><a href="#2、利用维特比算法求得最短路径。如5-7所示。" class="headerlink" title="2、利用维特比算法求得最短路径。如5-7所示。"></a><strong>2、利用维特比算法求得最短路径。如5-7所示。</strong></h1><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/b3e846a967b94d428af6d5c44fbb078f?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><p>图5-7</p><h1 id="六、附录"><a href="#六、附录" class="headerlink" title="六、附录"></a><strong>六、附录</strong></h1><h1 id="隐马尔可夫模型中的维特比算法与伪代码实现-1"><a href="#隐马尔可夫模型中的维特比算法与伪代码实现-1" class="headerlink" title="隐马尔可夫模型中的维特比算法与伪代码实现[1]"></a><strong>隐马尔可夫模型中的维特比算法与伪代码实现[1]</strong></h1><h1 id="1、算法描述"><a href="#1、算法描述" class="headerlink" title="1、算法描述"></a><strong>1、算法描述</strong></h1><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/42edad2f4a7c458989f0169937685b42?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><h1 id="2、伪代码实现"><a href="#2、伪代码实现" class="headerlink" title="2、伪代码实现"></a><strong>2、伪代码实现</strong></h1><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/3efd5d476e844c309c764909c83e9124?from=pc" alt="流行算法：动态规划最短路径-维特比算法"></p><h1 id="七、参考资料"><a href="#七、参考资料" class="headerlink" title="七、参考资料"></a><strong>七、参考资料</strong></h1><p>[1] zh.wikipedia.org&#x2F;wiki&#x2F;维特比算法</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>模拟退火算法</title>
      <link href="/2022/01/11/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
      <url>/2022/01/11/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="read://https_www.toutiao.com/?url=https%3A%2F%2Fwww.toutiao.com%2Farticle%2F6940338047137399299%2F%3Flog_from%3De0ab1a7254c4c_1636702361642">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>模拟退火算法（Simulated Annealing，SA）是一种通用概率演算法，用来在一个大的搜寻空间内找寻命题的最优解。“模拟退火”算法是源于对热力学中退火过程的模拟，在某一给定初温下，通过缓慢下降温度参数，使算法能够在多项式时间内给出一个近似最优解。最早的思想是由Metropolis在1953年提出，Kirkpatrick等人把模拟退火思想与组合最优化的相似点进行类比，将模拟退火算法引入到组合优化领域。模拟退火算法是解决传统方法难处理的NP完全问题(Nondeterministic Polynomial Complete Problem)，TSP（Traveling Salesman Problem）货郎担问题的有效方法之一。</p><p>模拟退火算法是一种高效、通用、易实现的优化算法，适合于解决大规模组合优化问题的通用而有效的近似算法。</p><p>模拟退火算法是一种贪心算法，也是一种随机算法，它有较强的局部搜索能力，对参数依赖性较强，并不一定能找到全局的最优解，但可以比较快的找到问题的近似最优解。</p><p>SA特别适合并行运算，描述简单、初始条件限制少、使用灵活且效率高，所以被广泛应用。</p><h1 id="二、与贪心算法、爬山算法、-梯度下降算法的比较"><a href="#二、与贪心算法、爬山算法、-梯度下降算法的比较" class="headerlink" title="二、与贪心算法、爬山算法、 梯度下降算法的比较"></a>二、与贪心算法、爬山算法、 梯度下降算法的比较</h1><p>爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。</p><p>爬山算法实现很简单，其主要缺点是会陷入局部最优解，而不一定能搜索到全局最优解。如图2-1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/0cae0994c4cb4dc6847f84cfbf328c3f?from=pc" alt="流行算法：模拟退火算法"></p><p>图2-1</p><p>模拟退火其实也是一种贪心算法，与爬山算法不同，它是下降寻找谷底，并且它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。如图2-2所示，它的搜索路径是A-&gt;B-&gt;C-&gt;D-&gt;E-&gt;F-G, 最终找到了G这个全局最优解。</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/593ceaffaefa466397760bbfacd03b96?from=pc" alt="流行算法：模拟退火算法"></p><p>图2-2</p><p>关于普通贪心算法与模拟退火，有一个有趣的比喻：</p><p>普通贪心算法：兔子朝着比现在低的地方跳去。它找到了不远处的最低的山谷。但是这座山谷不一定最低的。 它不能保证局部最优值就是全局最优值。</p><p>模拟退火：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向低处，也可能踏入平地。但是，它渐渐清醒了并朝最低的方向跳去。</p><p>梯度下降法和模拟退火算法都是求最优解的方法，但它们也有所不同。利用一阶导数去求极值的梯度下降法，可以求得局部最优解，但不能直接搜索到全局最优解；由于退火算法加入了候选解，候选解由一定的概率密度分布从解空间随机采样获得，故可以取到全局最优解。</p><h1 id="三、基本原理"><a href="#三、基本原理" class="headerlink" title="三、基本原理"></a>三、基本原理</h1><p>模拟退火算法的出发点是基于物理中固体物质的退火过程与一般组合优化问题之间的相似性。物理中固体物质的退火过程由加温阶段、平衡阶段、冷却阶段这三部分组成。加温阶段对应算法的设定初温，平衡阶段对应算法的采样过程，该过程须遵循Metropolis准则，冷却阶段对应算法控制参数下降。模拟退火基本原理如图3-1与图3-2所示。</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/8048bee54ce3409ca771c2f21cb1790c?from=pc" alt="流行算法：模拟退火算法"></p><p>图3-1 固体退火过程示意图</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/9214a19884524f3cb273b3a1f3ac2391?from=pc" alt="流行算法：模拟退火算法"></p><p>图3-2 模拟退火算法流程示意图</p><p>在把模拟退火算法应用于最优化问题时，一般可以将温度T当做控制参数，目标函数值f视为内能E，而固体在某温度T时的一个状态对应一个解。然后算法试图随着控制参数Ｔ的降低，使目标函数值f(内能E)也逐渐降低，直至趋于全局最小值(退火中低温时的最低能量状态)，就像固体退火过程一样。</p><h1 id="四、主要思想"><a href="#四、主要思想" class="headerlink" title="四、主要思想"></a>四、主要思想</h1><p>就函数最小值问题来说，模拟退火的主要思想是：在搜索区间（二维平面中）随机游走（即随机选择点），再以Metropolis抽样准则，使随机游走逐渐收敛于局部最优解。而温度即是Metropolis算法中的一个重要控制参数，可以认为这个参数的大小控制了搜索过程向局部或全局最优解移动的快慢。</p><p>冷却参数表、领域结构和新解产生器、接受准则和随机数产生器（即Metropolis算法）一起构成算法的三大支柱。</p><p>Metropolis是一种有效的重点抽样法：系统从能量一个状态变化到另一个状态时，相应的能量从E1变化到E2，概率为p &#x3D; exp[ - (E2- E1)&#x2F;T ]。如果E2 &lt; E1，系统接收此状态，否则，以一个随机的概率接收此或丢弃此状态。这种经常一定次数的迭代，系统会逐渐趋于一引稳定的分布状态。</p><p>重点抽样时，新状态下如果向下则接受（局部最优），若向上（全局搜索），以一定机率接受。模拟退火方法从某个初始解出发,经过大量解的变换后,可以求得给定控制参数值时组合优化问题的相对最优解。然后减小控制参数T的值，重复执行Metropolis算法，就可以在控制参数T趋于零时，最终求得组合优化问题的整体最优解。控制参数的值必须缓慢衰减。</p><p>其中温度是一个Metropolis的重要控制参数，模拟退火可视为递减控制参数T时Metroplis算法的迭代。开始T值大，可能接受较差的恶化解，随着T的减小，只能接受较好的恶化解，最后在T趋于0时，就不再接受任何恶化解了。</p><p>在无限高温时，系统立即均匀分布，接受所有提出的变换。T的衰减越小，T到达终点的时间越长，但可使马可夫链长越小，到达准平衡分布的时间越短，</p><h1 id="五、Metropolis准则"><a href="#五、Metropolis准则" class="headerlink" title="五、Metropolis准则"></a>五、Metropolis准则</h1><p>模拟退火算法使用Metropolis准则产生组合优化问题解的序列，并使用对应的转移概率P确定是否接受当前解到新解的转移。</p><h1 id="1、准则的由来"><a href="#1、准则的由来" class="headerlink" title="1、准则的由来"></a>1、准则的由来</h1><p>同样的问题，如果使用蒙特卡洛(MonteCarlo)算法模拟，需要大量采样，工作量很大。因而1953年Metropolis提出了这样一个重要性采样的方法， 即设从当前状态i生成新状态j。若新状态的内能小于状态i的内能(即Ej&lt;Ei)，则接受新状态j作为新的当前状态； 否则，以概率接受状态j， 这就是通常所说的Metropolis准则。</p><h1 id="2、准则详解"><a href="#2、准则详解" class="headerlink" title="2、准则详解"></a>2、准则详解</h1><p>Metropolis准则，其实就是以概率p接受新状态。</p><p>若在温度T，当前状态i → 新状态j，则概率p满足：</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/b3057b4082be4749a4c9f0c70cf948cc?from=pc" alt="流行算法：模拟退火算法"></p><ul><li>当Ej &lt; Ei时，接受新状态的概率p&#x3D;1, 即完全接受 j 为当前状态；</li><li>当Ej ≥ Ei时，若概率 p&#x3D;exp[-(Ej-Ei)&#x2F;T]大于[0,1)区间的随机数，则仍接受状态 j 为当前状态；若不成立，则保留状态 i 为当前状态。</li></ul><p>温度T越小，则降温概率p就越小；温度越高，降温概率p就越大，p越大，则j状态是重要状态的概率就越大。若j是重要状态，则取代i成为当前状态，否则舍弃新状态。再重复以上新状态产生过程。</p><p>很明显，在高温下，可接受与当前状态能量差较大的新状态；在低温下，只接受与当前状态能量差较小的新状态。这与不同温度下热运动的影响完全一致，在温度趋于零时，就不能接受任何成立时的新状态j了。</p><p>上述接受新状态的准则称为Metropolis准则，相应的算法被称为Metropolis算法。</p><h1 id="六、算法实现步骤"><a href="#六、算法实现步骤" class="headerlink" title="六、算法实现步骤"></a>六、算法实现步骤</h1><h1 id="SA算法具体步骤"><a href="#SA算法具体步骤" class="headerlink" title="ＳＡ算法具体步骤:"></a>ＳＡ算法具体步骤:</h1><h1 id="1、流程图表示"><a href="#1、流程图表示" class="headerlink" title="1、流程图表示"></a>1、流程图表示</h1><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/7432882bdbfc41d9a7501b85397c1657?from=pc" alt="流行算法：模拟退火算法"></p><p>图6-1</p><p>最简单的状态产生函数：ω’ &#x3D; ω + R (R是个随机数）。</p><h1 id="2、文字描述表示"><a href="#2、文字描述表示" class="headerlink" title="2、文字描述表示"></a>2、文字描述表示</h1><p>模拟退火算法用冷却进度表来控制算法的进程，是算法在控制参数T徐徐降温并趋于零时最终求得组合优化问题的相对全局最优解。其中优化问题的一个解i及其目标函数f(i)分别与固体的一个微观状态ｉ及其能量Ｅi相对应。</p><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/e2df5554992d43789b27e338da141fab?from=pc" alt="流行算法：模拟退火算法"></p><h1 id="3、从新解的产生与接受来表示"><a href="#3、从新解的产生与接受来表示" class="headerlink" title="3、从新解的产生与接受来表示"></a>3、从新解的产生与接受来表示</h1><p>模拟退火算法新解的产生和接受可分为如下四个步骤：</p><p>第1步，由一个产生函数从当前解产生一个位于解空间的新解。为便于后续的计算和接受，减少算法耗时，通常选择由当前新解经过简单地变换即可产生新解的方法，如对构成新解的全部或部分元素进行置换、互换等，注意到产生新解的变换方法决定了当前新解的邻域结构，因而对冷却进度表的选取有一定的影响。</p><p>第2步，计算与新解所对应的目标函数差。因为目标函数差仅由变换部分产生，所以目标函数差的计算最好按增量计算。事实表明，对大多数应用而言，这是计算目标函数差的最快方法。</p><p>第3步，判断新解是否被接受。判断的依据是一个接受准则，最常用的接受准则是Metropolis准则: 若Δt′&lt;0则接受S′作为新的当前解S，否则以概率exp(-Δt′&#x2F;T)接受S′作为新的当前解S。</p><p>第4步，当新解被确定接受时，用新解代替当前解，这只需将当前解中对应于产生新解时的变换部分予以实现，同时修正目标函数值即可。此时，当前解实现了一次迭代。可在此基础上开始下一轮试验。而当新解被判定为舍弃时，则在原当前解的基础上继续下一轮试验。</p><p>模拟退火算法与初始值无关，算法求得的解与初始解状态S(是算法迭代的起点)无关；模拟退火算法具有渐近收敛性，已在理论上被证明是一种以概率收敛于全局最优解的全局优化算法。</p><h1 id="七、算法要素"><a href="#七、算法要素" class="headerlink" title="七、算法要素"></a>七、算法要素</h1><h1 id="1、状态空间"><a href="#1、状态空间" class="headerlink" title="1、状态空间"></a>1、状态空间</h1><p>状态空间，即解空间，也称为搜索空间，是经过编码的可能解组成的集合。</p><h1 id="2、状态产生函数"><a href="#2、状态产生函数" class="headerlink" title="2、状态产生函数"></a>2、状态产生函数</h1><p>应尽可能保证产生的候选解遍布整个状态空间，可采用附加扰动、随机产生、移位、平滑、边界取值等多种算子作为模拟退火的状态产生函数。状态产生函数通常由产生的候选解的方式和候选解产生的概率分布两部分组成。候选解一般采用按照某一概率密度函数对解空间进行随机采样来获得。概率分布可以是均匀分布、正态分布、指数分布等。</p><h1 id="3、状态转移概率"><a href="#3、状态转移概率" class="headerlink" title="3、状态转移概率"></a>3、状态转移概率</h1><ul><li>状态转移概率是指从一个状态向另一个状态的转移概率。</li><li>通俗的理解是接受一个新解为当前解的概率。</li><li>它与当前的温度参数T有关，随温度下降而减小。</li><li>一般采用Metropolis准则。</li></ul><h1 id="4、初始温度"><a href="#4、初始温度" class="headerlink" title="4、初始温度"></a>4、初始温度</h1><p>实验表明，初始温度越高，得到高质量解的概率越大，但算法计算的时间越长。兼顾算法求解的质量和执行效率，选择初温的方法常有:随机产生一组状态，确定最大值和最小值间的差值d，利用函数得到初温，如 T0&#x3D; dp，其中p为初始接受概率; 或均匀抽样一组状态，以各状态的目标值的方差为初温。</p><h1 id="5、冷却进度表"><a href="#5、冷却进度表" class="headerlink" title="5、冷却进度表"></a>5、冷却进度表</h1><p>冷却进度表是指从某一高温状态T向低温状态冷却时的降温管理表，也即温度衰减函数，也可称为退火策略。</p><ol><li>最简单的控制参数衰减函数（指数降温）为：</li></ol><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/4518ce0616e04782b49704ab2d48ac8f?from=pc" alt="流行算法：模拟退火算法"></p><p>其中a常取值0.5~0.9, 这种退火策略很常用。</p><ol><li>经典的模拟退火衰减函数（对数降温）为：</li></ol><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/ac9607c4c73e4912b632d2a094e1d740?from=pc" alt="流行算法：模拟退火算法"></p><ol><li>可以选择其它的衰减函数：</li></ol><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/dbab96ab24834881bd995095b9112d76?from=pc" alt="流行算法：模拟退火算法"></p><p>衰减函数应选择小的比较合适，可避免过长的马尔可夫链和迭代次数增加。</p><h1 id="6、内循环终止的准则"><a href="#6、内循环终止的准则" class="headerlink" title="6、内循环终止的准则"></a>6、内循环终止的准则</h1><p>内循环终止的准则，也称为Metropolis抽样稳定准则，用于决定各温度下产生候选解的个数，该准则包括有：目标函数的平均值是否稳定；连续若干步目标函数值变化较小；按一定的步数抽样。</p><h1 id="7、外循环终止的准则"><a href="#7、外循环终止的准则" class="headerlink" title="7、外循环终止的准则"></a>7、外循环终止的准则</h1><p>外循环终止的准则，即决定算法何时结束，主要包括：循环迭代的次数设置；终止温度的阈值设置；检验系统熵值是否稳定；检验算法收敛到最优值连续若干步保持不变。其中终止温度的设置常用的是Kirkpatrick等提出准则：在若干个马尔可夫链中解无任何变化（恶化或优化）就终止算法。迭代次数L的选取与冷却进度表密切相关，一般T衰减小，则 L值就适当大。</p><h1 id="八、参数的选择"><a href="#八、参数的选择" class="headerlink" title="八、参数的选择"></a>八、参数的选择</h1><p>模拟退火算法的应用很广泛，可以求解NP完全问题，但其参数难以控制，其主要问题有以下几点：</p><h1 id="1、状态转换步长"><a href="#1、状态转换步长" class="headerlink" title="1、状态转换步长"></a>1、状态转换步长</h1><p>状态转换很重要，状态的搜索、跳转策略直接影响着算法的性能。可采用附加扰动、随机产生、移位、平滑、边界取值等多种算子作为状态产生函数X(i+1) &#x3D; G(Xi)。</p><p>比如根据具体的问题去分析，状态有没有边界，如果有边界的话，在状态跳转时要用边界限制。状态转换是从当前状态至下一状态，每一次跳动都是随机的，但不是完全随机的，是在一定范围内进行的跳动。那么，两个问题来了，</p><ul><li>范围怎么确定?</li><li>范围确定后，状态怎么转换?</li></ul><p>退火的跳动幅度是随着温度的下降逐渐降低的，因此，跳动范围须与温度建立关系，随温度下降，跳动范围减少。这个减少可以是线性的，也可以是非线性的。范围确定了，下一次状态的跳转可由高斯随机数生成，通过控制高斯方差就可以控制跳动范围。</p><p>状态转换完全随机会怎样？完全随机意味着 状态开始时乱跳，温度降下来后也乱跳，这就就违背了退火算法思想。</p><h1 id="2、控制参数初值T0的选取"><a href="#2、控制参数初值T0的选取" class="headerlink" title="2、控制参数初值T0的选取"></a>2、控制参数初值T0的选取</h1><p>温度T的初始值设置是影响模拟退火算法全局搜索性能的重要因素之一。初始温度高，则搜索到全局最优解的可能性大，但因此要花费大量的计算时间；反之，则可节约计算时间，但全局搜索性能可能受到影响。</p><p>一般要求初始值T0的值要充分大，即一开始就处于高温状态，且Metropolis的接收率约为1。 在无限高温时，系统立即均匀分布，接受所有提出的变换。实际应用过程中，初始温度一般需要依据实验结果进行若干次调整。</p><h1 id="3、衰减函数的选取"><a href="#3、衰减函数的选取" class="headerlink" title="3、衰减函数的选取"></a>3、衰减函数的选取</h1><p>衰减函数用于控制温度的退火速度，一个常用的指数函数为：T(n + 1) &#x3D; K*T(n)，其中K是一个非常接近于1的常数。</p><p>T的衰减越小，T到达终点的时间越长，但可使马可夫链长越小，到达准平衡分布的时间越短，</p><h1 id="4、马可夫链长度L（内循环迭代次数）的选取"><a href="#4、马可夫链长度L（内循环迭代次数）的选取" class="headerlink" title="4、马可夫链长度L（内循环迭代次数）的选取"></a>4、马可夫链长度L（内循环迭代次数）的选取</h1><p>马可夫链长度L是指每一次随机游走过程，要迭代多少次，才能趋于一个准平衡分布，即一个局部收敛解位置。</p><p>原则是，在衰减参数T的衰减函数已选定的前提下，L应选得在控制参数的每一取值上都能恢复准平衡。</p><h1 id="5、终止条件"><a href="#5、终止条件" class="headerlink" title="5、终止条件"></a>5、终止条件</h1><p>有很多种终止条件的选择，各种不同的条件对算法的性能和解的质量有很大影响。这里介绍一个常用的终止条件，即上一个最优解与最新的一个最优解的之差小于某个容差，就可停止此次马尔可夫链的迭代。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>有效的参数选择判据是：</p><ul><li>算法的收敛：主要取决于衰减函数和马可夫链的长度及停止准则的选择；</li><li>算法的实验性能：最终解的质量和CPU的时间。</li></ul><h1 id="例子：使用模拟退火法求函数f-x-y-x3D-5sin-xy-x-2-y-2的最小值。"><a href="#例子：使用模拟退火法求函数f-x-y-x3D-5sin-xy-x-2-y-2的最小值。" class="headerlink" title="例子：使用模拟退火法求函数f(x,y) &#x3D; 5sin(xy) + x^2 + y^2的最小值。"></a>例子：使用模拟退火法求函数f(x,y) &#x3D; 5sin(xy) + x^2 + y^2的最小值。</h1><p>解：根据题意，我们设计参数为：</p><p>Metropolis的步长为0.02</p><p>初始温度为100</p><p>衰减参数为0.95</p><p>马可夫链长度为10000</p><p>结束条件为根据上一个最优解与最新的一个最优解的之差小于某个容差。</p><h1 id="九、算法设计的注意事项"><a href="#九、算法设计的注意事项" class="headerlink" title="九、算法设计的注意事项"></a>九、算法设计的注意事项</h1><ol><li><p>设计合适的状态产生函数，使其根据搜索进程的需要表现出状态的全空间分散性或局部区域性；</p></li><li><p>设计高效的退火策略；</p></li><li><p>避免状态的迂回搜索；</p></li><li><p>采用并行搜索结构；</p></li><li><p>为避免陷入局部极小，改进对温度的控制方式；</p></li><li><p>选择合适的初始状态；</p></li><li><p>设计合适的算法终止准则。</p></li></ol><h1 id="十、模拟退火算法改进策略"><a href="#十、模拟退火算法改进策略" class="headerlink" title="十、模拟退火算法改进策略"></a>十、模拟退火算法改进策略</h1><p>通过对模拟退火算法的要素的改进或与其它算法相结合可提高模拟退火算法的性能。</p><h1 id="自身要素的改进"><a href="#自身要素的改进" class="headerlink" title="自身要素的改进"></a>自身要素的改进</h1><ol><li>提升初温。算法初始化，增加升温或重升温过程，将温度适当提高，以激活各状态的接受概率，可以避免算法在局部极小解处停滞不前。</li><li>增加记忆功能。增加存储环节，将当前最好状态记录下来，避免搜索过程由于因根据 Metropolis准则接受当前解时，可能丢掉当前遇到的最优解。</li><li>增加补充搜索过程。在退火结束后，把当前最优解设置为初始状态，再次执行模拟退火过程或局部性的搜索。</li><li>多次搜索策略。对每一当前状态，采用多次搜索策略，取代标准SA算法的单次比较方式。</li></ol><h1 id="与其它搜索算法相结合。"><a href="#与其它搜索算法相结合。" class="headerlink" title="与其它搜索算法相结合。"></a>与其它搜索算法相结合。</h1><p>混合模拟退火算法，模拟退火与遗传算法相结合。近年来，模拟退火算法与遗传算法的融合在计划调度、机器人研究、软硬划分等方向均有应用。因此，今后要进一步深入研究 SA 与 GA 算法的结合，充分利用双方的优势，使其在相应的科学领域发挥更大的作用。</p><h1 id="十一、应用"><a href="#十一、应用" class="headerlink" title="十一、应用"></a>十一、应用</h1><p>模拟退火算法可以较高的效率求解很多问题, 应用也非常广泛。</p><p>1、最大截问题(Max Cut Problem)</p><p>2、0-1背包问题(Zero One Knapsack Problem)</p><p>3、图着色问题(Graph Colouring Problem)</p><p>4、调度问题(Scheduling Problem)</p><p>5、在VLSI设计中的应用</p><p>利用模拟退火算法进行VLSI的最优设计，是目前模拟退火算法最成功的应用实例之一。用模拟退火算法几乎可以很好地完成所有优化的VLSI设计工作。如全局布线、布板、布局和逻辑最小化等等。</p><p>6、在神经网计算机中的应用</p><p>模拟退火算法具有跳出局部最优陷阱的能力。在Boltzmann机中，即使系统落入了局部最优的陷阱，经过一段时间后，它还能再跳出来，再系统最终将往全局最优值的方向收敛。</p><p>7、在图像处理中的应用</p><p>模拟退火算法可用来进行图像恢复等工作，即把一幅被污染的图像重新恢复成清晰的原图，滤掉其中被畸变的部分。因此它在图像处理方面的应用前景是广阔的。</p><p>8、模拟退火算法的其他应用</p><p>除了上述应用外，模拟退火算法还用于其它各种组合优化问题，如TSP和Knapsack问题等。大量的模拟实验表明，模拟退火算法在求解这些问题时能产生令人满意的近似最优解，而且所用的时间也不很长。</p><h1 id="十二、附录"><a href="#十二、附录" class="headerlink" title="十二、附录"></a>十二、附录</h1><h1 id="附录1-模拟退火算法的数学模型"><a href="#附录1-模拟退火算法的数学模型" class="headerlink" title="附录1 模拟退火算法的数学模型"></a>附录1 模拟退火算法的数学模型</h1><p><img src="https://p26.toutiaoimg.com/origin/pgc-image/f2d8c42aac23480eaed50f1b9d6df732?from=pc" alt="流行算法：模拟退火算法"></p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>追根溯源之统计推断常用概率分布</title>
      <link href="/2022/01/05/%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90%E4%B9%8B%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/"/>
      <url>/2022/01/05/%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90%E4%B9%8B%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6949464472935268895/">今日头条</a></p><h1 id="一、统计与概率论的一些概念"><a href="#一、统计与概率论的一些概念" class="headerlink" title="一、统计与概率论的一些概念"></a>一、统计与概率论的一些概念</h1><h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>PDF- 概率密度函数</p><p>CDF - 累积分布函数</p><p>Quantile - 分位数</p><p>Mean - 均值</p><p>Median - 中位数</p><p>Mode - 众数</p><p>Variance - 方差</p><p>Skewness - 偏度</p><p>Kurtosis - 峰度</p><p>Entropy - 信息熵</p><p>CF - Characteristic Function, 特征函数</p><p>MGF - Moment Generating Function, 矩量母函数, 也被称为动差生成函数</p><p>Erf - Error function, 误差函数</p><h1 id="1、累积分布函数CDF-Cumulative-Distribution-Function"><a href="#1、累积分布函数CDF-Cumulative-Distribution-Function" class="headerlink" title="1、累积分布函数CDF(Cumulative Distribution Function)"></a>1、累积分布函数CDF(Cumulative Distribution Function)</h1><p>If X is any random variable, then its CDF is defined for any real number x by</p><p>F(x)&#x3D;P(X ≤ x)</p><h1 id="2、概率密度函数PDF-Probability-Density-Function"><a href="#2、概率密度函数PDF-Probability-Density-Function" class="headerlink" title="2、概率密度函数PDF(Probability Density Function)"></a>2、概率密度函数PDF(Probability Density Function)</h1><p>The probability density function (PDF) f(x) of a continuous distribution is defined as the derivative of the (cumulative) distribution function F(x),</p><p>ƒ(x) &#x3D;dF(x)&#x2F;dx</p><p>so we have</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/86d351e34e76419884a4f608286b69fb?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="3、众数-Mode"><a href="#3、众数-Mode" class="headerlink" title="3、众数 (Mode)"></a>3、众数 (Mode)</h1><p>它是样本观测值在频数分布表中频数最多的那一组的组中值，主要应用于大面积普查研究之中。例如：1，2，3，3，4的众数是3；例如：1，2，2，3，3，4的众数是2和3；例如：1，2，3，4，5没有众数；在高斯分布中，众数位于峰值。</p><h1 id="4、分位数-Quantile"><a href="#4、分位数-Quantile" class="headerlink" title="4、分位数(Quantile )"></a>4、分位数(Quantile )</h1><p>分位数就是用概率作为依据将一批数据分开的那个点，是为了便于研究区间估计而挑选的对应一定概率的随机变量取值。</p><p>一天，班主任气冲冲地走进教室对我们说：“太不像话了，这次考试竟然有60%的同学不及格！”　班主任这句话里就有一个分位数的应用。</p><h1 id="5、中位数-Median"><a href="#5、中位数-Median" class="headerlink" title="5、中位数 (Median)"></a>5、中位数 (Median)</h1><p>中位数，又称中点数，中值。中位数是按顺序排列的一组数据中居于中间位置的数，即在这组数据中，有一半的数据比他大，有一半的数据比他小。</p><p>有一组数据：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/dea0cf5c42e24f5288d7f68d40c2958b?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>将它按从小到大的顺序排序为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5da57b80c77e43f982c829e1c59f0014?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>则当N为奇数时，</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/8d10bdf8d0184e1cb51764037ee04249?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>当N为偶数时，</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/95e295060b6d4df196013ab146cba618?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="6、偏度-Skewness"><a href="#6、偏度-Skewness" class="headerlink" title="6、偏度(Skewness)"></a>6、偏度(Skewness)</h1><p>对于离散随机变量：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b8046dd6839a490cbd3243d7e98172c8?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>一般化公式：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/fdda9b6567874e32a8d1c39c66f1c12f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中µ为均值或期望，σ为标准差。</p><p>可以用来度量随机变量概率分布的不对称性。是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。偏度亦称偏态、偏态系数。它表征概率分布密度曲线相对于平均值不对称程度的特征数。直观看来就是密度函数曲线尾部的相对长度。</p><p>例如：一组数据为1、2、2、4、1，均值为2，标准差约为1.22，所以偏度为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5f3eb65018fc40e297acf632adb7e83c?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>几何意义：</p><p>偏度的取值范围为(-∞,+∞)</p><ul><li>当偏度&lt;0时，概率分布图左偏。</li><li>当偏度&#x3D;0时，表示数据相对均匀的分布在平均值两侧，不一定是绝对的对称分布。</li><li>当偏度&gt;0时，概率分布图右偏。</li></ul><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/129605bc5ea74bb5ad97e8fab17ecbf0?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="7、峰度"><a href="#7、峰度" class="headerlink" title="7、峰度"></a>7、峰度</h1><p>峰度被定义为四阶累积量除以二阶累积量的平方，它等于四阶中心矩除以概率分布方差的平方再减去3：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/18b1afdbad614d8faf83eb8c729fb026?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>“减3”是为了让正态分布的峰度为0。</p><p>对于具有n个值的样本，样本峰度为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/bb0a2fc8d42a44138478cd441072fc84?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>在统计学中，峰度衡量实数随机变量概率分布的峰态。峰度高就意味着方差增大是由低频度的大于或小于平均值的极端差值引起的。</p><p>根据变量值的集中与分散程度，峰度一般可表现为三种形态：尖顶峰度、平顶峰度和标准峰度。当变量值的次数在众数周围分布比较集中，使次数分布曲线比正态分布曲线顶峰更为隆起尖峭，称为尖顶峰度；当变量值的次数在众数周围分布较为分散，使次数分布曲线较正态分布曲线更为平缓，称为平顶峰度。可见，尖顶峰度或平顶峰度都是相对正态分布曲线的标准峰度而言的。</p><h1 id="8、信息熵-Entropy"><a href="#8、信息熵-Entropy" class="headerlink" title="8、信息熵(Entropy)"></a>8、信息熵(Entropy)</h1><p>熵在信息论中代表随机变量不确定度的度量。一个离散型随机变量X的熵H(X)定义为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/8ea91d0f530b47788618f6203f7a33f8?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中p(xi)代表随机事件X为xi的概率。</p><p>令f(x) &#x3D; log(1&#x2F;p(x)), E[f(x)] &#x3D; ∑p(x)f(x). H(x)可以看作f(x)的数学期望。</p><p>信息论之父克劳德·香农，总结出了信息熵的三条性质：</p><ul><li>单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。f(x) &#x3D; 1&#x2F;p(x)是不是很容易想得到？</li><li>非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。</li><li>累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。</li></ul><p>我们希望信息熵满足：</p><p>所有可能发生事件所带来的信息量的期望：E[f(x)]；</p><p>概率越高，信息熵越低：f(x) &#x3D; log(1&#x2F;p(x))；</p><p>非负性：f(x) &#x3D; log(1&#x2F;p(x))；</p><p>如果两个事件A，B同时发生，且相互独立，有H(A,B) &#x3D; H(A)+H(B)。</p><p>因此，构造了我们现在的信息熵公式。</p><p>香农从数学上，严格证明了满足上述几个个条件的随机变量不确定性度量函数具有唯一形式：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/9d8f4b00921645f1936b95c402e73d9c?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中的C为常数，我们将其归一化为C&#x3D;1， 即得到了信息熵公式。</p><p>如果两个事件不相互独立，那么满足：</p><p>H(A,B) &#x3D; H(A) + H(B) - I(A, B),</p><p>其中I(A, B)是互信息,代表一个随机变量包含另一个随机变量信息量的度量，这个概念在通信中用处很大。比如一个点到点通信系统中，发送端信号为X，通过信道后，接收端接收到的信号为Y，那么信息通过信道传递的信息量就是互信息I(X,Y)。根据这个概念，香农推出了一个十分伟大的公式，香农公式，给出了临界通信传输速率的值，即信道容量：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0de11e01d0d34752befcd537aee90af6?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="9、特征函数-Characteristic-Function-CF"><a href="#9、特征函数-Characteristic-Function-CF" class="headerlink" title="9、特征函数 (Characteristic Function, CF)"></a>9、特征函数 (Characteristic Function, CF)</h1><p>如果F(x)是累积分布函数，那么特征函数由黎曼-斯蒂尔切斯积分给出：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/500bf8ad52a14f0cbb739d539ca40c3a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>如果f(x)是概率密度函数，则特征函数：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0666f1fc343641e7835e283d87dd1e42?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>它被用于中心极限定理的最常见的证明中，还可以用来求出某个随机变量的矩。</p><h1 id="10、动差生成函数-Moment-Generating-Function-MGF"><a href="#10、动差生成函数-Moment-Generating-Function-MGF" class="headerlink" title="10、动差生成函数(Moment Generating Function, MGF)"></a>10、动差生成函数(Moment Generating Function, MGF)</h1><p>在统计学中，矩又被称为动差(Moment)。矩量母函数(Moment Generating Function,简称mgf)又被称为动差生成函数。定义为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e4a8af5112814ce7842831aea5fb1d01?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>前提是这个期望值存在。</p><p>如果X具有连续概率密度函数f(x)，则它的动差生成函数由下式给出：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/376b6aeb8cd74996a020a306eb6dfcca?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中，mi是第i个矩。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/eabbb1976db0471b803eaa17d7dd06e0?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>如果两个随机变量具有相同的mgf，那么它们具有相同的概率分布。</p><p>动差生成函数可用来求n阶原点矩。</p><p>只要动差生成函数在t &#x3D; 0周围的开区间存在，第n阶原点矩为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ec42df4110a641e8b716f30026e10926?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="11、误差函数-Error-function-Erf"><a href="#11、误差函数-Error-function-Erf" class="headerlink" title="11、误差函数 (Error function, Erf)"></a>11、误差函数 (Error function, Erf)</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7ab78e312760431d8b48f64baf0447f4?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>误差函数的级数展开式为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/6a502f87525c456b87932911d27beaec?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c1915df025514921ad19b7eaab7450a9?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>误差函数是特殊的不完全伽玛函数之一。</p><p>误差函数在概率论、统计学以及偏微分方程和半导体物理中都有广泛的应用。</p><h1 id="二、正态分布（Normal-Distribution）"><a href="#二、正态分布（Normal-Distribution）" class="headerlink" title="二、正态分布（Normal Distribution）"></a>二、正态分布（Normal Distribution）</h1><h1 id="（1）概率密度函数（PDF）"><a href="#（1）概率密度函数（PDF）" class="headerlink" title="（1）概率密度函数（PDF）"></a>（1）概率密度函数（PDF）</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2a42aa4dbdd7487494471a46b64d251d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>以上结果可表示为 X~N(µ, σ2), µ为均值或期望，σ为标准差。</p><p>标准正态分布（standard normal distribution）表示为N(0,1)</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f62260c0b6334b6c93e73fa9454ffc89?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/234c00e0f8354521ac8d534975ea99c1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-CDF"><a href="#（2）累积分布函数-CDF" class="headerlink" title="（2）累积分布函数 (CDF)"></a>（2）累积分布函数 (CDF)</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c603e0ad406041bea30575698f0a74de?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/bc6184a4bab84a299127052481709f1e?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）正态分布的性质"><a href="#（3）正态分布的性质" class="headerlink" title="（3）正态分布的性质"></a>（3）正态分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/fefe8e89dab14349bdb147c9696e6839?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结"><a href="#（4）小结" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>卡方分布、t-分布、F分布统计学三大分布皆来自正态分布。</p><p>正态分布还有诸多迷人的数学特性，我们可以欣赏一下：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c1f5d284327b4f228ff471a634d53358?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="三、泊松分布-Poisson-Distribution"><a href="#三、泊松分布-Poisson-Distribution" class="headerlink" title="三、泊松分布(Poisson Distribution)"></a>三、泊松分布(Poisson Distribution)</h1><p>泊松分布，是一种统计与概率学里常见到的离散概率分布，由法国数学家西莫恩·德尼·泊松（Siméon-Denis Poisson）在1838年时发表。</p><p>泊松分布是用来描述某段时间内事件的发生概率。</p><h1 id="（1）概率密度函数"><a href="#（1）概率密度函数" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/9eab971935bb4e87a5e1963352871366?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/248b4a233c2c4393a9d9e9a08d4c4128?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数"><a href="#（2）累积分布函数" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0636288c24dd40bebd5003d22273ee58?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）泊松分布的性质"><a href="#（3）泊松分布的性质" class="headerlink" title="（3）泊松分布的性质"></a>（3）泊松分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ad9a5caec3164984b172bd6685f70888?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-1"><a href="#（4）小结-1" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>泊松分布通常用于两个主要目的：</p><ul><li>预测事件在选定时间段内将发生多少次。 该技术可用于不同的风险分析应用，例如房屋保险价格估计。</li><li>考虑到事件过去发生的频率，估计事件发生的可能性(例如，未来两个月停电的可能性有多大)。</li></ul><p>许多随机变量服从泊松分布，如某医院一天内的就诊人数，一段时间内一个网站的登陆人数，某路口一段时间内通过的车辆数。</p><h1 id="四、伯努利分布-Bernoulli-Distribution"><a href="#四、伯努利分布-Bernoulli-Distribution" class="headerlink" title="四、伯努利分布(Bernoulli Distribution)"></a>四、伯努利分布(Bernoulli Distribution)</h1><p>伯努利分布是一个离散型机率分布，是N&#x3D;1时二项分布的特殊情况，为纪念瑞士科学家詹姆斯·伯努利(Jacob Bernoulli 或James Bernoulli)而命名。</p><h1 id="（1）概率密度函数-1"><a href="#（1）概率密度函数-1" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/df999238bf28424c84d12da2ea1b114f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a9005372a5ec45568ce325461a024c2e?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-1"><a href="#（2）累积分布函数-1" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><h1 id="（3）伯努利分布的性质"><a href="#（3）伯努利分布的性质" class="headerlink" title="（3）伯努利分布的性质"></a>（3）伯努利分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e5b0e7b5c87843a8a663d882102ff335?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="五、二项分布-Binomial-Distribution"><a href="#五、二项分布-Binomial-Distribution" class="headerlink" title="五、二项分布(Binomial Distribution)"></a>五、二项分布(Binomial Distribution)</h1><h1 id="（1）概率密度函数-2"><a href="#（1）概率密度函数-2" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e4e93ede58b34fbf881b48e5047a70a5?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1765263e3dee4b07b49328d82203e35b?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-2"><a href="#（2）累积分布函数-2" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/70e334f6432646418cee983dc8effa1b?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>或</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/6308b4a8e2854a739bd65492ba92a26f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中，I是正则不完全的beta函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7a6a1ff320a0400bb525ea671dc5082f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）二项分布的性质"><a href="#（3）二项分布的性质" class="headerlink" title="（3）二项分布的性质"></a>（3）二项分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/473eecb926844bfab759eea29e3344f4?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="六、几何分布-Geometric-Distribution"><a href="#六、几何分布-Geometric-Distribution" class="headerlink" title="六、几何分布(Geometric Distribution)"></a>六、几何分布(Geometric Distribution)</h1><h1 id="（1）概率密度函数-3"><a href="#（1）概率密度函数-3" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4108a6302d8f498e9792a5f49057ea84?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/81ddcea005ef4916aa17195cab8ab1b4?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-3"><a href="#（2）累积分布函数-3" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/30ed409231a448089de8099038681943?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）几何分布的性质"><a href="#（3）几何分布的性质" class="headerlink" title="（3）几何分布的性质"></a>（3）几何分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0ddcc2611f44479595f9b3535c726471?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-2"><a href="#（4）小结-2" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>考虑一系列试验，其中每个试验只有两个可能的结果（指定的失败和成功）。假定每个试验的成功概率是相同的。在这样的试验序列中，几何分布可用于对首次成功之前的失败数量进行建模。分布给出了以下可能性：首次成功之前有0个失败，第一次成功之前有1个失败，第一次成功之前有2个失败，依此类推。</p><p>什么时候几何分布是合适的模型？如果以下假设成立，则几何分布是合适的模型。</p><p>一系列独立试验：</p><p>每次试验只有两种可能的结果，通常指定为成功或失败。</p><p>每个试验的成功概率p是相同的。</p><p>如果满足这些条件，则几何随机变量Y是首次成功之前失败次数的计数。</p><h1 id="七、超几何分布-Hypergeometric-Distribution"><a href="#七、超几何分布-Hypergeometric-Distribution" class="headerlink" title="七、超几何分布(Hypergeometric Distribution)"></a>七、超几何分布(Hypergeometric Distribution)</h1><h1 id="（1）概率密度函数-4"><a href="#（1）概率密度函数-4" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/43c32657f18348049ef906d96f35c46a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e8c78ec6003847faaf4b271edb6c6936?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-4"><a href="#（2）累积分布函数-4" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5df2f56d2a3643f7a3b46a83ea1d4a58?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）超几何分布性质"><a href="#（3）超几何分布性质" class="headerlink" title="（3）超几何分布性质"></a>（3）超几何分布性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7f3324eb95f342f0bd0c3386cbecf6b4?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="八、卡方分布-Chi-squared-χ2）"><a href="#八、卡方分布-Chi-squared-χ2）" class="headerlink" title="八、卡方分布(Chi-squared ( χ2）)"></a>八、卡方分布(Chi-squared ( χ2）)</h1><p>卡方分布是由阿贝(Abbe)于1863年首先提出的，后来由海尔墨特(Hermert)和现代统计学的奠基人之一的卡·皮尔逊(C K．Pearson)分别于1875年和1900年推导出来，是统计学中的一个非常有用的分布。</p><p>如果 Z1, Z2 …, Zn 是相互独立的随机变量，且都服从于 N(0,1)分布，那么随机变量X</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/722039d2271348919de01afca217c9bc?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>服从自由度（degree of freedom, df）为 k 的χ2分布，记为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/367348acf7564c2b973e6297a695ffa6?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（1）概率密度函数PDF-of-χ2"><a href="#（1）概率密度函数PDF-of-χ2" class="headerlink" title="（1）概率密度函数PDF of χ2"></a>（1）概率密度函数PDF of χ2</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/438d2bb450a04e7f8074ef1665c618d5?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>这里Γ(x)代表Gamma函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e4ae5042bc2c4e7c80f2e136e32ee9de?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数CDF-of-χ2"><a href="#（2）累积分布函数CDF-of-χ2" class="headerlink" title="（2）累积分布函数CDF of χ2"></a>（2）累积分布函数CDF of χ2</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/be9731b47fdf47a9af1f9ff9b4cbeb3c?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>这里γ(v,z)为不完全Gamma函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/93d9127b7f084675ac745435b15280f1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）卡方分布的性质"><a href="#（3）卡方分布的性质" class="headerlink" title="（3）卡方分布的性质"></a>（3）卡方分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b7cc1e9067d34f4a98c93c3bc70aed4b?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-3"><a href="#（4）小结-3" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>卡方分布特点：</p><ul><li>是伽玛分布的一个特例；</li><li>是统计学三大分布之一，是数理统计必须的知识点；</li><li>于检验样本是否偏离了期望；</li><li>可以用来测试随机变量之间是否相互独立，也可用来检测统计模型是否符合要求；</li><li>在分布曲线和数据拟合优度检验中是一个利器，皮尔逊的这个工作被认为是假设检验的开山之作；</li></ul><h1 id="九、t-分布-Student’s-t-distribution"><a href="#九、t-分布-Student’s-t-distribution" class="headerlink" title="九、t-分布(Student’s t-distribution)"></a>九、t-分布(Student’s t-distribution)</h1><p>戈赛特(W.S.Gosset)发现了t-分布。他依靠自己的化学知识进酿酒厂工作，工作期间考虑酿酒配方实验中的统计学问题，酒厂虽然禁止员工发表一切与酿酒研究有关的成果，但允许他在不提到酿酒的前提下，以笔名发表t分布的发现，所以论文使用了“学生”（Student）这一笔名。之后的相关理论经由罗纳德·费雪（Sir Ronald Aylmer Fisher）发扬光大，为了感谢戈塞特的功劳，费雪将此分布命名为学生t分布（Student’s t）。</p><p>戈赛特追随皮尔逊学习了一年的统计学，最终依靠自己的数学知识发现t-分布而名垂青史。1908年，戈塞特提出了正态样本中样本均值和标准差比值的分布，并给出了应用上极其重要的第一个分布表。戈塞特在t-分布的工作开创了小样本统计学的先河。</p><p>设X ~ N(0,1)和Y ~ χ2(n) ，且 X 和 Y 相互独立，则称随机变量</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1f37c726aa6d480db6172ec977764382?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>服从 自由度df 为 n 的 t-分布，记为 T ~ t(n)。</p><h1 id="（1）PDF-of-t-distribution"><a href="#（1）PDF-of-t-distribution" class="headerlink" title="（1）PDF of t-distribution"></a>（1）PDF of t-distribution</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/51e2ea9287f249f0b2226023570977e9?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中, υ为自由度数，Γ为伽玛函数。</p><p>或</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b389f372f6bb431c8dd15540fdc94736?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中 υ为自由度数，B为Beta函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/96604fdb7467437d91fcc9292e4549ee?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）CDF-of-t-distribution"><a href="#（2）CDF-of-t-distribution" class="headerlink" title="（2）CDF of t-distribution"></a>（2）CDF of t-distribution</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0ce488a340b3499f85a3e878c49246a5?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/00ddd5e102aa4aeea47a13cf60af672b?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）t-分布的性质"><a href="#（3）t-分布的性质" class="headerlink" title="（3）t-分布的性质"></a>（3）t-分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/82570a4b32e5459a891e70baa121390e?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-4"><a href="#（4）小结-4" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>t-分布特点：</p><ul><li>以0为中心，左右对称的单峰分布；</li><li>是一簇曲线，其形态变化与n（确切地说与自由度ν）大小有关。自由度ν越小，t分布曲线越低平；自由度ν越大，t分布曲线越接近标准正态分布曲线；</li><li>是统计学三大分布之一，是数理统计必须的知识点；</li><li>用于检验均值是否不同；</li><li>在概率统计中，在置信区间估计、显著性检验等问题的计算中发挥重要作用。</li></ul><h1 id="十、F-分布-F-distribution"><a href="#十、F-分布-F-distribution" class="headerlink" title="十、F-分布(F-distribution)"></a>十、F-分布(F-distribution)</h1><p>费希尔(R.A.Fisher), F分布就是为了纪念费希尔而用他的名字首字母命名的。费希尔统计造诣极高，受高斯的启发，系统地创立了极大似然估计法，这套理论现在在统计学参数估计中用处最广。</p><p>X 和 Y 是相互独立的χ2分布随机变量，df 分别为 d1 和 d2，则称随机变量</p><p>F &#x3D;（X&#x2F;d1 &#x2F;（Y&#x2F;d2)</p><p>服从df 为 (d1, d2)的 F-分布，且通常写为 F～F（d1,d2）。</p><h1 id="（1）PDF-of-F-distribution"><a href="#（1）PDF-of-F-distribution" class="headerlink" title="（1）PDF of F distribution"></a>（1）PDF of F distribution</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/bf884c9e25bc4fe3921dbcf2610b59cb?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>实数x&gt;0, B是Beta函数,d1和d2是正整数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/81935840226b4f04a8d0567c860945cf?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）CDF-of-F-distribution"><a href="#（2）CDF-of-F-distribution" class="headerlink" title="（2）CDF of F distribution"></a>（2）CDF of F distribution</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/78f72f2d703e4b72a3f70cd438a918d1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5082441e2daf4b81a3c0c86deeff781d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）F-分布的性质"><a href="#（3）F-分布的性质" class="headerlink" title="（3）F-分布的性质"></a>（3）F-分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/dce997c9490e4e5fa91d40c2e69ce24d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-5"><a href="#（4）小结-5" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>F分布的特点：</p><ul><li>是统计学三大分布之一，是数理统计必须的知识点。</li><li>用于检验方差是否不同。</li></ul><p>卡方分布、t-分布、F分布三大分布皆来自正态分布。</p><p>假设独立随机变量</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b1f7e1d0bd8a4fa39f6f20cb0fe7e43a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>则满足三大分布的随机变量可以构造如下：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a7f2c6fc48e74366af012078cdbbbfdb?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>有了统计学三大分布的加持，正态分布在数理统计学独领风骚。</p><h1 id="十一、指数分布-Exponential-Distribution"><a href="#十一、指数分布-Exponential-Distribution" class="headerlink" title="十一、指数分布(Exponential Distribution)"></a>十一、指数分布(Exponential Distribution)</h1><p>推导公式见附录1</p><h1 id="（1）概率密度函数-5"><a href="#（1）概率密度函数-5" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b524cfcd2f0f407f8cccca5f07283fa2?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中λ&gt;0为常数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2290a64b849c4f4dbbc251d3e60ed903?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-5"><a href="#（2）累积分布函数-5" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ef1a19841b1e4feaad67c25f3b6cd9bc?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5cbb742e7a804e1abbeed5160b211bf9?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）指数分布的性质"><a href="#（3）指数分布的性质" class="headerlink" title="（3）指数分布的性质"></a>（3）指数分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f2b044f7ec124233b45a12a2363a2819?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-6"><a href="#（4）小结-6" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>指数分布是伽玛分布和威布尔分布的特殊情况，可以看作是威布尔分布中的形状系数等于1的特殊分布。</p><p>指数函数的一个重要特征是无记忆性（Memoryless Property，又称遗失记忆性）。这表示如果一个随机变量呈指数分布，当s,t≥0时有P(T&gt;s+t|T&gt;t)&#x3D;P(T&gt;s)。所谓缺乏“记忆”，是指某种产品或零件经过一段时间t0的工作后,仍然如同新的产品一样,不影响以后的工作寿命值，或者说，经过一段时间t0的工作之后，该产品的寿命分布与原来还未工作时的寿命分布相同，显然，指数分布的这种特性，与机械零件的疲劳、磨损、腐蚀、蠕变等损伤过程的实际情况是完全矛盾的，它违背了产品损伤累积和老化这一过程。所以，指数分布不能作为机械零件功能参数的分布形式，因而限制了它在机械可靠性研究中的应用。</p><p>指数分布在电子元器件的可靠性研究中，通常用于描述对发生的缺陷数或系统故障数的测量结果。这种分布表现为均值越小，分布偏斜得越厉害。在日本的工业标准和美国军用标准中，半导体器件的抽验方案都是采用指数分布。</p><h1 id="十二、瑞利分布-Rayleigh-Distribution"><a href="#十二、瑞利分布-Rayleigh-Distribution" class="headerlink" title="十二、瑞利分布(Rayleigh Distribution)"></a>十二、瑞利分布(Rayleigh Distribution)</h1><p>瑞利分布（Rayleigh Distribution）:当一个随机二维向量的两个分量呈独立的、均值为0，有着相同的方差的正态分布时，这个向量的模呈瑞利分布。例如，当随机复数的实部和虚部独立同分布于0均值，同方差的正态分布时，该复数的绝对值服从瑞利分布。</p><p>瑞利分布是最常见的用于描述平坦衰落信号接收包络或独立多径分量接受包络统计时变特性的一种分布类型。两个正交高斯噪声信号之和的包络服从瑞利分布。</p><p>瑞利分布是威布尔分布的一种特殊情况，也就是说，当威布尔分布的形状参数为2时，威布尔分布就是瑞利分布。</p><p>推导公式见附录2</p><h1 id="（1）概率密度函数-6"><a href="#（1）概率密度函数-6" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p>如果连续型随机变量X的概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e7dfeb7ca15c49b68ba656dd5ec1adba?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中σ为尺度参数，且σ&gt;0, 则称随机变量X服从瑞利分布。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/776ded72a50e4faaa08948c0fa30b6c2?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-6"><a href="#（2）累积分布函数-6" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b7ad2c8a9ff242d8b9ecd347375b090a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b4c1c4196ef640c18dc303a33115e1a6?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）瑞利分布的性质"><a href="#（3）瑞利分布的性质" class="headerlink" title="（3）瑞利分布的性质"></a>（3）瑞利分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1a5b3f66f9f04d8aa29ef24a55b49c08?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-7"><a href="#（4）小结-7" class="headerlink" title="（4）小结"></a>（4）小结</h1><h1 id="瑞利分布在风资源评估中的应用"><a href="#瑞利分布在风资源评估中的应用" class="headerlink" title="瑞利分布在风资源评估中的应用"></a>瑞利分布在风资源评估中的应用</h1><p>风资源评估，风场选择，单位选择和容量规划应基于风资源评估的结果。瑞利分布和威布尔分布是风资源评估中使用最广泛的两个分布函数。</p><h1 id="瑞利分布在无线通信方面的应用"><a href="#瑞利分布在无线通信方面的应用" class="headerlink" title="瑞利分布在无线通信方面的应用"></a>瑞利分布在无线通信方面的应用</h1><p>在无线移动通信系统中，瑞利衰落分布常用于平坦衰落信号或独立多径分量接收中包络具备时变统计特性的衰落模型。瑞利衰落属于小尺度的衰落效应，小尺度衰落信道包络一般服从瑞利分布或莱斯分布。</p><p>瑞利衰落信道（Rayleigh fading channel）是一种无线电信号传播环境的统计模型。这种模型假设信号通过无线信道之后，其信号幅度是随机的，即“衰落”，并且其包络服从瑞利分布。这一信道模型能够描述由电离层和对流层反射的短波信道，以及建筑物密集的城市环境。瑞利衰落只适用于从发射机到接收机不存在直射信号的情况，否则应使用莱斯衰落信道作为信道模型。</p><h1 id="基于瑞利分布的粒子滤波跟踪算法"><a href="#基于瑞利分布的粒子滤波跟踪算法" class="headerlink" title="基于瑞利分布的粒子滤波跟踪算法"></a>基于瑞利分布的粒子滤波跟踪算法</h1><p>瑞利分布（RD）在许多现实生活中都有广泛的应用，特别是寿命测试，机械构件的载荷能力方面，可靠性分析，药品等。</p><h1 id="十三、韦伯分布（Weibull-Distribution）"><a href="#十三、韦伯分布（Weibull-Distribution）" class="headerlink" title="十三、韦伯分布（Weibull Distribution）"></a>十三、韦伯分布（Weibull Distribution）</h1><p>韦伯分布，又称威布尔分布，是可靠性分析和寿命检验的理论基础。历史来源：</p><ul><li>1927年，Fréchet首先给出这一分布的定义；</li><li>1933年，Rosin和Rammler在研究碎末的分布时，第一次应用了该分布（Rosin, P.; Rammler, E. (1933), “The Laws Governing the Fineness of Powdered Coal”, Journal of the Institute of Fuel 7: 29 - 36.）；</li><li>1951年，瑞典工程师、数学家Waloddi Weibull详细解释了这一分布，于是，该分布便以他的名字命名为Weibull Distribution。</li></ul><h1 id="（1）韦伯分布是连续性的概率分布，其概率密度函数为："><a href="#（1）韦伯分布是连续性的概率分布，其概率密度函数为：" class="headerlink" title="（1）韦伯分布是连续性的概率分布，其概率密度函数为："></a>（1）韦伯分布是连续性的概率分布，其概率密度函数为：</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7b0d957845014ef7b296f10cd4445b87?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中，x是随机变量，λ&gt;0是比例参数（scale parameter），k&gt;0是形状参数（shape parameter）。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/66a4909f03e648a592f6d4d1315f7bdd?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-7"><a href="#（2）累积分布函数-7" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a8165a24dd3a417288e2e2e995f29b0a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/be3d19ed95174a80a50ef47a922c3e30?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）韦伯分布的性质"><a href="#（3）韦伯分布的性质" class="headerlink" title="（3）韦伯分布的性质"></a>（3）韦伯分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/8a1282c292f145e29f3baa99e0da074d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-8"><a href="#（4）小结-8" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>威布尔分析最主要的优点：</p><ul><li>提供比较准确的失效分析和小数据样本的失效预测，对出现的问题尽早地制订解决方案。</li><li>为单个失效模式提供简单而有用的图表，使数据在不充足时，仍易于理解。</li><li>描述分布状态的形状可很好地选择相应的分布。</li><li>提供基于威布尔概率图的斜率的物理失效的线索。</li></ul><p>在所有可用的可靠性计算的分布当中，威布尔分布是唯一可用于工程领域的。威布尔分析广泛用于研究机械、化工、电气、电子、材料的失效，甚至人体疫病。具体应用如下：</p><ul><li>工业制造</li><li>研究生产过程和运输时间关系。</li><li>极值理论</li><li>预测天气</li><li>可靠性和失效分析</li><li>雷达系统</li><li>对接受到的杂波信号的依分布建模。</li><li>拟合度</li><li>无线通信技术中，相对指数衰减频道模型，韦伯衰减模型对衰减频道建模有较好的拟合度。</li><li>量化寿险模型的重复索赔</li><li>预测技术变革</li><li>风速</li><li>由于曲线形状与现实状况很匹配，被用来描述风速的分布。</li></ul><h1 id="十四、帕雷托分布-Pareto-Distribution"><a href="#十四、帕雷托分布-Pareto-Distribution" class="headerlink" title="十四、帕雷托分布(Pareto Distribution)"></a>十四、帕雷托分布(Pareto Distribution)</h1><p>帕雷托分布， 也称帕累托分布。以意大利经济学家 Vilfredo Pareto命名，他在1882年研究英国的财富分配情况时，发现前20%的人群拥有着社会80%的财富，这一现象可以用一个简单的概率分布函数来描述，即帕雷托分布。帕累托分布是从大量真实世界的现象中发现的幂次定律分布，这个分布在经济学以外， 也被称为布拉德福分布。</p><h1 id="（1）概率密度函数-7"><a href="#（1）概率密度函数-7" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a7b239df68b34b00b53d6e38105f2a07?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cc27741af4cd4880ab7bf860c1e21d76?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积概率分布"><a href="#（2）累积概率分布" class="headerlink" title="（2）累积概率分布"></a>（2）累积概率分布</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1f29093ca636419483fa95e036f982ab?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/843d0a0d59034794b5fbba1a6d551bde?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）帕雷托分布的性质"><a href="#（3）帕雷托分布的性质" class="headerlink" title="（3）帕雷托分布的性质"></a>（3）帕雷托分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/47b15813624045b1b62f50e02fc79b58?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-9"><a href="#（4）小结-9" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>帕累托分布被广泛应用于经济学研究中。被认为大致是帕累托分布的例子有：</p><ul><li>财富在个人之间的分布；</li><li>沙粒的大小；</li><li>人类住区的规模(更少的城市和更多的村庄)；</li><li>接近绝对零度时，玻色-爱因斯坦凝聚的团簇；</li><li>在互联网流量中文件尺寸的分布；</li><li>油田储量价值(大油田少，小油田多)；</li><li>龙卷风带来的灾难数量；</li><li>对维基百科条目的访问。</li></ul><h1 id="十五、拉普拉斯分布-Laplace-Distribution"><a href="#十五、拉普拉斯分布-Laplace-Distribution" class="headerlink" title="十五、拉普拉斯分布(Laplace Distribution)"></a>十五、拉普拉斯分布(Laplace Distribution)</h1><p>拉普拉斯分布是由两个指数函数组成的，所以又叫做双指数函数分布（double exponential distribution）</p><h1 id="（1）概率密度函数-8"><a href="#（1）概率密度函数-8" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p>设随机变量X具有密度函数</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e2dd0ca117794ab6baf1a5cb14abc464?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中尺度参数b，位置参数μ为常数，且b&gt;0，则称X服从参数为b，μ的拉普拉斯分布。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e15f31f4ed3149c4941b70386c6e1c1f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>服从拉普拉斯分布的随机变量，出现极端大的值的概率，要远远大于正态分布。</p><h1 id="（2）累积分布函数-8"><a href="#（2）累积分布函数-8" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/008bf97e96204aa3afa68dd7b88ae875?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/59971ce1d6c3430a9559b49c54feebec?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）拉普拉斯分布"><a href="#（3）拉普拉斯分布" class="headerlink" title="（3）拉普拉斯分布"></a>（3）拉普拉斯分布</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/44b10bc9c5da43deb31f0bebcbb2470f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（4）小结-10"><a href="#（4）小结-10" class="headerlink" title="（4）小结"></a>（4）小结</h1><p>拉普拉斯分布是比较常见的概率统计模型,它在生活中很多专业领域都有广泛的应用。如拉普拉斯分布在证券金融经济领域有着重要的作用。在工程中,对于测绘数据的处理以及在语音和图像数据等领域上也有着广泛的应用。</p><p>过去人们研究证券市场收益率的分布时大都假设其服从正态分布,后来发现此假设的缺点越来越明显——无法满足尖峰厚尾非正态的分布特性,而且根本就不能通过统计检验.出于上述考虑,引入拉普拉斯分布,比较成功地解决了这些问题,并且发现该类分布在证券市场上具有很大的应用前景。</p><p>拟合优度检验是统计学应用中的一个常见的问题,即检验来自总体中数据其分布是否与某种理论分布相一致的统计方法。一般情况下,先根据问题的实际情况、观测的样本数据以及抽样方法,猜测数据可能的理论分布,然后利用样本数据进一步检验这个猜测的模型。</p><h1 id="十六、逻辑斯蒂分布-Logistic-Distribution"><a href="#十六、逻辑斯蒂分布-Logistic-Distribution" class="headerlink" title="十六、逻辑斯蒂分布(Logistic Distribution)"></a>十六、逻辑斯蒂分布(Logistic Distribution)</h1><h1 id="（1）概率密度函数-9"><a href="#（1）概率密度函数-9" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/93c913689ee44467b98764262eb20585?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中μ为均值参数，s&gt;0为尺度参数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/93bdfddeab1749f684f43f77cc170103?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-9"><a href="#（2）累积分布函数-9" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/c249c0a8941a4946a2943d5bd0086430?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中μ为均值参数，s&gt;0为尺度参数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/9ab291faba6f4a7faabfdedb5c2cd349?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）逻辑斯蒂分布"><a href="#（3）逻辑斯蒂分布" class="headerlink" title="（3）逻辑斯蒂分布"></a>（3）逻辑斯蒂分布</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/493b20957f83447e83aad4aadf4085ec?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="十七、柯西分布-Cauchy-Distribution"><a href="#十七、柯西分布-Cauchy-Distribution" class="headerlink" title="十七、柯西分布(Cauchy Distribution)"></a>十七、柯西分布(Cauchy Distribution)</h1><p>柯西分布也叫作柯西一洛伦兹分布，它是以奥古斯丁-路易-柯西与亨德里克-洛伦兹名字命名的连续概率分布。</p><p>柯西分布是一个数学期望不存在的连续型概率分布。</p><h1 id="（1）概率密度函数-10"><a href="#（1）概率密度函数-10" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2e8ca88813f341629a59a057f2ba725f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中，x0定义分布峰值位置的位置参数；γ为最大值一半处的一半宽度的尺度参数。</p><p>如果x0&#x3D;0, γ&#x3D;1, 则为标准柯西分布：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1b00d10cfc0443e3b6df9dc1e78570d1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f1f4a5311ab141dc92e2c2e70778f614?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-10"><a href="#（2）累积分布函数-10" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/9adc9ba2365747e28b574a3538d85339?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/71719e0fb04a44f6892b03b881adfc96?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）柯西分布性质"><a href="#（3）柯西分布性质" class="headerlink" title="（3）柯西分布性质"></a>（3）柯西分布性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/35feb21a40e44b839a922684cdb221d4?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="十八、贝塔分布-Beta-Distribution"><a href="#十八、贝塔分布-Beta-Distribution" class="headerlink" title="十八、贝塔分布(Beta Distribution)"></a>十八、贝塔分布(Beta Distribution)</h1><p>贝塔分布（Beta Distribution) 是一个作为伯努利分布和二项式分布的共轭先验分布的概率函数，在机器学习和数理统计学中有重要应用。在概率论中，贝塔分布，也称Βeta分布，是指一组定义在(0,1) 区间的连续概率分布。</p><p>推导公式见附录3</p><h1 id="（1）概率密度函数-11"><a href="#（1）概率密度函数-11" class="headerlink" title="（1）概率密度函数"></a>（1）概率密度函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1231f93eb6e941c2a82111ab112b41af?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中Γ(x)代表Gamma函数，参数α&gt;0, β&gt;0， B(α，β)是Beta函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/d33bbf7c882641729bfa6cc6d0ca3be1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（2）累积分布函数-11"><a href="#（2）累积分布函数-11" class="headerlink" title="（2）累积分布函数"></a>（2）累积分布函数</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cfdcea3f6096434692d4765425f5be91?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/24b3a944795540b093afb792584e9fab?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中B(x;a,b)是不完全Beta函数。Ix(α,β)是正则不完全贝塔函数。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/5c0748edc87548639d024e33ad746de7?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="（3）Beta分布的性质"><a href="#（3）Beta分布的性质" class="headerlink" title="（3）Beta分布的性质"></a>（3）Beta分布的性质</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1a9e7ac1448142fcabbff258304e6272?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><h1 id="十九、附录"><a href="#十九、附录" class="headerlink" title="十九、附录"></a>十九、附录</h1><h1 id="附录1-指数分布的推导"><a href="#附录1-指数分布的推导" class="headerlink" title="附录1 指数分布的推导"></a>附录1 指数分布的推导</h1><p>设某电子元件使用时间超过x的概率为P(X&gt;x)。元件在x和x+h时间段内失效的概率为 P(x≤X≤x+h)。假设元件已经使用了 x时长了， 那么，元件在x和x+h时间段内失效应等于条件概率：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7a56ddcff0c742f0bf28128cbfe0a235?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>用上面的条件概率除以h, 也就得到了h时段的平均失效率：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7891ac0e7eb043848f841ca6cad05ca1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>当h—&gt;0时，上式当元件已经使用了X时长，元件在下一时刻的失效概率恒为λ</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f3284bb0f9ab429a80c6c129e6267f85?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>设B事件为 X&gt;x, A事件为x≤X≤x+h,有条件概率公式：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/49193f35fa864b3c861d4d4720ea331f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>则有</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2093be5d5c1040be88e1af673558c00f?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>设F(x) &#x3D; P(X≤x), 有</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cc7325f22dc4434a935ef4ee4defe459?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>得到微分方程：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2f2aa626ca7d4194bcc9b84ac294a59d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>可以转化为：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/1b582f81310e43a7bb83e65fedaea531?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>是一个具有常数系数和常数项的一阶微分方程。</p><p>有通解公式：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7d6ee6bb9f5141f6a3d5c2c474ecd1b5?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>这就是指数分布的分布函数，求导即得到概率密度函数：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/77ea02d41f2642eca4ad3f998a00c034?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>推导完毕。</p><h1 id="附录2-瑞利分布的推导"><a href="#附录2-瑞利分布的推导" class="headerlink" title="附录2 瑞利分布的推导"></a>附录2 瑞利分布的推导</h1><p>定理1 若 随机变 量 X与Y相互 独立 , 又 f( x) 、 g (x ) 是两个连 续函 数 , 则随机 变量 f (X)如与 g (Y)相互独立 。</p><p>定理2 随机变量X的概率密度函数为Ψ(x), 则随机变量Y&#x3D;X2的概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ec8bae5791b04016a5e290c0eb24d806?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>定理3 若 随机变 量 X与Y相互独立 , 且其概率密度函数分别为Φ(x)、θ(y)。</p><p>则随机变量Z&#x3D;X+Y的概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0eef23cc0c1f47489d432804016cb8c2?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>定理4 设X是一个连续型随机变量 , 其概率密度函数为Ψ(x), 由函数y&#x3D;f(x)严格单调，其反函数h(y)有连续导数,随机变量Y&#x3D;f(X)也是一个连续型随机变量，其概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/749fda67702a4682a508feec038346d1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中α &#x3D; min{f(-∞), f(+∞)}, β &#x3D; max{f(-∞), f(+∞)}</p><p>设两个随机变量ζ与η相互独立且它们均服从正态分布N(0,σ2) , 则随机变量</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a3e2fab9cc5848bc8cc3e7c712e7e63d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>服从瑞利分布</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cb1971d4487e4d4eb89ef10b00a54400?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>其中σ为常数，且σ&gt;0。</p><p>推导如下：</p><p>由定理1、2， 可得X&#x3D;ζ2 和 Y&#x3D;η2 相互独立，其概率密度函数分别为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7e318217ec27419cb741dd71114784ef?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/cbf2f5eb49844e5f97bcc47229548309?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>由定理3随机变量Z&#x3D;X+Y的概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/089d6cc5bb124e459edb969cf92c6039?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>故Z&#x3D;X+Y的概率密度函数为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b528d119d61a469ba63bb695c13a212c?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>由定理4可得随机变量</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a15dd424966c447d8c29235ee09d72f1?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>的概率分布服从瑞利分布：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/6ba3ffd6c7694529867b8035466f20ba?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>推导完毕。</p><h1 id="附录3-Beta分布的推导"><a href="#附录3-Beta分布的推导" class="headerlink" title="附录3 Beta分布的推导"></a>附录3 Beta分布的推导</h1><p>在贝叶斯公式中，离散随机变量的后验概率可以由先验概率和条件概率得到</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ea6f84df27b3473ca7ee0401e637d50a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>在已经知道参数先验分布信息与样本信息的情况下，我们可以应用贝叶斯公式得到参数的连续随机变量的后验分布信息。</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3b25b32721cc44dbbdad80e20da10637?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>这里，θ表示需要估计的未知参数，x表示样本信息，π ( θ )表示θ 的先验密度函数，L ( x | θ ) 表示x 关于θ 的条件密度函数，Θ 表示参数θ 的取值空间。</p><p>已知样本信息x，需要选择合适的参数θ使发生样本所代表事件的概率最大，所以L ( x | θ ) 在这里是一个似然函数。</p><p>假设随机变量X服从二项分布B ( n , θ ) ，那么似然函数:</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/816a0fcf3201409a8d43c612c5fe384d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>如果我们对参数θ一无所知，那么对θ的先验分布π ( θ ) ，可以做θ~U ( 0 , 1 )均匀分布的假设，我们称之为贝叶斯假设：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/89e186a0ea094c03a1364c7766f0ea7a?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>由以上似然函数和参数的先验分布可以得出参数的后验分布：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0fc3332b1b434f0e974e22e3d3ffd38d?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>令x&#x3D;θ，α &#x3D; x + 1，β &#x3D; n − x + 1，可以得到Beta分布的概率密度函数：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/44c474deebd64d4bba48d59a5a68b1e0?from=pc" alt="流行算法：追根溯源之统计推断常用概率分布"></p><p>推导完成。</p><h1 id="二十、参考文献"><a href="#二十、参考文献" class="headerlink" title="二十、参考文献"></a>二十、参考文献</h1><p>[1] Hurst, Simon. The Characteristic Function of the Student tDistribution, Financial Mathematics Research Report No. FMRR006-95, Statistics Research Report No. SRR044-95 Archived February 18, 2010, at the Wayback Machine.</p><p>[2] Johnson, Norman Lloyd; Samuel Kotz; N. Balakrishnan (1995). Continuous Univariate Distributions, Volume 2 (Second Edition, Section 27). Wiley. [1] ISBN 0-471-58494-0.</p><p>[3] Abramowitz, Milton; Stegun, Irene Ann, eds. (1983) [June 1964]. “Chapter 26”. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. Applied Mathematics Series. 55 (Ninth reprint with additional corrections of tenth original printing with corrections (December 1972); first ed.). Washington D.C.; New York: United States Department of Commerce, National Bureau of Standards; Dover Publications. p. 946. ISBN 978-0-486-61272-0. LCCN 64-60036. MR 0167642. LCCN 65-12253.</p><p>[4] Lazo, A.V.; Rathie, P. (1978). “On the entropy of continuous probability distributions”. IEEE Transactions on Information Theory. IEEE. 24 (1): 120–122.[1] doi:10.1109&#x2F;tit.1978.1055832.</p><p>[5] en.wikipedia.org&#x2F;wiki&#x2F;Beta_distribution</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Box–Muller变换法</title>
      <link href="/2021/12/26/Box%E2%80%93Muller%E5%8F%98%E6%8D%A2%E6%B3%95/"/>
      <url>/2021/12/26/Box%E2%80%93Muller%E5%8F%98%E6%8D%A2%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6946910326591963660/?log_from=1ed72a8905505_1636700263179">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>Box–Muller 变换是一种快速产生符合标准正态分布随机数对的一种方法。基本思想是先得到服从均匀分布的随机数，再将服从均匀分布的随机数转变为服从标准正态分布（零期望，单位方差）的独立的随机数对。</p><p>它是由 George E. P. Box 与 Mervin E. Muller 在1958年提出，是最早运用与产生高斯白噪声的著名算法之一，它的基本原理是计算出高斯随机数的相位和幅度，进而产生高斯随机数对的算法。实际上，该方法最早是在1934年由Raymond E. A. C. Paley和Norbert Wiener明确提及的。</p><p>George E. P. Box是一位统计学大师，统计学中的很多名词术语都以他的名字命名。Box 之于统计学的家学渊源相当深厚，他的导师是统计学开山鼻祖皮尔逊的儿子-英国统计学家Egon Pearson；同时，Box还是统计学的另外一位巨擘级奠基人费希尔的女婿。统计学中的名言“All models are wrong, but some are useful”（所有模型都是错的，但其中一些是有用的）也出自Box之口。</p><p>Box-Muller变换通常以标准和极坐标两种形式表示。 Box和Muller给出的基本形式是从区间[0，1]上的均匀分布中获取两个样本，并将它们映射为两个标准的正态分布样本。极坐标形式从不同的区间[-1，+1]中获取两个样本，并将它们映射到两个正态分布的样本，而无需使用正弦或余弦函数。</p><h1 id="二、优点"><a href="#二、优点" class="headerlink" title="二、优点"></a>二、优点</h1><p>目前，产生正态分布随机数的主流方法有：</p><ul><li>用中心极限定理生成正态分布</li><li>逆变换法</li><li>Ziggurat 算法</li><li>Box-Muller变换</li></ul><p>Box-Muller变换是作为逆变换采样方法的一种计算效率更高的替代方法而开发的。Ziggurat算法为标量处理器（例如，旧的CPU）提供了一种更有效的方法，而Box-Muller变换对于具有矢量单位的处理器（例如，GPU或现代CPU）更胜一筹。</p><h1 id="三、两种形式"><a href="#三、两种形式" class="headerlink" title="三、两种形式"></a>三、两种形式</h1><h1 id="1、标准形式的-Box–Muller-变换"><a href="#1、标准形式的-Box–Muller-变换" class="headerlink" title="1、标准形式的 Box–Muller 变换"></a>1、标准形式的 Box–Muller 变换</h1><h1 id="（1）公式"><a href="#（1）公式" class="headerlink" title="（1）公式"></a>（1）公式</h1><p>假设变变量 U1 和变量 U2 是（0，1]均匀分布的随机数，</p><p>且 U1 和 U2 彼此独立，令：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/251f652ac3f7434195b61860cc28feaf?from=pc" alt="流行算法：Box–Muller变换法"></p><p>则 Z0 和 Z1 就是服从 N（0，1）的标准正态分布随机数，并且 Z0 和 Z1 相互独立。</p><h1 id="（2）标准形式的python代码演示"><a href="#（2）标准形式的python代码演示" class="headerlink" title="（2）标准形式的python代码演示"></a>（2）标准形式的python代码演示</h1><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def box_muller_trans():</span><br><span class="line">    x1 = <span class="number">0</span></span><br><span class="line">    x2 = <span class="number">0</span></span><br><span class="line">    w = <span class="number">0</span></span><br><span class="line">    x1 = <span class="built_in">np</span>.<span class="built_in">random</span>.rand()</span><br><span class="line">    x2 = <span class="built_in">np</span>.<span class="built_in">random</span>.rand()</span><br><span class="line">    y1 = <span class="built_in">np</span>.<span class="built_in">cos</span>(<span class="number">2.0</span>*<span class="built_in">np</span>.pi*x1) * <span class="built_in">np</span>.<span class="built_in">sqrt</span>(-<span class="number">2.0</span>*<span class="built_in">np</span>.<span class="built_in">log</span>(x1))</span><br><span class="line">    y2 = <span class="built_in">np</span>.<span class="built_in">sin</span>(<span class="number">2.0</span>*<span class="built_in">np</span>.pi*x2) * <span class="built_in">np</span>.<span class="built_in">sqrt</span>(-<span class="number">2.0</span>*<span class="built_in">np</span>.<span class="built_in">log</span>(x2))</span><br><span class="line">    <span class="built_in">return</span> y1,y2</span><br></pre></td></tr></table></figure><h1 id="2、-极坐标形式的-Box–Muller-变换"><a href="#2、-极坐标形式的-Box–Muller-变换" class="headerlink" title="2、 极坐标形式的 Box–Muller 变换"></a>2、 极坐标形式的 Box–Muller 变换</h1><h1 id="（1）公式-1"><a href="#（1）公式-1" class="headerlink" title="（1）公式"></a>（1）公式</h1><p>假设变量 u 和变量 v 是在[-1，1]上的均匀分布随机量，</p><p>u 和 v 相互独立，令：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3dc0840591c043a3bd58c2d1cb26a858?from=pc" alt="流行算法：Box–Muller变换法"></p><p>故而，随机数 z0 和 z1 计算后得出如下结果：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3c161ec4b3d54509a185f7d4f8863308?from=pc" alt="流行算法：Box–Muller变换法"></p><p>z0 和 z1 是服从分布 N（0，1）的随机数，并且 z0 和 z1 相互 独立。</p><h1 id="（2）极坐标形式的python代码演示"><a href="#（2）极坐标形式的python代码演示" class="headerlink" title="（2）极坐标形式的python代码演示"></a>（2）极坐标形式的python代码演示</h1><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">def</span> box_muller_trans():</span><br><span class="line">    <span class="attribute">x1</span> = <span class="number">0</span></span><br><span class="line">    <span class="attribute">x2</span> = <span class="number">0</span></span><br><span class="line">    <span class="attribute">w</span> = <span class="number">0</span></span><br><span class="line">    <span class="attribute">while</span> (w &lt;= <span class="number">0</span>)|(w&gt;=<span class="number">1</span>.<span class="number">0</span>):</span><br><span class="line">        <span class="attribute">x1</span> = <span class="number">2</span>.<span class="number">0</span>*np.random.rand()-<span class="number">1</span></span><br><span class="line">        <span class="attribute">x2</span> = <span class="number">2</span>.<span class="number">0</span>*np.random.rand()-<span class="number">1</span></span><br><span class="line">        <span class="attribute">w</span> = x1*x1 + x2*x2</span><br><span class="line">        <span class="attribute">w</span> = np.sqrt(-<span class="number">2</span>.<span class="number">0</span>*np.log(w)/w)</span><br><span class="line">        <span class="attribute">y1</span> = x1*w</span><br><span class="line">        <span class="attribute">y2</span> = x2*w</span><br><span class="line">    <span class="attribute">return</span> y1,y2</span><br></pre></td></tr></table></figure><h1 id="3、演示结果"><a href="#3、演示结果" class="headerlink" title="3、演示结果"></a>3、演示结果</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/552b7d606f2341bd8f05179715d6eb0a?from=pc" alt="流行算法：Box–Muller变换法"></p><h1 id="4、小结"><a href="#4、小结" class="headerlink" title="4、小结"></a>4、小结</h1><p>也可利用Matlab工具进行算法验证。在实际工程应用中，如果直接采用标准形式的Box-Muller算法的时候，需要计算正弦（sin）和余弦（cos）函数，这种计算耗时较多，效率比较低。Box-Muller 的极坐标形式更加常用，它避开了三角函数的计算，可以在很短的时间内产生大量的符合正态分布的随机数，能够满足了工程计算中对计算速度的要求。</p><p>更多的统计分布如何通过均匀分布的变换生成出来，大家可以参考 Sheldon M. Ross 的《统计模拟》。</p><h1 id="四、Box-Muller变换的证明"><a href="#四、Box-Muller变换的证明" class="headerlink" title="四、Box-Muller变换的证明"></a>四、Box-Muller变换的证明</h1><h1 id="1、标准形式推导"><a href="#1、标准形式推导" class="headerlink" title="1、标准形式推导"></a>1、标准形式推导</h1><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7af192f485fd4fee9201928c5921528e?from=pc" alt="流行算法：Box–Muller变换法"></p><h1 id="2、标准形式证明"><a href="#2、标准形式证明" class="headerlink" title="2、标准形式证明"></a>2、标准形式证明</h1><p>设U1,U2相互独立，均服从分布U(0,1)</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/40d0e85cc9884aac90cdf247f1b8961e?from=pc" alt="流行算法：Box–Muller变换法"></p><p>则X、Y相互独立且均服从标准正态分布。</p><p>证明：U1,U2~U(0,1) 所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/472ca2653b6c4719ae81913a1b6f0160?from=pc" alt="流行算法：Box–Muller变换法"></p><p>因为θ&#x3D;2πU2, 根据一元随机变量函数分布（见附录）， 有：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f992e227573f49c4a310b490b8a0c6a4?from=pc" alt="流行算法：Box–Muller变换法"></p><p>因为</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/267ea12591d5447a852b13135ce3ad6e?from=pc" alt="流行算法：Box–Muller变换法"></p><p>同样，根据一元随机变量函数分布，有：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7dfb4e6930274d708d34830a42a2aa7b?from=pc" alt="流行算法：Box–Muller变换法"></p><p>因为U1、U2互相独立，而 θ只取决于U2，R只取决于U1，所以θ 、R互相独立。而独立随机变量联合分布等于边缘分布之积。所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/8ad1e7cc6c1545e4aac7c2e2885238f2?from=pc" alt="流行算法：Box–Muller变换法"></p><p>因为X &#x3D; Rcosθ, y&#x3D;Rsinθ, 根据二元随机变量函数分布，有：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/a1c62e3c44834dbfa86fc5de10c57b54?from=pc" alt="流行算法：Box–Muller变换法"></p><p>其中</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/87958bd7e742442dbce56148f6187b16?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/245bf068c4084380973569c4ce86c6bb?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ea4f2df61f954f9ca201d743783c7c3f?from=pc" alt="流行算法：Box–Muller变换法"></p><p>同理可得：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4881e699214f412eb462cdd9ae4103a0?from=pc" alt="流行算法：Box–Muller变换法"></p><p>可见X和Y均服从标准正态分布。又因可验证</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2b4843ee25ae4010a19ae2c3e18dbf4d?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以X,Y互相独立。</p><p>证毕。</p><h1 id="3、极坐标形式的推导与证明"><a href="#3、极坐标形式的推导与证明" class="headerlink" title="3、极坐标形式的推导与证明"></a>3、极坐标形式的推导与证明</h1><p>设u,v相互独立，均服从分布U(-1,1)，且有</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/7b0148e1fb4340079200bc0f5baf8dd7?from=pc" alt="流行算法：Box–Muller变换法"></p><p>则X、Y相互独立且均服从标准正态分布。</p><p>根据Box-Muller变换的标准形式</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0735064f0f1a4fb3be3598c365c52c51?from=pc" alt="流行算法：Box–Muller变换法"></p><p>作变换</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/321d382ae2c2413588369176ec1c1752?from=pc" alt="流行算法：Box–Muller变换法"></p><p>可得：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f7f6fabd51bd4fa5b56d988b26025850?from=pc" alt="流行算法：Box–Muller变换法"></p><p>现在证明(*)变换满足: U1,U2相互独立，均服从分布U(0,1)。</p><p>证明：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3a975d9ffd8549a1a657039b2618dcbe?from=pc" alt="流行算法：Box–Muller变换法"></p><p>图4-1</p><p>如图4-1所示。首先确认U1,U2的取值范围均为(0,1)。因为(u,v)均匀分布在单位圆内，而</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/e2f43c81c76c4107a0c082579daf24af?from=pc" alt="流行算法：Box–Muller变换法"></p><p>故U1取值范围为(0,1)。θ&#x3D;2πU2,因为θ的取值范围是(0,2π), 故U2的取值范围是(0,1)。</p><p>因为(u,v)均匀分布在单位圆内，故</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/de1075ce52a443309aeeb6c983130084?from=pc" alt="流行算法：Box–Muller变换法"></p><p>根据二元随机变量函数分布(见附录)，有</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b2decb1604ae4abca404e600df16905f?from=pc" alt="流行算法：Box–Muller变换法"></p><p>现在求J</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/82bad890f3924ce1bfc6def8b839c398?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/b675fcc6e41d4e9bacb164e6d130cf83?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/614beb43c19742da83c36f452c2738af?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3616a80a2d0145aa8a101bcbe432575b?from=pc" alt="流行算法：Box–Muller变换法"></p><p>同理:</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/ffbcee4fdaee4516bfb30484ba320dca?from=pc" alt="流行算法：Box–Muller变换法"></p><p>故U1，U2均服从U(0,1)。</p><p>又</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/0ab2272149fb4ce3a5b94a7c934c5012?from=pc" alt="流行算法：Box–Muller变换法"></p><p>故U1,U2相互独立。</p><p>证毕。</p><h1 id="五、附录"><a href="#五、附录" class="headerlink" title="五、附录"></a>五、附录</h1><h1 id="1、一元随机变量函数的分布"><a href="#1、一元随机变量函数的分布" class="headerlink" title="1、一元随机变量函数的分布"></a>1、一元随机变量函数的分布</h1><p>如果</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/d8d72501e64d4ccb884de4d68f50001b?from=pc" alt="流行算法：Box–Muller变换法"></p><p>y为单调函数，则</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/f419a05fe1d246a18934c41df5943301?from=pc" alt="流行算法：Box–Muller变换法"></p><p>证明：因为</p><p>P{Y ≤y(x)} &#x3D; P{X ≤x},</p><p>P{Y ≤y(x+dx)} &#x3D; P{X ≤x+dx}</p><p>两式相减得</p><p>P{y(x) ≤Y ≤y(x+dx)} &#x3D; P{x ≤X ≤x+dx}</p><p>又</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/55dad8afe5cd4d40958c6b733ace020c?from=pc" alt="流行算法：Box–Muller变换法"></p><p>dy之所以加绝对值，是因为dy有可能为负（当y为减函数时，区间[y(x),y(x+dx)]为负区间，dy为负）。</p><p>得</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/d1ee6e9928bc4874ae614239015b260d?from=pc" alt="流行算法：Box–Muller变换法"></p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/4829fb2deff845068ebae54f15ef14ba?from=pc" alt="流行算法：Box–Muller变换法"></p><h1 id="2、二元随机变量函数的分布"><a href="#2、二元随机变量函数的分布" class="headerlink" title="2、二元随机变量函数的分布"></a>2、二元随机变量函数的分布</h1><p>设X,Y均为二元随机变量，即X&#x3D;(X1,X2), Y&#x3D;(Y1, Y2)</p><p>向量y&#x3D;(y1, y2), 如果</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/2a772bda88f24b3897d911278f462597?from=pc" alt="流行算法：Box–Muller变换法"></p><p>则：</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/bb4fcb5011b743b9a5ca990fa7c486a4?from=pc" alt="流行算法：Box–Muller变换法"></p><p>证明：</p><p>对于X取值区域上的面元dS，将经过函数y映射为Y的取值区域上的面元记为y(dS)，有：</p><p>P{X∈dS} &#x3D; P{Y∈y(dS)}</p><p>又</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3c4a4e0105fd451fb8b8568a841ed5da?from=pc" alt="流行算法：Box–Muller变换法"></p><p>其中Area(y(dS))加绝对值，是因为Area(y(dS))可能为负（J&lt;0时）。</p><p>又因为面积元之比等于雅可比行列式，即</p><p>Area(y(dS))&#x3D;J * Area(dS)</p><p>所以</p><p><img src="https://p3.toutiaoimg.com/origin/pgc-image/3c8ecdb3fe6c414faba71f05f7498ce3?from=pc" alt="流行算法：Box–Muller变换法"></p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Dijkstra算法</title>
      <link href="/2021/12/04/Dijkstra%E7%AE%97%E6%B3%95/"/>
      <url>/2021/12/04/Dijkstra%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://www.toutiao.com/article/6935059545899205134/">今日头条</a></p><h1 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h1><p>Dijkstra算法（迪杰斯特拉算法）是很有代表性的最短路径算法，用于计算一个结点到其他结点的最短路径。该算法指定一个点（源点）到其余各个结点的最短路径，因此也叫做单源最短路径算法。该算法是由荷兰计算机科学家Edsger W.Dijkstra于1959年发表。</p><p>Dijkstra算法是一种用于计算带权有向图中单源最短路径算法，不存在回溯的过程，因此它还不适用于带有负权重的情况。如果权值存在负数，那么被派生出来的可能是更短的路径，这就需要过程可以回溯，之前的路径需要被更短的路径替换掉，而Dijkstra算法是不能回溯的，它的每一步都是以当前最优选择为前提的。</p><p>Dijkstra算法的思想是广度优先搜索（BFS）+贪心策略。对于计算非加权图中的最短路径，也可使用BFS算法。Dijkstra算法是对BFS算法的推广，以起始点为中心向外层层扩展，并且每一次都选择最优的结点进行扩展，直到扩展到终点为止。Dijkstra算法可以划归为贪心算法，下一条路径都是由当前更短的路径派生出来的更长的路径。</p><p>Dijkstra算法在很多专业课程中都作为基本内容有详细的介绍，如数据结构、图论、运筹学等。</p><h1 id="二、演示例子"><a href="#二、演示例子" class="headerlink" title="二、演示例子"></a>二、演示例子</h1><h1 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h1><p>第1步，创建距离表。第1列是结点名称，第2列是从起点A到对应结点的已知最短距离。开始我们并不知道A到其它结点的最短距离是多少，默认初始距离是无穷大。如图2-1-1所示：</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/a8689f20f2ae481f9b30b16fc4647a1e?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-1</p><p>第2步，遍历起点A的所有相邻结点，找到起点A的邻接结点B和C。从A到B的距离是5，从A到C的距离是2，刷新距离表中起点A到各结点的最短距离(绿色表示刷新)。如图2-1-2所示。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/2598ec87cb944b8894d9491adef654bf?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-2</p><p>第3步，从图2-1-2距离表中找到从A出发距离最短的点，也就是结点C（最小距离是2）。遍历结点C的所有相邻结点，找到结点C的相邻结点D和F（A已经遍历过，不需要考虑）。从C到D的距离是1，所以A到D的距离是A-C-D&#x3D;2+1&#x3D;3；从C到F的距离是8；从A到F的距离是A-C-F&#x3D;2+8&#x3D;10。然后刷新距离表(绿色表示刷新)。如图2-1-3所示：</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/d243ccb9e3b64be99f33831eba6c9f47?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-3</p><p>第4步，从图2-1-3距离表中找到从A出发距离最短的点（红色结点C已经遍历过，不需要考虑），也就是结点D（最小距离是3）。遍历结点D的所有相邻结点，找到相邻结点B、E和F（C已遍历过，不考虑）。从A-C-D-B的距离是3+1&#x3D;4；从A-C-D-E的距离是3+1&#x3D;4；从A-C-D-F的距离是3+2&#x3D;5。刷新距离表中起点A到各结点的最短距离。如图2-1-4所示。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/786de4ae95f5433580bc25861ad66961?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-4</p><p>第5步，从图2-1-4距离表中找到从A出发距离最短的点（红色结点C、D已经遍历过，不需要考虑），也就是结点B和E（最小距离是4）。遍历结点B的所有相邻结点，找到相邻结点E(D遍历过，不考虑),从A-C-D-B-E的距离为10，比当前A到E的最小距离4要大，不考虑。遍历结点E的所有相邻结点，找到相邻结点G、B(D遍历过，不考虑),从A-C-D-E-G的距离为4+7&#x3D;11&lt;∞, 刷新距离表;A-C-D-E-B的距离4+6&#x3D;10&gt;4,不考虑。如图2-1-5所示。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/ab214866087b4d2ba1057b03976525c0?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-5</p><p>第6步，从图2-1-5距离表中找到从A出发距离最短的点（红色结点B、C、D、E已经遍历过，不需要考虑），也就是结点F（最小距离是5）。从A-C-D-F-G的距离为8, 比当前最小距离11要小，刷新距离表。如图2-1-6所示。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/066a452e5efe4a1c8caaaf342ccc9313?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-1-6</p><p>就这样，除终点以外的全部结点都已经遍历完毕，距离表中存储的是从起点A到所有结点的最短距离。</p><h1 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h1><p>图2-2-1是原始连通图。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/aa5d6c4688134f5cb1faf25b8e8510b0?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-2-1</p><p>用Dijkstra算法找出以A为起点的单源最短路径步骤如下：</p><table><thead><tr><th>步骤</th><th>集合S</th><th>集合Q</th></tr></thead><tbody><tr><td>1</td><td>选择A到集合S&#x3D;{A}此时最短路径A-&gt;A&#x3D;0以A为中间点，查找相邻点</td><td>Q&#x3D;{B,C,D,E,F,G}A-&gt;-B&#x3D;5A-&gt;C&#x3D;2A-&gt;其它Q中结点&#x3D;∞发现A-&gt;C&#x3D;2权值为最短</td></tr><tr><td>2</td><td>选择C到S&#x3D;{A,C}此时最短路径A-&gt;A&#x3D;0,A-&gt;C&#x3D;2以C为中间点，从A-&gt;C这条路径开始找</td><td>Q&#x3D;{B,D,E,F,G}A-&gt;B&#x3D;5(由第1步得到)A-&gt;C-&gt;D&#x3D;3A-&gt;C-&gt;F&#x3D;10A-&gt;C-&gt;其它Q中结点&#x3D;∞在A到Q的结点中，发现A-&gt;C-&gt;D&#x3D;3权值为最短</td></tr><tr><td>3</td><td>选择D到S&#x3D;{A,C,D}此时最短路径A-&gt;A&#x3D;0,A-&gt;C&#x3D;2,A-&gt;C-&gt;D&#x3D;3,以D为中间点，从A-&gt;C-&gt;D这条路径开始找</td><td>Q&#x3D;{B,E,F,G}A-&gt;C-&gt;D-&gt;B&#x3D;4(比第1步的A-&gt;B&#x3D;5要短，替换之)A-&gt;C-&gt;D-&gt;E&#x3D;4A-&gt;C-&gt;D-&gt;F&#x3D;5(比第2步的A-&gt;C-&gt;F&#x3D;10要短，替换之)A-&gt;C-&gt;D-&gt;G&#x3D;∞在A到Q的结点中，发现A-&gt;C-&gt;D-&gt;B&#x3D;4或A-&gt;C-&gt;D-&gt;E&#x3D;4权值为最短</td></tr><tr><td>4</td><td>选择B、E到S&#x3D;{A,C,D,B,E}此时最短路径A-&gt;A&#x3D;0,A-&gt;C&#x3D;2,A-&gt;C-&gt;D&#x3D;3,A-&gt;C-&gt;D-&gt;B&#x3D;4,A-&gt;C-&gt;D-&gt;E&#x3D;4,以B、E为中间点，分别从A-&gt;C-&gt;D-&gt;B、从A-&gt;C-&gt;D-&gt;E路径开始找</td><td>Q&#x3D;{F,G}A-&gt;C-&gt;D-&gt;E-&gt;G&#x3D;11A-&gt;C-&gt;D-&gt;F&#x3D;5(从第3步获得)在A到Q的结点中，发现A-&gt;C-&gt;D-&gt;F权值为最短</td></tr><tr><td>5</td><td>选择F到S&#x3D;{A,C,D,B,E,F}此时最短路径A-&gt;A&#x3D;0,A-&gt;C&#x3D;2,A-&gt;C-&gt;D&#x3D;3,A-&gt;C-&gt;D-&gt;B&#x3D;4,A-&gt;C-&gt;D-&gt;E&#x3D;4,A-&gt;C-&gt;D-&gt;F&#x3D;5以F为中间点，从,A-&gt;C-&gt;D-&gt;F这条路径开始找</td><td>Q&#x3D;{G}A-&gt;C-&gt;D-&gt;F-&gt;G&#x3D;8(比第4步的A-&gt;C-&gt;D-&gt;E-&gt;G&#x3D;11要短，替换之)</td></tr><tr><td>6</td><td>选择G到S&#x3D;{A,C,D,B,E,F,G}此时最短路径A-&gt;A&#x3D;0,A-&gt;C&#x3D;2,A-&gt;C-&gt;D&#x3D;3,A-&gt;C-&gt;D-&gt;B&#x3D;4,A-&gt;C-&gt;D-&gt;E&#x3D;4,A-&gt;C-&gt;D-&gt;F&#x3D;5,A-&gt;C-&gt;D-&gt;F-&gt;G&#x3D;8</td><td>集合Q为空，查找完毕。</td></tr></tbody></table><p>显示更多</p><h1 id="例子3"><a href="#例子3" class="headerlink" title="例子3"></a>例子3</h1><p>Dijkstra算法的执行过程：设初始集合S&#x3D;{s}, Q&#x3D;{t,y,x,z}. 源结点s为最左边的结点，每个结点中（圆圈中）的数值为该结点的最短路径的估计值（当前中间值）。黑色的结点属于集合S，白色的结点属于集合Q。每次从集合 S中选择最新加入的结点，分别计算并刷新与它直接相邻的结点的最短路径的估计值，然后从集合Q中选择最小估计值的结点，加入到集合S中。例如(b)中，集合Q中刷新后各结点的估计值为10，5，∞，∞，选择最小估计值为5的结点y，加入到集合S中, 接着计算并刷新结点y的相邻结点的最短路径的估计值。依次类推，直到集合Q中的所有结点全部加入到集合S中，算法结束。如图2-3-1所示。</p><p><img src="https://p9.toutiaoimg.com/origin/pgc-image/f9a8c374a1ec41b3bcf7e73406208a6a?from=pc" alt="流行算法：Dijkstra 算法"></p><p>图2-3-1</p><h1 id="三、应用"><a href="#三、应用" class="headerlink" title="三、应用"></a>三、应用</h1><p>一切能抽象成图或树的场景，如果要求最短路径，Dijkstra算法可考虑。比如，查找两个城市之间的最短路径；在地图中寻找两个地点之间的最短路径；在网络连接中为路由器寻找最短的传输路径等。</p>]]></content>
      
      
      <categories>
          
          <category> 流行算法 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
